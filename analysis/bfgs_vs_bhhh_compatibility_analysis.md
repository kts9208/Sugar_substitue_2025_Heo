# BFGS vs BHHH í˜¸í™˜ì„± ë¶„ì„

## ğŸ“‹ ìš”ì•½

**ì§ˆë¬¸**: í˜„ì¬ BFGS ëª¨ë“ˆì´ ì–´ë–»ê²Œ êµ¬í˜„ë˜ì–´ ìˆëŠ”ì§€, BHHH ê¸°ë²•ìœ¼ë¡œ ë³€ê²½í•˜ë ¤ë©´ í˜¸í™˜ì´ í˜ë“ ì§€ ê²€í† 

**ë‹µë³€**: 
- âœ… **BFGS â†’ BHHH ë³€ê²½ì€ ì™„ì „íˆ í˜¸í™˜ ê°€ëŠ¥**
- âœ… **ì´ë¯¸ BHHH êµ¬í˜„ ì™„ë£Œ** (`docs/early_stopping_hessian_optimization.md` ì°¸ì¡°)
- âœ… **BFGSëŠ” ìµœì í™” ì•Œê³ ë¦¬ì¦˜, BHHHëŠ” Hessian ê·¼ì‚¬ ë°©ë²•** (ì„œë¡œ ë‹¤ë¥¸ ì—­í• )
- âœ… **í•¨ê»˜ ì‚¬ìš© ê°€ëŠ¥**: BFGSë¡œ ìµœì í™” + BHHHë¡œ í‘œì¤€ì˜¤ì°¨ ê³„ì‚°

---

## 1. í˜„ì¬ BFGS êµ¬í˜„ êµ¬ì¡°

### **1.1. ìµœì í™” í”„ë ˆì„ì›Œí¬**

**ìœ„ì¹˜**: `src/analysis/hybrid_choice_model/iclv_models/simultaneous_estimator_fixed.py`

```python
# Line 1234-1242
result = optimize.minimize(
    early_stopping_wrapper.objective,  # ëª©ì  í•¨ìˆ˜ (negative log-likelihood)
    initial_params_scaled,              # ì´ˆê¸° íŒŒë¼ë¯¸í„°
    method=self.config.estimation.optimizer,  # 'BFGS' ë˜ëŠ” 'L-BFGS-B'
    jac=jac_function,                   # Gradient í•¨ìˆ˜ (analytic)
    bounds=bounds if self.config.estimation.optimizer == 'L-BFGS-B' else None,
    callback=early_stopping_wrapper.callback,  # Iteration callback
    options=optimizer_options           # BFGS ì˜µì…˜
)
```

**BFGS ì˜µì…˜** (Line 1207-1217):
```python
if self.config.estimation.optimizer == 'BFGS':
    optimizer_options = {
        'maxiter': 200,  # Major iteration ìµœëŒ€ íšŸìˆ˜
        'ftol': 1e-3,    # í•¨ìˆ˜ê°’ ìƒëŒ€ì  ë³€í™” 0.1% ì´í•˜ë©´ ì¢…ë£Œ
        'gtol': 1e-3,    # ê·¸ë˜ë””ì–¸íŠ¸ norm í—ˆìš© ì˜¤ì°¨
        'c1': 1e-4,      # Armijo ì¡°ê±´ íŒŒë¼ë¯¸í„° (scipy ê¸°ë³¸ê°’)
        'c2': 0.9,       # Curvature ì¡°ê±´ íŒŒë¼ë¯¸í„° (scipy ê¸°ë³¸ê°’)
        'disp': True
    }
```

### **1.2. BFGS ë‚´ë¶€ ë™ì‘ ì›ë¦¬**

**Scipyì˜ BFGS êµ¬í˜„** (`scipy.optimize._minimize._minimize_bfgs`):

```python
def _minimize_bfgs(fun, x0, jac, callback, ...):
    # 1. ì´ˆê¸°í™”
    x = x0
    H = np.eye(n)  # Hessian ì—­í–‰ë ¬ ì´ˆê¸°ê°’ (ë‹¨ìœ„ í–‰ë ¬)

    for k in range(maxiter):
        # 2. Gradient ê³„ì‚°
        g = jac(x)  # â† ìš°ë¦¬ì˜ analytic gradient í•¨ìˆ˜ í˜¸ì¶œ

        # 3. íƒìƒ‰ ë°©í–¥ ê³„ì‚°
        p = -H @ g  # H^{-1} Ã— (-g)

        # 4. Line search (Wolfe ì¡°ê±´)
        alpha = line_search(fun, jac, x, p, g, ...)

        # 5. íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        x_new = x + alpha * p

        # 6. Hessian ì—­í–‰ë ¬ ì—…ë°ì´íŠ¸ (BFGS ê³µì‹)
        s = x_new - x
        y = jac(x_new) - g

        # BFGS ì—…ë°ì´íŠ¸ ê³µì‹
        rho = 1.0 / (y.T @ s)
        H = (I - rho * s @ y.T) @ H @ (I - rho * y @ s.T) + rho * s @ s.T

        # 7. Callback í˜¸ì¶œ
        if callback is not None:
            callback(x_new)  # âš ï¸ x_newë§Œ ì „ë‹¬, HëŠ” ì „ë‹¬ ì•ˆ ë¨!

        # 8. ìˆ˜ë ´ ì²´í¬
        if converged:
            break

        x = x_new

    # 9. ê²°ê³¼ ë°˜í™˜
    return OptimizeResult(x=x, hess_inv=H, ...)  # âœ… ì •ìƒ ì¢…ë£Œ ì‹œì—ë§Œ H ë°˜í™˜
```

**í•µì‹¬ í¬ì¸íŠ¸**:
1. âœ… **BFGSëŠ” Hessian ì—­í–‰ë ¬ì„ ê·¼ì‚¬**í•˜ì—¬ íƒìƒ‰ ë°©í–¥ ê³„ì‚°
2. âœ… **ë§¤ iterationë§ˆë‹¤ H ì—…ë°ì´íŠ¸** (s_k, y_k ì‚¬ìš©)
3. âŒ **Callbackì—ì„œ H ì ‘ê·¼ ë¶ˆê°€** (x_newë§Œ ì „ë‹¬)
4. âœ… **ì •ìƒ ì¢…ë£Œ ì‹œ `result.hess_inv`ë¡œ H ë°˜í™˜**

### **1.3. í˜„ì¬ Hessian ì‚¬ìš© ë°©ì‹**

**í‘œì¤€ì˜¤ì°¨ ê³„ì‚°** (Line 1296-1372):

```python
if self.config.estimation.calculate_se:
    # BFGSì˜ hess_invê°€ ìˆìœ¼ë©´ ì‚¬ìš©
    if hasattr(result, 'hess_inv') and result.hess_inv is not None:
        self.logger.info("Hessian ì—­í–‰ë ¬: BFGSì—ì„œ ìë™ ì œê³µ (ì¶”ê°€ ê³„ì‚° 0íšŒ)")

        hess_inv = result.hess_inv
        if hasattr(hess_inv, 'todense'):
            hess_inv_array = hess_inv.todense()
        else:
            hess_inv_array = hess_inv

        # Hessian ì—­í–‰ë ¬ ì €ì¥
        self.hessian_inv_matrix = np.array(hess_inv_array)

        # ëŒ€ê° ì›ì†Œ = ë¶„ì‚° ê·¼ì‚¬
        diag_elements = np.diag(hess_inv_array)

        # í‘œì¤€ì˜¤ì°¨ = sqrt(ë¶„ì‚°)
        standard_errors = np.sqrt(np.abs(diag_elements))
    else:
        # L-BFGS-BëŠ” hess_inv ì œê³µ ì•ˆ í•¨
        self.logger.warning("Hessian ì—­í–‰ë ¬ ì—†ìŒ (L-BFGS-BëŠ” hess_inv ì œê³µ ì•ˆ í•¨)")
        self.hessian_inv_matrix = None
```

**ë¬¸ì œì **:
- âŒ **ì¡°ê¸° ì¢…ë£Œ ì‹œ `result.hess_inv` ì—†ìŒ** (StopIteration ì˜ˆì™¸)
- âŒ **L-BFGS-BëŠ” hess_inv ì œê³µ ì•ˆ í•¨** (ë©”ëª¨ë¦¬ ì œí•œ)
- âŒ **BFGSì˜ HëŠ” ê·¼ì‚¬ê°’** (ì‹¤ì œ Hessianê³¼ ì°¨ì´ ìˆìŒ)

---

## 2. BHHH ë°©ë²• ê°œìš”

### **2.1. BHHHë€?**

**BHHH (Berndt-Hall-Hall-Hausman, 1974)**:
- Maximum Likelihood Estimationì—ì„œ Hessianì„ ê·¼ì‚¬í•˜ëŠ” ë°©ë²•
- **ê°œì¸ë³„ gradientì˜ outer product í•©**ìœ¼ë¡œ Hessian ê·¼ì‚¬

### **2.2. ì´ë¡ ì  ë°°ê²½**

**ì •í™•í•œ Hessian**:
```
H = âˆ‚Â²LL/âˆ‚Î¸âˆ‚Î¸^T = Î£_i âˆ‚Â²LL_i/âˆ‚Î¸âˆ‚Î¸^T
```

**BHHH ê·¼ì‚¬**:
```
H_BHHH â‰ˆ -Î£_i (âˆ‚LL_i/âˆ‚Î¸) Ã— (âˆ‚LL_i/âˆ‚Î¸)^T
        = -Î£_i (grad_i Ã— grad_i^T)
```

**Information Matrix Equality** (MLE ì´ë¡ ):
```
E[-âˆ‚Â²LL/âˆ‚Î¸âˆ‚Î¸^T] = E[(âˆ‚LL/âˆ‚Î¸) Ã— (âˆ‚LL/âˆ‚Î¸)^T]
```

**ì˜ë¯¸**:
- ìµœì ì  ê·¼ì²˜ì—ì„œ BHHH ê·¼ì‚¬ëŠ” **Fisher Information Matrix**ì™€ ë™ì¼
- í‘œì¤€ì˜¤ì°¨ ê³„ì‚°ì— ì í•©

### **2.3. BHHH êµ¬í˜„ (ì´ë¯¸ ì™„ë£Œ)**

**ìœ„ì¹˜**: `src/analysis/hybrid_choice_model/iclv_models/simultaneous_estimator_fixed.py` (ì¡°ê¸° ì¢…ë£Œ í›„)

```python
# ì¡°ê¸° ì¢…ë£Œ í›„ Hessian ì—­í–‰ë ¬ ê³„ì‚° (BHHH ë°©ë²•)
if self.config.estimation.calculate_se:
    self.logger.info("ì¡°ê¸° ì¢…ë£Œ í›„ Hessian ì—­í–‰ë ¬ ê³„ì‚° ì¤‘ (BHHH ë°©ë²•)...")

    n_params = len(early_stopping_wrapper.best_x)

    # 1. ê°œì¸ë³„ gradient ê³„ì‚°
    individual_gradients = []
    param_dict = self._unpack_parameters(
        early_stopping_wrapper.best_x,
        measurement_model,
        structural_model,
        choice_model
    )

    # 2. ê° ê°œì¸ì— ëŒ€í•´ gradient ê³„ì‚° (ìµœëŒ€ 50ëª…)
    for i, (person_id, ind_data) in enumerate(data.groupby('person_id')):
        if i >= 50:  # ìƒ˜í”Œë§
            break

        ind_draws = halton_draws[i] if i < len(halton_draws) else halton_draws[0]

        # ê°œì¸ë³„ gradient ê³„ì‚° (GPU ë°°ì¹˜ ì²˜ë¦¬)
        grad_dict = self.joint_grad.compute_individual_gradient(
            ind_data=ind_data,
            ind_draws=ind_draws,
            params_dict=param_dict,
            measurement_model=measurement_model,
            structural_model=structural_model,
            choice_model=choice_model
        )

        # Gradientë¥¼ ë²¡í„°ë¡œ ë³€í™˜
        grad_vector = self._pack_gradient(
            grad_dict,
            measurement_model,
            structural_model,
            choice_model
        )

        individual_gradients.append(grad_vector)

    # 3. BHHH Hessian ê³„ì‚°: H = -Î£ (g_i Ã— g_i^T)
    hessian_bhhh = np.zeros((n_params, n_params))
    for grad in individual_gradients:
        hessian_bhhh -= np.outer(grad, grad)  # ìŒìˆ˜ (ìµœëŒ€í™” â†’ ìµœì†Œí™”)

    # 4. Hessian ì—­í–‰ë ¬ ê³„ì‚°
    hess_inv = np.linalg.inv(hessian_bhhh)

    # 5. í‘œì¤€ì˜¤ì°¨ ê³„ì‚°
    standard_errors = np.sqrt(np.diag(hess_inv))
```

**ê³„ì‚° ë¹„ìš©**:
- ìš°ë„ ê³„ì‚°: **0íšŒ** (gradientë§Œ ê³„ì‚°)
- Gradient ê³„ì‚°: **50íšŒ** (ê°œì¸ë³„)
- ì†Œìš” ì‹œê°„: **~1.5ë¶„** (GPU ì‚¬ìš© ì‹œ)

---

## 3. BFGS vs BHHH ë¹„êµ

### **3.1. ì—­í•  ì°¨ì´**

| í•­ëª© | BFGS | BHHH |
|------|------|------|
| **ëª©ì ** | ìµœì í™” ì•Œê³ ë¦¬ì¦˜ | Hessian ê·¼ì‚¬ ë°©ë²• |
| **ì‚¬ìš© ì‹œì ** | íŒŒë¼ë¯¸í„° ì¶”ì • ì¤‘ | ì¶”ì • ì™„ë£Œ í›„ (í‘œì¤€ì˜¤ì°¨ ê³„ì‚°) |
| **ì…ë ¥** | ëª©ì  í•¨ìˆ˜ + Gradient | ê°œì¸ë³„ Gradient |
| **ì¶œë ¥** | ìµœì  íŒŒë¼ë¯¸í„° + Hessian ì—­í–‰ë ¬ | Hessian ì—­í–‰ë ¬ |
| **Hessian ì—…ë°ì´íŠ¸** | ë§¤ iteration (s_k, y_k ì‚¬ìš©) | 1íšŒ (ìµœì ì ì—ì„œ) |

### **3.2. Hessian ê·¼ì‚¬ ì •í™•ë„**

| ë°©ë²• | ì •í™•ë„ | ì¡°ê±´ |
|------|--------|------|
| **BFGS** | ì¤‘ê°„ | Ill-conditioned ì‹œ ë¶€ì •í™• |
| **BHHH** | ë†’ìŒ | ìµœì ì  ê·¼ì²˜ì—ì„œ ì •í™• |
| **ìˆ˜ì¹˜ì  ë°©ë²•** | ê°€ì¥ ë†’ìŒ | ê³„ì‚° ë¹„ìš© ë§¤ìš° í¼ |

**í˜„ì¬ ë¬¸ì œ (Iteration #9 íŠ¹ì´í–‰ë ¬)**:
- BFGS Hessianì´ **ill-conditioned** â†’ íŠ¹ì´í–‰ë ¬
- BHHHëŠ” **ê°œì¸ë³„ gradient ì§ì ‘ ì‚¬ìš©** â†’ ë” ì•ˆì •ì 

### **3.3. ê³„ì‚° ë¹„ìš© ë¹„êµ**

| ë°©ë²• | ìš°ë„ ê³„ì‚° | Gradient ê³„ì‚° | ì†Œìš” ì‹œê°„ | Hessian í¬ê¸° |
|------|-----------|---------------|-----------|--------------|
| **BFGS (ìë™)** | 0íšŒ | 0íšŒ | 0ì´ˆ | ì „ì²´ (80Ã—80) |
| **BHHH** | 0íšŒ | 50íšŒ | 1.5ë¶„ | ì „ì²´ (80Ã—80) |
| **ìˆ˜ì¹˜ì  (ëŒ€ê°)** | 41,209íšŒ | 0íšŒ | 10.5ì¼ | ëŒ€ê°ë§Œ (80ê°œ) |

**BHHH ì¥ì **:
- âœ… BFGSë³´ë‹¤ **ë” ì •í™•** (ì‹¤ì œ gradient ì‚¬ìš©)
- âœ… ìˆ˜ì¹˜ì  ë°©ë²•ë³´ë‹¤ **10,080ë°° ë¹ ë¦„**
- âœ… **ì „ì²´ Hessian** ê³„ì‚° (ìƒê´€ê´€ê³„ í¬í•¨)

---

## 4. í˜¸í™˜ì„± ë¶„ì„

### **4.1. BFGS â†’ BHHH ë³€ê²½ ê°€ëŠ¥ì„±**

**ì§ˆë¬¸**: BFGSë¥¼ BHHHë¡œ ì™„ì „íˆ ëŒ€ì²´í•  ìˆ˜ ìˆëŠ”ê°€?

**ë‹µë³€**: âŒ **ë¶ˆê°€ëŠ¥** (ì—­í• ì´ ë‹¤ë¦„)

**ì´ìœ **:
1. **BFGS**: ìµœì í™” ì•Œê³ ë¦¬ì¦˜ (íŒŒë¼ë¯¸í„° ì¶”ì •)
2. **BHHH**: Hessian ê·¼ì‚¬ ë°©ë²• (í‘œì¤€ì˜¤ì°¨ ê³„ì‚°)

**ì˜¬ë°”ë¥¸ ì§ˆë¬¸**: BFGSì˜ Hessian ê·¼ì‚¬ë¥¼ BHHHë¡œ ëŒ€ì²´í•  ìˆ˜ ìˆëŠ”ê°€?

**ë‹µë³€**: âœ… **ê°€ëŠ¥** (ì´ë¯¸ êµ¬í˜„ë¨)

### **4.2. í˜„ì¬ êµ¬í˜„ ìƒíƒœ**

**ì‹œë‚˜ë¦¬ì˜¤ 1: ì •ìƒ ì¢…ë£Œ**
```python
result = optimize.minimize(..., method='BFGS', ...)

if result.success:
    # BFGSì˜ Hessian ì—­í–‰ë ¬ ì‚¬ìš©
    hess_inv = result.hess_inv
    standard_errors = np.sqrt(np.diag(hess_inv))
```

**ì‹œë‚˜ë¦¬ì˜¤ 2: ì¡°ê¸° ì¢…ë£Œ**
```python
# BFGS ì¡°ê¸° ì¢…ë£Œ (result.hess_inv ì—†ìŒ)

# BHHH ë°©ë²•ìœ¼ë¡œ Hessian ê³„ì‚°
hess_inv = compute_bhhh_hessian(individual_gradients)
standard_errors = np.sqrt(np.diag(hess_inv))
```

**ê²°ë¡ **: âœ… **ì´ë¯¸ í˜¸í™˜ ê°€ëŠ¥í•˜ê²Œ êµ¬í˜„ë¨**

### **4.3. ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ë³€ê²½ ì˜µì…˜**

**í˜„ì¬ ì§€ì›í•˜ëŠ” ë°©ë²•**:
1. **BFGS**: Hessian ì—­í–‰ë ¬ ê·¼ì‚¬ (ë¬´ì œí•œ ë©”ëª¨ë¦¬)
2. **L-BFGS-B**: ì œí•œëœ ë©”ëª¨ë¦¬ BFGS (bounds ì§€ì›)
3. **Nelder-Mead**: Gradient ë¶ˆí•„ìš” (ëŠë¦¼)

**ì¶”ê°€ ê°€ëŠ¥í•œ ë°©ë²•**:
1. **Trust Region**: Ill-conditioned Hessianì— ê°•í•¨
2. **Newton-CG**: ì •í™•í•œ Hessian ì‚¬ìš© (BHHH ì œê³µ ê°€ëŠ¥)
3. **Custom BHHH Optimizer**: BHHHë¡œ Hessian ê·¼ì‚¬í•˜ëŠ” ìµœì í™”

---

## 5. BHHHë¥¼ ìµœì í™”ì— ì‚¬ìš©í•˜ëŠ” ë°©ë²•

### **5.1. Custom Optimizer êµ¬í˜„**

**ì•„ì´ë””ì–´**: BFGS ëŒ€ì‹  BHHHë¡œ Hessianì„ ê·¼ì‚¬í•˜ëŠ” Newton ë°©ë²•

```python
def bhhh_optimizer(func, grad_func, x0, individual_grad_func, data, ...):
    """
    BHHH ë°©ë²•ì„ ì‚¬ìš©í•œ Newton ìµœì í™”

    Args:
        func: ëª©ì  í•¨ìˆ˜ (negative log-likelihood)
        grad_func: ì „ì²´ gradient í•¨ìˆ˜
        x0: ì´ˆê¸° íŒŒë¼ë¯¸í„°
        individual_grad_func: ê°œì¸ë³„ gradient í•¨ìˆ˜
        data: ë°ì´í„°
    """
    x = x0

    for iteration in range(max_iter):
        # 1. ì „ì²´ gradient ê³„ì‚°
        g = grad_func(x)

        # 2. ê°œì¸ë³„ gradient ê³„ì‚°
        individual_grads = []
        for person_id, ind_data in data.groupby('person_id'):
            grad_i = individual_grad_func(x, ind_data)
            individual_grads.append(grad_i)

        # 3. BHHH Hessian ê³„ì‚°
        H_bhhh = np.zeros((len(x), len(x)))
        for grad_i in individual_grads:
            H_bhhh -= np.outer(grad_i, grad_i)

        # 4. Newton ë°©í–¥ ê³„ì‚°
        try:
            H_inv = np.linalg.inv(H_bhhh)
            p = -H_inv @ g
        except np.linalg.LinAlgError:
            # Hessian íŠ¹ì´í–‰ë ¬ â†’ Gradient descent
            p = -g

        # 5. Line search
        alpha = line_search(func, grad_func, x, p, g)

        # 6. íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        x = x + alpha * p

        # 7. ìˆ˜ë ´ ì²´í¬
        if np.linalg.norm(g) < gtol:
            break

    return x, H_inv
```

**ì¥ì **:
- âœ… **ë§¤ iterationë§ˆë‹¤ ì •í™•í•œ Hessian** ì‚¬ìš©
- âœ… **Ill-conditioned ë¬¸ì œì— ê°•í•¨**
- âœ… **í‘œì¤€ì˜¤ì°¨ ìë™ ê³„ì‚°**

**ë‹¨ì **:
- âŒ **ê³„ì‚° ë¹„ìš© ë†’ìŒ** (ë§¤ iterationë§ˆë‹¤ ê°œì¸ë³„ gradient ê³„ì‚°)
- âŒ **ë©”ëª¨ë¦¬ ì‚¬ìš© ë§ìŒ** (ì „ì²´ Hessian ì €ì¥)

### **5.2. í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼**

**ê¶Œì¥ ë°©ë²•**: BFGSë¡œ ìµœì í™” + BHHHë¡œ í‘œì¤€ì˜¤ì°¨ ê³„ì‚°

```python
# 1. BFGSë¡œ ë¹ ë¥´ê²Œ ìµœì í™”
result = optimize.minimize(
    func,
    x0,
    method='BFGS',
    jac=grad_func,
    options={'maxiter': 200}
)

# 2. BHHHë¡œ ì •í™•í•œ í‘œì¤€ì˜¤ì°¨ ê³„ì‚°
hess_inv_bhhh = compute_bhhh_hessian(
    result.x,
    individual_grad_func,
    data
)

standard_errors = np.sqrt(np.diag(hess_inv_bhhh))
```

**ì¥ì **:
- âœ… **ë¹ ë¥¸ ìˆ˜ë ´** (BFGS)
- âœ… **ì •í™•í•œ í‘œì¤€ì˜¤ì°¨** (BHHH)
- âœ… **ì´ë¯¸ êµ¬í˜„ë¨**

---

## 6. í˜„ì¬ ë¬¸ì œ í•´ê²° ë°©ì•ˆ

### **6.1. Iteration #9 íŠ¹ì´í–‰ë ¬ ë¬¸ì œ**

**ë¬¸ì œ**: BFGS Hessianì´ íŠ¹ì´í–‰ë ¬ â†’ íƒìƒ‰ ë°©í–¥ ê³„ì‚° ë¶ˆê°€

**í•´ê²° ë°©ì•ˆ 1: Trust Region ë°©ë²•**
```python
result = optimize.minimize(
    func,
    x0,
    method='trust-ncg',  # Trust Region Newton-CG
    jac=grad_func,
    hess=bhhh_hessian_func,  # BHHH Hessian ì œê³µ
    options={'maxiter': 200}
)
```

**í•´ê²° ë°©ì•ˆ 2: BHHH Optimizer (Custom)**
- ìœ„ì˜ 5.1 ì°¸ì¡°
- ë§¤ iterationë§ˆë‹¤ BHHH Hessian ê³„ì‚°

**í•´ê²° ë°©ì•ˆ 3: Hessian Reset**
```python
# BFGS callbackì—ì„œ Hessian ë¦¬ì…‹
def callback(xk):
    if iteration % 5 == 0:
        # Hessianì„ Identityë¡œ ë¦¬ì…‹
        # (scipy BFGSëŠ” ì§ì ‘ ë¦¬ì…‹ ë¶ˆê°€ëŠ¥)
        pass
```

**ê¶Œì¥**: **Trust Region + BHHH Hessian**

---

## 7. ê²°ë¡ 

### **7.1. í˜¸í™˜ì„± ìš”ì•½**

| ì§ˆë¬¸ | ë‹µë³€ |
|------|------|
| BFGSë¥¼ BHHHë¡œ ëŒ€ì²´ ê°€ëŠ¥? | âŒ ì—­í• ì´ ë‹¤ë¦„ (ìµœì í™” vs Hessian ê·¼ì‚¬) |
| BFGS Hessianì„ BHHHë¡œ ëŒ€ì²´ ê°€ëŠ¥? | âœ… ê°€ëŠ¥ (ì´ë¯¸ êµ¬í˜„ë¨) |
| BHHHë¥¼ ìµœì í™”ì— ì‚¬ìš© ê°€ëŠ¥? | âœ… ê°€ëŠ¥ (Custom Optimizer í•„ìš”) |
| í˜„ì¬ êµ¬í˜„ í˜¸í™˜ì„±? | âœ… ì™„ì „ í˜¸í™˜ (ì¡°ê¸° ì¢…ë£Œ ì‹œ BHHH ì‚¬ìš©) |

### **7.2. ê¶Œì¥ ì‚¬í•­**

**í˜„ì¬ ìƒí™© (Iteration #9 íŠ¹ì´í–‰ë ¬)**:

1. âœ… **Trust Region + BHHH Hessian** (ê°€ì¥ ì•ˆì •ì )
   ```python
   method='trust-ncg'
   hess=bhhh_hessian_func
   ```

2. âœ… **L-BFGS-B + BHHH í‘œì¤€ì˜¤ì°¨** (í˜„ì¬ ë°©ì‹ ìœ ì§€)
   ```python
   method='L-BFGS-B'
   # ì¡°ê¸° ì¢…ë£Œ í›„ BHHHë¡œ í‘œì¤€ì˜¤ì°¨ ê³„ì‚°
   ```

3. âš ï¸ **Custom BHHH Optimizer** (êµ¬í˜„ í•„ìš”, ê³„ì‚° ë¹„ìš© ë†’ìŒ)

**ìµœì¢… ê¶Œì¥**: **Option 1 (Trust Region + BHHH)**

### **7.3. êµ¬í˜„ ë‚œì´ë„**

| ë°©ë²• | êµ¬í˜„ ë‚œì´ë„ | ê³„ì‚° ë¹„ìš© | ì•ˆì •ì„± |
|------|------------|----------|--------|
| Trust Region + BHHH | â˜…â˜…â˜†â˜†â˜† (ì‰¬ì›€) | â˜…â˜…â˜…â˜†â˜† (ì¤‘ê°„) | â˜…â˜…â˜…â˜…â˜… (ìµœê³ ) |
| L-BFGS-B + BHHH SE | â˜…â˜†â˜†â˜†â˜† (ë§¤ìš° ì‰¬ì›€) | â˜…â˜…â˜†â˜†â˜† (ë‚®ìŒ) | â˜…â˜…â˜…â˜†â˜† (ì¤‘ê°„) |
| Custom BHHH Optimizer | â˜…â˜…â˜…â˜…â˜† (ì–´ë ¤ì›€) | â˜…â˜…â˜…â˜…â˜† (ë†’ìŒ) | â˜…â˜…â˜…â˜…â˜… (ìµœê³ ) |

**ë‹¤ìŒ ë‹¨ê³„**: Trust Region ë°©ë²• êµ¬í˜„ ì‹œë„?

<function_calls>
<invoke name="view">
<parameter name="path">src/analysis/hybrid_choice_model/iclv_models/simultaneous_estimator_fixed.py
