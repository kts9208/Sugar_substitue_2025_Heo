# Hessian 특이행렬 발생 원인 분석

## 1. 문제 발생 시점

**Iteration #8 → #9 전환 시 Hessian이 특이행렬(singular matrix)이 됨**

```
Iteration #8:
  - LL: -44465.6290
  - Line Search: 1회 함수 호출 - ⚠️ 정체 (함수값 변화 없음)
  - 탐색 방향 d norm: 0.000000e+00  ← 문제!

Iteration #9:
  - LL: -3260000000000.0000  ← 오버플로우!
  - NaN 발생으로 계산 실패
```

---

## 2. Iteration별 Hessian 업데이트 정보 비교

### **Iteration #6 → #7**

```
Hessian 업데이트:
  - s_k norm: 1.325726e-01
  - y_k norm: 4.919145e+02
  - s_k^T · y_k: 2.296410e+01 (양수 ✓)
  - ρ = 1/(s_k^T · y_k): 4.354623e-02
  - ||y_k|| / ||s_k||: 3,710

결과:
  - LL 개선: -45800.21 → -45696.71 (+103.5)
  - 탐색 방향 norm: 2.421417e-01 ✓
```

### **Iteration #7 → #8**

```
Hessian 업데이트:
  - s_k norm: 3.026772e-01
  - y_k norm: 1.064617e+03
  - s_k^T · y_k: 1.317678e+02 (양수 ✓)
  - ρ = 1/(s_k^T · y_k): 7.589108e-03
  - ||y_k|| / ||s_k||: 3,517

결과:
  - LL 개선: -45468.93 → -44465.63 (+1003.3) ← 큰 개선!
  - 탐색 방향 norm: 정상 ✓
```

### **Iteration #8 → #9 (실패)**

```
Hessian 업데이트:
  - s_k norm: 5.115830e-01
  - y_k norm: 1.529526e+03
  - s_k^T · y_k: 4.336153e+02 (양수 ✓)
  - ρ = 1/(s_k^T · y_k): 2.306192e-03
  - ||y_k|| / ||s_k||: 2,990

결과:
  - LL 개선: 없음 (정체)
  - 탐색 방향 norm: 0.000000e+00 ← 특이행렬!
```

---

## 3. 핵심 문제: Iteration #7 → #8의 극단적 파라미터 변화

### **선택모델 파라미터의 급격한 변화**

| 파라미터 | Iter #7 | Iter #8 | 변화량 | 변화율 |
|---------|---------|---------|--------|--------|
| **lambda_mod_nutrition_knowledge** | +2.089963 | +2.438751 | **+0.349** | **+16.7%** |
| **beta_intercept** | -0.787020 | -0.981876 | **-0.195** | **+24.8%** |
| **sigma_sq_perceived_price_q27** | 1.269889 | 1.088399 | **-0.181** | **-14.3%** |
| **sigma_sq_perceived_price_q29** | 1.222087 | 1.030505 | **-0.192** | **-15.7%** |
| **sigma_sq_purchase_intention_q18** | 1.878043 | 2.091538 | **+0.213** | **+11.4%** |

**총 파라미터 변화 norm: 0.512** (이전 iteration의 2-4배!)

### **Gradient의 극단적 변화**

| 파라미터 | Iter #7 Grad | Iter #8 Grad | Grad 변화 | 변화율 |
|---------|--------------|--------------|-----------|--------|
| **lambda_mod_perceived_price** | +4796.97 | +4329.11 | **-467.86** | **-9.8%** |
| **lambda_main** | +2529.57 | +2427.87 | **-101.70** | **-4.0%** |
| **beta_intercept** | +1791.93 | +1532.94 | **-258.99** | **-14.5%** |
| **beta_price** | +1350.49 | +867.71 | **-482.78** | **-35.7%** |

**Gradient 변화 norm: 1529.53** (매우 큼!)

---

## 4. 왜 Hessian이 특이행렬이 되었나?

### **4.1. BFGS 업데이트 공식**

```
H_{k+1} = (I - ρ_k s_k y_k^T) H_k (I - ρ_k y_k s_k^T) + ρ_k s_k s_k^T

where:
  - s_k = x_{k+1} - x_k  (파라미터 변화)
  - y_k = ∇f_{k+1} - ∇f_k  (gradient 변화)
  - ρ_k = 1 / (s_k^T · y_k)
```

### **4.2. 문제 발생 메커니즘**

**Step 1: Iteration #7 → #8에서 큰 LL 개선 (+1003.3)**
- 파라미터가 크게 변화 (s_k norm = 0.512)
- Gradient도 크게 변화 (y_k norm = 1529.5)
- 하지만 **s_k^T · y_k = 433.6**으로 상대적으로 작음

**Step 2: ρ_k = 1/433.6 = 0.0023 (매우 작음)**
- ρ_k가 작다 = s_k와 y_k가 거의 직교
- **의미**: 파라미터 변화 방향과 gradient 변화 방향이 일치하지 않음!

**Step 3: Hessian 업데이트 시 수치 오차 누적**
```
H_{k+1} ≈ H_k - (작은 수정항)
```
- ρ_k가 작아서 Hessian이 거의 업데이트되지 않음
- 하지만 실제로는 **큰 파라미터 변화**가 있었음
- **Hessian과 실제 곡률의 불일치** 발생

**Step 4: Iteration #8에서 Line Search 실패**
- Hessian이 잘못된 탐색 방향 제시
- Line search가 함수값 개선을 찾지 못함
- **파라미터 변화 없음** (s_k = 0)

**Step 5: Iteration #9에서 특이행렬**
```
s_k = 0  →  s_k^T · y_k = 0  →  ρ_k = ∞
```
- BFGS 업데이트 불가능
- Hessian이 특이행렬이 됨
- 탐색 방향 d = -H^{-1} g를 계산할 수 없음

---

## 5. 근본 원인: Ill-Conditioned Problem

### **5.1. Gradient 불균형**

**Iteration #8 Gradient 분석:**

| 파라미터 그룹 | Gradient 범위 | 평균 크기 |
|--------------|--------------|----------|
| **선택모델 lambda** | 867 ~ 4329 | **2,542** |
| **선택모델 beta** | 29 ~ 1533 | **497** |
| **측정모델 sigma_sq** | 349 ~ 2,309 | **1,100** |
| **측정모델 zeta** | 66 ~ 905 | **450** |
| **구조모델 gamma** | 79 ~ 136 | **108** |

**Gradient 비율: 최대/최소 = 4329 / 66 = 65.6배!**

### **5.2. Hessian 조건수 추정**

```
κ(H) ≈ (최대 gradient / 최소 gradient)² 
     ≈ 65.6² 
     ≈ 4,300

조건수 > 1000 → Ill-conditioned!
```

**의미**: Hessian의 고유값이 4,300배 차이 → 역행렬 계산 시 수치 오차 심각

### **5.3. 파라미터 스케일 불균형**

| 파라미터 | 값 범위 | 스케일 |
|---------|--------|--------|
| **lambda_mod_perceived_price** | -2.32 | O(1) |
| **lambda_mod_nutrition_knowledge** | +2.44 | O(1) |
| **sigma_sq_nutrition_knowledge_q35** | 0.347 | O(0.1) |
| **sigma_sq_purchase_intention_q18** | 2.092 | O(1) |

**파라미터 스케일은 비교적 균일하지만, Gradient 스케일이 매우 불균일!**

---

## 6. 왜 Iteration #7 → #8에서 큰 개선이 있었나?

### **6.1. 파라미터 변화 분석**

**가장 큰 변화 (상위 5개):**
```
[79] lambda_mod_nutrition_knowledge: +0.349 (+16.7%)
[73] beta_intercept: -0.195 (+24.8%)
[24] sigma_sq_perceived_price_q27: -0.181 (-14.3%)
[26] sigma_sq_perceived_price_q29: -0.192 (-15.7%)
[68] sigma_sq_purchase_intention_q18: +0.213 (+11.4%)
```

**의미**: 
- **조절효과 파라미터** (lambda_mod_nutrition_knowledge)가 크게 증가
- **선택모델 절편** (beta_intercept)이 더 음수로 이동
- **가격 인식 측정오차** (sigma_sq_perceived_price)가 감소
- **구매의도 측정오차** (sigma_sq_purchase_intention)가 증가

### **6.2. 왜 이것이 LL을 크게 개선했나?**

**가설 1: 조절효과의 올바른 방향 발견**
- `lambda_mod_nutrition_knowledge`가 +2.09 → +2.44로 증가
- 영양지식이 높을수록 구매의도의 선택 영향이 **더 강해짐**
- 이것이 데이터를 더 잘 설명

**가설 2: 측정모델 개선**
- 가격 인식의 측정오차 감소 → 잠재변수 추정 정확도 향상
- 구매의도의 측정오차 증가 → 과적합 방지

**가설 3: 선택모델 절편 조정**
- beta_intercept가 더 음수로 → opt-out 확률 증가
- 실제 데이터에서 opt-out 비율이 높을 가능성

---

## 7. 결론

### **7.1. 특이행렬 발생 원인**

1. ✅ **Gradient 불균형**: 선택모델 lambda 파라미터의 gradient가 다른 파라미터보다 10-100배 큼
2. ✅ **Ill-conditioned Hessian**: 조건수 ~4,300으로 매우 높음
3. ✅ **BFGS 누적 오차**: 8번의 업데이트 후 Hessian 근사가 실제 곡률과 크게 불일치
4. ✅ **Line Search 실패**: Iteration #8에서 함수값 개선 실패 → s_k = 0
5. ✅ **ρ_k → ∞**: s_k = 0이므로 BFGS 업데이트 불가능 → 특이행렬

### **7.2. 왜 갑자기 발생했나?**

**Iteration #7 → #8에서 "운이 좋게" 큰 개선을 얻었지만:**
- 파라미터 변화 방향과 gradient 변화 방향이 거의 직교 (s_k^T · y_k 작음)
- Hessian 근사가 실제 곡률을 제대로 반영하지 못함
- 다음 iteration에서 잘못된 탐색 방향 제시
- Line search 실패 → 파라미터 변화 없음 → 특이행렬

**비유**: 
- 산을 오르다가 갑자기 큰 바위를 발견하고 우회했더니 정상에 가까워짐
- 하지만 지도(Hessian)는 바위를 모르고 직진하라고 함
- 다음 단계에서 지도를 따라가니 절벽에 막힘
- 더 이상 갈 곳이 없어서 멈춤

### **7.3. 해결 방안**

**즉시 적용 가능:**
1. **Hessian Reset**: Iteration 5, 10, 15마다 H = I로 초기화
2. **L-BFGS**: 제한된 메모리 BFGS (최근 m개 업데이트만 사용)
3. **Trust Region**: BFGS 대신 trust-ncg 또는 trust-krylov 사용

**근본적 해결:**
1. **파라미터 스케일링**: Lambda 파라미터에 대한 적응적 스케일링
2. **Gradient Clipping**: Gradient가 1000 이상이면 clipping
3. **Sequential Estimation**: 측정모델 → 구조모델 → 선택모델 순차 추정

---

## 8. 추가 분석: s_k와 y_k의 직교성

### **Iteration #8 → #9**

```
s_k norm: 5.115830e-01
y_k norm: 1.529526e+03
s_k^T · y_k: 4.336153e+02

코사인 유사도 = (s_k^T · y_k) / (||s_k|| × ||y_k||)
              = 433.6 / (0.512 × 1529.5)
              = 433.6 / 782.3
              = 0.554
```

**의미**: 
- 코사인 유사도 = 0.554 (완전 일치 = 1.0, 직교 = 0.0)
- s_k와 y_k가 **약 56.4도 각도**로 벌어짐
- 이상적으로는 거의 평행해야 함 (BFGS 가정)
- **BFGS 가정 위배** → Hessian 근사 부정확

### **비교: Iteration #7 → #8**

```
s_k norm: 3.026772e-01
y_k norm: 1.064617e+03
s_k^T · y_k: 1.317678e+02

코사인 유사도 = 131.8 / (0.303 × 1064.6)
              = 131.8 / 322.6
              = 0.409
```

**더 나쁨!** 코사인 유사도 = 0.409 (약 65.8도 각도)

---

## 9. 최종 진단

**문제의 본질**: 
- ICLV 모델의 **파라미터 공간이 매우 복잡**
- 측정모델, 구조모델, 선택모델이 **강하게 결합**
- 선택모델의 조절효과 파라미터가 **고도로 비선형**
- BFGS가 이러한 복잡한 곡률을 근사하기에 **부적합**

**권장 해결책**:
1. **Trust Region 방법** (가장 안정적)
2. **L-BFGS with Hessian Reset** (중간 안정성)
3. **파라미터 스케일링 + BFGS** (현재 방법 개선)

