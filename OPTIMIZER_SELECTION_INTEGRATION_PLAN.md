# Optimizer ì„ ì • ë¡œì§ í†µí•© ë°©ì•ˆ

**ë‚ ì§œ**: 2025-11-23  
**ë¬¸ì œ**: Optimizer ì„ ì •ì´ ì—¬ëŸ¬ ê³³ì—ì„œ ì´ë£¨ì–´ì ¸ ì¶©ëŒ ë°œìƒ

---

## ğŸ“‹ ë¬¸ì œ ë¶„ì„

### 1. í˜„ì¬ ë¬¸ì œì 

**ì¦ìƒ**:
- Configì—ì„œ `optimizer='trust-constr'` ì„¤ì •
- ì‹¤ì œ ì‹¤í–‰ ì‹œ `Nelder-Mead` ì‚¬ìš©ë¨

**ì›ì¸**:
```python
# Line 432, 706
use_gradient = self.config.estimation.optimizer in ['BFGS', 'L-BFGS-B', 'BHHH']
```

- `trust-constr`ëŠ” ë¦¬ìŠ¤íŠ¸ì— ì—†ìŒ â†’ `use_gradient = False`
- `use_gradient = False` â†’ Nelder-Mead ì‚¬ìš© (Line 1620-1634)

---

### 2. Optimizer ì„ ì • ë¡œì§ ìœ„ì¹˜

| ìœ„ì¹˜ | ë¼ì¸ | ì—­í•  | ë¬¸ì œ |
|------|------|------|------|
| **Line 432** | `use_gradient = ...` | Gradient calculator ì´ˆê¸°í™” ì—¬ë¶€ ê²°ì • | âŒ Trust Region ë¯¸í¬í•¨ |
| **Line 706** | `use_gradient = ...` | Optimizer ë¶„ê¸° ê²°ì • | âŒ Trust Region ë¯¸í¬í•¨ |
| **Line 1231** | `if use_gradient:` | Gradient-based vs Gradient-free ë¶„ê¸° | âŒ ì˜ëª»ëœ ë¶„ê¸° |
| **Line 1291-1392** | Optimizerë³„ ë¶„ê¸° | ì‹¤ì œ optimize.minimize() í˜¸ì¶œ | âœ… ì •ìƒ (else ë¶„ê¸° ì¡´ì¬) |

---

## ğŸ”§ í†µí•© ë°©ì•ˆ

### ë°©ì•ˆ 1: Gradient-based Optimizer ë¦¬ìŠ¤íŠ¸ í™•ì¥ (ê¶Œì¥)

**í•µì‹¬ ì•„ì´ë””ì–´**:
- `use_gradient` ê²°ì • ë¡œì§ì„ **Gradient-free optimizer ë¦¬ìŠ¤íŠ¸**ë¡œ ë³€ê²½
- Trust Region, Newton-CG ë“± ëª¨ë“  gradient-based optimizer ìë™ ì§€ì›

**ìˆ˜ì • ìœ„ì¹˜**: Line 432, 706

**Before**:
```python
use_gradient = self.config.estimation.optimizer in ['BFGS', 'L-BFGS-B', 'BHHH']
```

**After**:
```python
# Gradient-free optimizer ë¦¬ìŠ¤íŠ¸ (ëª…ì‹œì )
GRADIENT_FREE_OPTIMIZERS = ['Nelder-Mead', 'Powell', 'COBYLA']

# Gradient-based optimizer ìë™ íŒë‹¨
use_gradient = self.config.estimation.optimizer not in GRADIENT_FREE_OPTIMIZERS
```

**ì¥ì **:
- âœ… ìƒˆë¡œìš´ gradient-based optimizer ì¶”ê°€ ì‹œ ìë™ ì§€ì›
- âœ… ëª…ì‹œì ì´ê³  ìœ ì§€ë³´ìˆ˜ ì‰¬ì›€
- âœ… Trust Region, Newton-CG, SLSQP ë“± ëª¨ë‘ ìë™ ì§€ì›

**ë‹¨ì **:
- âš ï¸ ì˜ëª»ëœ optimizer ì´ë¦„ ì…ë ¥ ì‹œ gradient-basedë¡œ ê°„ì£¼ë¨

---

### ë°©ì•ˆ 2: Optimizer íƒ€ì… ëª…ì‹œì  ì •ì˜

**í•µì‹¬ ì•„ì´ë””ì–´**:
- Optimizerë¥¼ íƒ€ì…ë³„ë¡œ ëª…ì‹œì ìœ¼ë¡œ ë¶„ë¥˜
- Configì—ì„œ optimizer íƒ€ì… ê²€ì¦

**ìˆ˜ì • ìœ„ì¹˜**: `iclv_config.py`, Line 432, 706

**EstimationConfig ìˆ˜ì •**:
```python
# iclv_config.py
GRADIENT_BASED_OPTIMIZERS = [
    'BFGS', 'L-BFGS-B', 'BHHH', 
    'trust-constr', 'trust-ncg', 'trust-exact', 'trust-krylov',
    'Newton-CG', 'CG', 'SLSQP', 'dogleg'
]

GRADIENT_FREE_OPTIMIZERS = [
    'Nelder-Mead', 'Powell', 'COBYLA'
]

@dataclass
class EstimationConfig:
    optimizer: str = 'BFGS'
    
    def __post_init__(self):
        """Optimizer ê²€ì¦"""
        all_optimizers = GRADIENT_BASED_OPTIMIZERS + GRADIENT_FREE_OPTIMIZERS
        if self.optimizer not in all_optimizers:
            raise ValueError(
                f"Unknown optimizer: {self.optimizer}. "
                f"Supported: {all_optimizers}"
            )
    
    def is_gradient_based(self) -> bool:
        """Gradient-based optimizer ì—¬ë¶€"""
        return self.optimizer in GRADIENT_BASED_OPTIMIZERS
```

**simultaneous_estimator_fixed.py ìˆ˜ì •**:
```python
# Line 432, 706
use_gradient = self.config.estimation.is_gradient_based()
```

**ì¥ì **:
- âœ… ê°€ì¥ ëª…ì‹œì ì´ê³  ì•ˆì „
- âœ… Optimizer ê²€ì¦ ìë™í™”
- âœ… ì˜ëª»ëœ optimizer ì´ë¦„ ì¡°ê¸° ë°œê²¬

**ë‹¨ì **:
- âš ï¸ ìƒˆ optimizer ì¶”ê°€ ì‹œ ë¦¬ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ í•„ìš”

---

### ë°©ì•ˆ 3: Scipy Optimizer ë©”íƒ€ë°ì´í„° í™œìš©

**í•µì‹¬ ì•„ì´ë””ì–´**:
- Scipyì˜ optimizer ì •ë³´ë¥¼ ë™ì ìœ¼ë¡œ í™•ì¸
- Gradient í•„ìš” ì—¬ë¶€ ìë™ íŒë‹¨

**ìˆ˜ì • ìœ„ì¹˜**: Line 432, 706

**êµ¬í˜„**:
```python
def requires_gradient(optimizer_name: str) -> bool:
    """
    Optimizerê°€ gradientë¥¼ í•„ìš”ë¡œ í•˜ëŠ”ì§€ í™•ì¸
    
    Scipyì˜ minimize() í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜ë¥¼ í™•ì¸í•˜ì—¬ íŒë‹¨
    """
    # Gradient-free optimizer (ëª…ì‹œì )
    gradient_free = ['Nelder-Mead', 'Powell', 'COBYLA']
    
    if optimizer_name in gradient_free:
        return False
    
    # ë‚˜ë¨¸ì§€ëŠ” ëª¨ë‘ gradient-basedë¡œ ê°„ì£¼
    # (Trust Region, BFGS, L-BFGS-B, Newton-CG, CG, SLSQP ë“±)
    return True

# Line 432, 706
use_gradient = requires_gradient(self.config.estimation.optimizer)
```

**ì¥ì **:
- âœ… ê°„ë‹¨í•˜ê³  í™•ì¥ ê°€ëŠ¥
- âœ… ìƒˆ optimizer ìë™ ì§€ì›

**ë‹¨ì **:
- âš ï¸ Scipy ë²„ì „ ë³€ê²½ ì‹œ ì˜í–¥ ë°›ì„ ìˆ˜ ìˆìŒ

---

## ğŸ¯ ê¶Œì¥ ë°©ì•ˆ: **ë°©ì•ˆ 2 (ëª…ì‹œì  ì •ì˜)**

**ì´ìœ **:
1. âœ… **ì•ˆì „ì„±**: Optimizer ê²€ì¦ìœ¼ë¡œ ì˜¤íƒ€ ë°©ì§€
2. âœ… **ëª…ì‹œì„±**: ì§€ì›í•˜ëŠ” optimizer ëª…í™•íˆ ë¬¸ì„œí™”
3. âœ… **ìœ ì§€ë³´ìˆ˜**: ìƒˆ optimizer ì¶”ê°€ ì‹œ í•œ ê³³ë§Œ ìˆ˜ì •
4. âœ… **í™•ì¥ì„±**: í–¥í›„ optimizerë³„ íŠ¹ìˆ˜ ì²˜ë¦¬ ê°€ëŠ¥

---

## ğŸ“ êµ¬í˜„ ë‹¨ê³„

### Step 1: `iclv_config.py` ìˆ˜ì •

**íŒŒì¼**: `src/analysis/hybrid_choice_model/iclv_models/iclv_config.py`

**ì¶”ê°€ ë‚´ìš©** (Line 150 ì´ì „):
```python
# Optimizer ë¶„ë¥˜
GRADIENT_BASED_OPTIMIZERS = [
    # Quasi-Newton methods
    'BFGS', 'L-BFGS-B',
    
    # Newton methods
    'Newton-CG', 'CG',
    
    # Trust Region methods
    'trust-constr', 'trust-ncg', 'trust-exact', 'trust-krylov', 'dogleg',
    
    # Sequential Quadratic Programming
    'SLSQP',
    
    # Custom methods
    'BHHH'  # Berndt-Hall-Hall-Hausman (Newton-CG with OPG Hessian)
]

GRADIENT_FREE_OPTIMIZERS = [
    'Nelder-Mead',  # Simplex method
    'Powell',       # Powell's method
    'COBYLA'        # Constrained Optimization BY Linear Approximation
]
```

**EstimationConfig ìˆ˜ì •** (Line 153-195):
```python
@dataclass
class EstimationConfig:
    """ì¶”ì • ì„¤ì •"""
    
    # ì¶”ì • ë°©ë²•
    method: Literal['simultaneous', 'sequential'] = 'simultaneous'
    
    # ì‹œë®¬ë ˆì´ì…˜ ì„¤ì •
    n_draws: int = 1000
    draw_type: Literal['halton', 'random', 'mlhs'] = 'halton'
    scramble_halton: bool = True
    
    # ìµœì í™” ì„¤ì •
    optimizer: str = 'BFGS'
    max_iterations: int = 2000
    convergence_tolerance: float = 1e-6

    # Gradient ì„¤ì • (Apollo ë°©ì‹)
    use_analytic_gradient: bool = True  # True: analytic gradient, False: numerical gradient

    # Parameter Scaling ì„¤ì •
    use_parameter_scaling: bool = True  # True: parameter scaling í™œì„±í™”, False: ë¹„í™œì„±í™”

    # Data Standardization ì„¤ì •
    standardize_choice_attributes: bool = True
    
    # Gradient ë¡œê¹… ì„¤ì •
    gradient_log_level: Literal['MINIMAL', 'MODERATE', 'DETAILED'] = 'DETAILED'

    # ë³‘ë ¬ì²˜ë¦¬ ì„¤ì •
    use_parallel: bool = False
    n_cores: Optional[int] = None

    # í‘œì¤€ì˜¤ì°¨ ê³„ì‚°
    calculate_se: bool = True
    se_method: Literal['hessian', 'bootstrap', 'robust'] = 'hessian'

    # ë¶€íŠ¸ìŠ¤íŠ¸ë© ì„¤ì •
    n_bootstrap: int = 500

    # ì¡°ê¸° ì¢…ë£Œ ì„¤ì •
    early_stopping: bool = False
    early_stopping_patience: int = 5
    early_stopping_tol: float = 1e-6
    
    def __post_init__(self):
        """Optimizer ê²€ì¦"""
        all_optimizers = GRADIENT_BASED_OPTIMIZERS + GRADIENT_FREE_OPTIMIZERS
        if self.optimizer not in all_optimizers:
            import warnings
            warnings.warn(
                f"Unknown optimizer: '{self.optimizer}'. "
                f"Supported optimizers: {all_optimizers}. "
                f"Assuming gradient-based optimizer."
            )
    
    def is_gradient_based(self) -> bool:
        """Gradient-based optimizer ì—¬ë¶€ í™•ì¸"""
        return self.optimizer not in GRADIENT_FREE_OPTIMIZERS
```

---

### Step 2: `simultaneous_estimator_fixed.py` ìˆ˜ì •

**íŒŒì¼**: `src/analysis/hybrid_choice_model/iclv_models/simultaneous_estimator_fixed.py`

**ìˆ˜ì • 1**: Line 432
```python
# Before
use_gradient = self.config.estimation.optimizer in ['BFGS', 'L-BFGS-B', 'BHHH']

# After
use_gradient = self.config.estimation.is_gradient_based()
```

**ìˆ˜ì • 2**: Line 706
```python
# Before
use_gradient = self.config.estimation.optimizer in ['BFGS', 'L-BFGS-B', 'BHHH']

# After
use_gradient = self.config.estimation.is_gradient_based()
```

---

## âœ… ê²€ì¦ ë°©ë²•

### 1. Trust Region í…ŒìŠ¤íŠ¸
```python
config = create_sugar_substitute_multi_lv_config(
    optimizer='trust-constr',
    use_analytic_gradient=True,
    se_method='robust'
)

# ì˜ˆìƒ ê²°ê³¼:
# - use_gradient = True
# - Analytic gradient ì´ˆê¸°í™”
# - Trust Region optimizer ì‚¬ìš©
# - Sandwich Estimator ê³„ì‚°
```

### 2. Nelder-Mead í…ŒìŠ¤íŠ¸
```python
config = create_sugar_substitute_multi_lv_config(
    optimizer='Nelder-Mead'
)

# ì˜ˆìƒ ê²°ê³¼:
# - use_gradient = False
# - Gradient calculator ì´ˆê¸°í™” ì•ˆ í•¨
# - Nelder-Mead optimizer ì‚¬ìš©
```

### 3. ì˜ëª»ëœ Optimizer í…ŒìŠ¤íŠ¸
```python
config = create_sugar_substitute_multi_lv_config(
    optimizer='INVALID_OPTIMIZER'
)

# ì˜ˆìƒ ê²°ê³¼:
# - Warning ë©”ì‹œì§€ ì¶œë ¥
# - Gradient-basedë¡œ ê°„ì£¼ (fallback)
```

---

## ğŸ“Š ì§€ì› Optimizer ëª©ë¡

| Optimizer | íƒ€ì… | Gradient | Hessian | Bounds | Constraints |
|-----------|------|----------|---------|--------|-------------|
| **BFGS** | Quasi-Newton | âœ… | Approx | âŒ | âŒ |
| **L-BFGS-B** | Quasi-Newton | âœ… | Approx | âœ… | âŒ |
| **trust-constr** | Trust Region | âœ… | Approx | âœ… | âœ… |
| **trust-ncg** | Trust Region | âœ… | âœ… | âŒ | âŒ |
| **Newton-CG** | Newton | âœ… | âœ… | âŒ | âŒ |
| **SLSQP** | SQP | âœ… | Approx | âœ… | âœ… |
| **BHHH** | Custom | âœ… | OPG | âŒ | âŒ |
| **Nelder-Mead** | Simplex | âŒ | âŒ | âœ… | âŒ |
| **Powell** | Direction Set | âŒ | âŒ | âœ… | âŒ |
| **COBYLA** | Linear Approx | âŒ | âŒ | âœ… | âœ… |

---

**ë¶„ì„ ì™„ë£Œ ì¼ì‹œ**: 2025-11-23

