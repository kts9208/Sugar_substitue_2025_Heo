"""
GPU ë°°ì¹˜ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°

Analytic gradientë¥¼ GPU ë°°ì¹˜ë¡œ ê³„ì‚°í•˜ì—¬ ì†ë„ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

CPU êµ¬í˜„ (multi_latent_gradient.py)ì˜ ë¡œì§ì„ ë”°ë¥´ë©´ì„œ:
1. Importance weighting êµ¬í˜„ (Apollo ë°©ì‹)
2. ëª¨ë“  ì„ íƒ ìƒí™© ì²˜ë¦¬
3. Likelihood ê³„ì‚° í›„ ê°€ì¤‘í‰ê· 
4. ìˆ˜ì¹˜ ì•ˆì •ì„± ê°•í™”
5. GPU ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì„±ëŠ¥ í–¥ìƒ
"""

import numpy as np
import pandas as pd
from typing import Dict, List
import logging

logger = logging.getLogger(__name__)

try:
    import cupy as cp
    from cupyx.scipy.special import ndtr as cp_ndtr
    CUPY_AVAILABLE = True

    # CuPyì—ëŠ” norm.pdfê°€ ì—†ìœ¼ë¯€ë¡œ ì§ì ‘ êµ¬í˜„
    def cp_norm_pdf(x):
        """í‘œì¤€ì •ê·œë¶„í¬ PDF: Ï†(x) = (1/âˆš(2Ï€)) * exp(-xÂ²/2)"""
        return cp.exp(-0.5 * x**2) / cp.sqrt(2 * cp.pi)

    def log_sum_exp_gpu(log_values):
        """
        Log-sum-exp trick for numerical stability
        log(Î£ exp(x_i)) = max(x) + log(Î£ exp(x_i - max(x)))
        """
        max_val = cp.max(log_values)
        return max_val + cp.log(cp.sum(cp.exp(log_values - max_val)))

except ImportError:
    CUPY_AVAILABLE = False
    logger.warning("CuPy not available. GPU gradient acceleration disabled.")
    cp_norm_pdf = None
    log_sum_exp_gpu = None


def compute_measurement_grad_wrt_lv_gpu(
    gpu_measurement_model,
    ind_data: pd.DataFrame,
    lvs_list: List[Dict[str, float]],
    params_measurement: Dict,
    target_lv: str
) -> np.ndarray:
    """
    ì¸¡ì •ëª¨ë¸ ìš°ë„ì˜ ì ì¬ë³€ìˆ˜ì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°

    âˆ‚LL_measurement/âˆ‚LV for each draw

    Args:
        gpu_measurement_model: GPU ì¸¡ì •ëª¨ë¸
        ind_data: ê°œì¸ ë°ì´í„°
        lvs_list: ê° drawì˜ ì ì¬ë³€ìˆ˜ ê°’
        params_measurement: ì¸¡ì •ëª¨ë¸ íŒŒë¼ë¯¸í„°
        target_lv: ëŒ€ìƒ ì ì¬ë³€ìˆ˜ ì´ë¦„

    Returns:
        ê° drawì˜ âˆ‚LL_measurement/âˆ‚LV (n_draws,)
    """
    if not CUPY_AVAILABLE:
        raise RuntimeError("CuPy not available")

    n_draws = len(lvs_list)
    grad_ll_wrt_lv = cp.zeros(n_draws)

    # ëŒ€ìƒ LVì˜ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
    if target_lv not in params_measurement:
        return cp.asnumpy(grad_ll_wrt_lv)

    lv_params = params_measurement[target_lv]
    zeta = lv_params['zeta']
    n_indicators = len(zeta)

    # ì¸¡ì • ë°©ë²• í™•ì¸
    measurement_method = getattr(gpu_measurement_model.models[target_lv], 'measurement_method', 'ordered_probit')
    config = gpu_measurement_model.models[target_lv].config

    # LV ê°’ë“¤ì„ ë°°ì—´ë¡œ ë³€í™˜
    lv_values = np.array([lvs[target_lv] for lvs in lvs_list])
    lv_values_gpu = cp.asarray(lv_values)  # (n_draws,)
    zeta_gpu = cp.asarray(zeta)  # (n_indicators,)

    # ì²« ë²ˆì§¸ í–‰ë§Œ ì‚¬ìš© (ì¸¡ì •ëª¨ë¸ì€ ê°œì¸ ìˆ˜ì¤€)
    row = ind_data.iloc[0]

    # ê° ì§€í‘œë³„ë¡œ âˆ‚LL/âˆ‚LV ê³„ì‚°
    for i, indicator in enumerate(config.indicators):
        if indicator not in row.index:
            continue

        y = row[indicator]
        if pd.isna(y):
            continue

        if measurement_method == 'continuous_linear':
            # Continuous Linear: Y = Î¶ * LV + Îµ
            # âˆ‚LL/âˆ‚LV = Î¶ * (y - Î¶*LV) / ÏƒÂ²
            sigma_sq = lv_params['sigma_sq']
            sigma_sq_gpu = cp.asarray(sigma_sq)

            y_pred = zeta_gpu[i] * lv_values_gpu
            residual = y - y_pred
            grad_ll_wrt_lv += zeta_gpu[i] * residual / sigma_sq_gpu[i]

        else:
            # Ordered Probit
            tau = lv_params['tau']
            tau_gpu = cp.asarray(tau)
            k = int(y) - 1

            V = zeta_gpu[i] * lv_values_gpu
            tau_i = tau_gpu[i]

            # P(Y=k) ê³„ì‚°
            if k == 0:
                prob = cp_ndtr(tau_i[0] - V)
                phi_upper = cp_norm_pdf(tau_i[0] - V)
                phi_lower = cp.zeros_like(V)
            elif k == config.n_categories - 1:
                prob = 1 - cp_ndtr(tau_i[-1] - V)
                phi_upper = cp.zeros_like(V)
                phi_lower = cp_norm_pdf(tau_i[-1] - V)
            else:
                prob = cp_ndtr(tau_i[k] - V) - cp_ndtr(tau_i[k-1] - V)
                phi_upper = cp_norm_pdf(tau_i[k] - V)
                phi_lower = cp_norm_pdf(tau_i[k-1] - V)

            prob = cp.clip(prob, 1e-10, 1 - 1e-10)

            # âˆ‚LL/âˆ‚LV = (Ï†_upper - Ï†_lower) / P * (-Î¶)
            grad_ll_wrt_lv += (phi_upper - phi_lower) / prob * (-zeta_gpu[i])

    return cp.asnumpy(grad_ll_wrt_lv)


def compute_choice_grad_wrt_lv_gpu(
    ind_data: pd.DataFrame,
    lvs_list: List[Dict[str, float]],
    params_choice: Dict,
    target_lv: str,
    choice_attributes: List[str]
) -> np.ndarray:
    """
    ì„ íƒëª¨ë¸ ìš°ë„ì˜ ì ì¬ë³€ìˆ˜ì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°

    âˆ‚LL_choice/âˆ‚LV for each draw

    Args:
        ind_data: ê°œì¸ ë°ì´í„°
        lvs_list: ê° drawì˜ ì ì¬ë³€ìˆ˜ ê°’
        params_choice: ì„ íƒëª¨ë¸ íŒŒë¼ë¯¸í„°
        target_lv: ëŒ€ìƒ ì ì¬ë³€ìˆ˜ ì´ë¦„
        choice_attributes: ì„ íƒ ì†ì„± ë¦¬ìŠ¤íŠ¸

    Returns:
        ê° drawì˜ âˆ‚LL_choice/âˆ‚LV (n_draws,)
    """
    if not CUPY_AVAILABLE:
        raise RuntimeError("CuPy not available")

    n_draws = len(lvs_list)
    n_choice_situations = len(ind_data)

    # Lambda íŒŒë¼ë¯¸í„° í™•ì¸
    lambda_key = f'lambda_{target_lv}'
    if lambda_key not in params_choice:
        # ì´ LVê°€ ì„ íƒëª¨ë¸ì— í¬í•¨ë˜ì§€ ì•ŠìŒ
        return np.zeros(n_draws)

    lambda_val = params_choice[lambda_key]
    intercept = params_choice['intercept']

    # Beta íŒŒë¼ë¯¸í„°
    if 'beta' in params_choice:
        beta = params_choice['beta']
    else:
        beta_keys = sorted([k for k in params_choice.keys() if k.startswith('beta_')])
        beta = np.array([params_choice[k] for k in beta_keys])

    # LV ê°’ë“¤
    lv_values = np.array([lvs[target_lv] for lvs in lvs_list])
    lv_values_gpu = cp.asarray(lv_values)  # (n_draws,)

    # ì„ íƒ ë³€ìˆ˜ ì°¾ê¸°
    choice_var = None
    for col in ['choice', 'chosen', 'choice_binary']:
        if col in ind_data.columns:
            choice_var = col
            break

    if choice_var is None:
        return np.zeros(n_draws)

    # ì†ì„± ë°ì´í„°ì™€ ì„ íƒ ì¤€ë¹„
    attributes_matrix = []
    choices = []

    for idx in range(n_choice_situations):
        row = ind_data.iloc[idx]
        attr_values = [row[attr] if attr in row.index and not pd.isna(row[attr]) else 0.0 for attr in choice_attributes]
        attributes_matrix.append(attr_values)
        choices.append(row[choice_var])

    attributes_matrix = np.array(attributes_matrix)
    choices = np.array(choices)

    # GPUë¡œ ì „ì†¡
    attr_gpu = cp.asarray(attributes_matrix)
    choices_gpu = cp.asarray(choices)
    beta_gpu = cp.asarray(beta)

    # V ê³„ì‚° (ë°°ì¹˜)
    attr_batch = attr_gpu[None, :, :]  # (1, n_situations, n_attributes)
    V_batch = intercept + cp.dot(attr_batch, beta_gpu[:, None]).squeeze(-1)  # (n_draws, n_situations)
    V_batch = V_batch + lambda_val * lv_values_gpu[:, None]  # (n_draws, n_situations)

    # Î¦(V), Ï†(V)
    prob_batch = cp_ndtr(V_batch)
    prob_batch = cp.clip(prob_batch, 1e-10, 1 - 1e-10)
    phi_batch = cp_norm_pdf(V_batch)

    # ì‹¤ì œ ì„ íƒì— ë”°ë¼
    prob_final_batch = cp.where(choices_gpu[None, :] == 1, prob_batch, 1 - prob_batch)

    # Mills ratio
    mills_batch = phi_batch / prob_final_batch
    sign_batch = cp.where(choices_gpu[None, :] == 1, 1.0, -1.0)

    # âˆ‚LL_choice/âˆ‚LV = Î£_situations (sign * mills * âˆ‚V/âˆ‚LV)
    # âˆ‚V/âˆ‚LV = lambda
    grad_ll_wrt_lv = cp.sum(sign_batch * mills_batch * lambda_val, axis=1)  # (n_draws,)

    return cp.asnumpy(grad_ll_wrt_lv)


def compute_joint_likelihood_batch_gpu(
    gpu_measurement_model,
    ind_data: pd.DataFrame,
    lvs_list: List[Dict[str, float]],
    draws: np.ndarray,
    params_dict: Dict,
    structural_model,
    choice_model
) -> np.ndarray:
    """
    ê° drawì˜ ê²°í•© likelihood ê³„ì‚° (importance weightingìš©)

    ê¸°ì¡´ gpu_batch_utilsì˜ í•¨ìˆ˜ë“¤ì„ í™œìš©í•©ë‹ˆë‹¤.

    Args:
        gpu_measurement_model: GPU ì¸¡ì •ëª¨ë¸
        ind_data: ê°œì¸ ë°ì´í„°
        lvs_list: ê° drawì˜ ì ì¬ë³€ìˆ˜ ê°’ ë¦¬ìŠ¤íŠ¸
        draws: ê°œì¸ì˜ draws (n_draws, n_dimensions)
        params_dict: ëª¨ë“  íŒŒë¼ë¯¸í„° {'measurement': ..., 'structural': ..., 'choice': ...}
        structural_model: êµ¬ì¡°ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
        choice_model: ì„ íƒëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤

    Returns:
        ê° drawì˜ log-likelihood ë°°ì—´ (n_draws,)
    """
    if not CUPY_AVAILABLE:
        raise RuntimeError("CuPy not available")

    # ê¸°ì¡´ GPU ìš°ë„ ê³„ì‚° í•¨ìˆ˜ë“¤ import
    from . import gpu_batch_utils

    # 1. ì¸¡ì •ëª¨ë¸ ìš°ë„ (ë°°ì¹˜)
    ll_measurement = gpu_batch_utils.compute_measurement_batch_gpu(
        gpu_measurement_model,
        ind_data,
        lvs_list,
        params_dict['measurement']
    )

    # 2. ì„ íƒëª¨ë¸ ìš°ë„ (ë°°ì¹˜)
    ll_choice = gpu_batch_utils.compute_choice_batch_gpu(
        ind_data,
        lvs_list,
        params_dict['choice'],
        choice_model
    )

    # 3. ê²°í•© ìš°ë„
    # âœ… êµ¬ì¡°ëª¨ë¸ ìš°ë„ëŠ” í¬í•¨í•˜ì§€ ì•ŠìŒ (êµ¬ì¡°ëª¨ë¸ì€ LV ìƒì„±ë§Œ ë‹´ë‹¹)
    ll_joint = ll_measurement + ll_choice

    return ll_joint


def compute_importance_weights_gpu(ll_batch: np.ndarray, individual_id: int = None) -> np.ndarray:
    """
    Importance weights ê³„ì‚° (Apollo ë°©ì‹)

    w_r = L_r / Î£_s L_s = exp(ll_r) / Î£_s exp(ll_s)

    Log-sum-exp trick ì‚¬ìš©í•˜ì—¬ ìˆ˜ì¹˜ ì•ˆì •ì„± í™•ë³´

    Args:
        ll_batch: ê° drawì˜ log-likelihood (n_draws,)
        individual_id: ê°œì¸ ID (ë””ë²„ê¹…ìš©)

    Returns:
        importance weights (n_draws,)
    """
    if not CUPY_AVAILABLE:
        raise RuntimeError("CuPy not available")

    ll_gpu = cp.asarray(ll_batch)

    # âœ… ë””ë²„ê¹…: likelihood ê°’ í™•ì¸
    ll_min = float(cp.min(ll_gpu))
    ll_max = float(cp.max(ll_gpu))
    ll_mean = float(cp.mean(ll_gpu))

    # NaN/Inf ì²´í¬ (ì…ë ¥ ë‹¨ê³„)
    if cp.any(cp.isnan(ll_gpu)):
        n_nan = int(cp.sum(cp.isnan(ll_gpu)))
        logger.error(f"[Individual {individual_id}] NaN detected in input LL batch ({n_nan}/{len(ll_batch)} draws)")
        logger.error(f"  LL range: [{ll_min:.2f}, {ll_max:.2f}], mean: {ll_mean:.2f}")
        logger.error(f"  First 10 LL values: {ll_batch[:10]}")
        raise ValueError(f"NaN detected in likelihood for individual {individual_id}. Cannot compute importance weights.")

    if cp.any(cp.isinf(ll_gpu)):
        n_inf = int(cp.sum(cp.isinf(ll_gpu)))
        logger.error(f"[Individual {individual_id}] Inf detected in input LL batch ({n_inf}/{len(ll_batch)} draws)")
        logger.error(f"  LL range: [{ll_min:.2f}, {ll_max:.2f}], mean: {ll_mean:.2f}")
        raise ValueError(f"Inf detected in likelihood for individual {individual_id}. Cannot compute importance weights.")

    # Log-sum-exp trick
    log_sum = log_sum_exp_gpu(ll_gpu)
    log_weights = ll_gpu - log_sum
    weights = cp.exp(log_weights)

    # ìˆ˜ì¹˜ ì•ˆì •ì„± ì²´í¬ (ì¶œë ¥ ë‹¨ê³„)
    if cp.any(cp.isnan(weights)):
        logger.error(f"[Individual {individual_id}] NaN detected in importance weights after exp()")
        logger.error(f"  Input LL range: [{ll_min:.2f}, {ll_max:.2f}], mean: {ll_mean:.2f}")
        logger.error(f"  log_sum: {float(log_sum)}")
        logger.error(f"  log_weights range: [{float(cp.min(log_weights)):.2f}, {float(cp.max(log_weights)):.2f}]")
        raise ValueError(f"NaN in importance weights for individual {individual_id} after normalization.")

    if cp.any(cp.isinf(weights)):
        logger.error(f"[Individual {individual_id}] Inf detected in importance weights after exp()")
        logger.error(f"  Input LL range: [{ll_min:.2f}, {ll_max:.2f}], mean: {ll_mean:.2f}")
        raise ValueError(f"Inf in importance weights for individual {individual_id} after normalization.")

    # ì •ê·œí™” í™•ì¸
    weight_sum = cp.sum(weights)
    if not cp.isclose(weight_sum, 1.0):
        logger.warning(f"[Individual {individual_id}] Weights sum to {float(weight_sum)}, renormalizing")
        weights = weights / weight_sum

    return cp.asnumpy(weights)


def compute_measurement_gradient_batch_gpu(
    gpu_measurement_model,
    ind_data: pd.DataFrame,
    lvs_list: List[Dict[str, float]],
    params: Dict[str, Dict],
    weights: np.ndarray,
    iteration_logger=None,
    log_level: str = 'DETAILED'
) -> Dict[str, Dict]:
    """
    ì¸¡ì •ëª¨ë¸ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ GPU ë°°ì¹˜ë¡œ ê³„ì‚° (ê°€ì¤‘í‰ê·  ì ìš©)

    ğŸ”´ SIGN PROTOCOL (Level 3 - Kernel):
    ==========================================
    This function computes and returns the POSITIVE GRADIENT (âˆ‡LL) - the ASCENT direction.

    Mathematical Formulas:
    ----------------------
    Continuous Linear:
        âˆ‚LL/âˆ‚Î¶_i = (y - Î¶*LV) * LV / ÏƒÂ²  [POSITIVE, ascent]
        âˆ‚LL/âˆ‚ÏƒÂ²_i = -0.5/ÏƒÂ² + 0.5*(y - Î¶*LV)Â²/Ïƒâ´  [POSITIVE, ascent]

    Ordered Probit:
        âˆ‚LL/âˆ‚Î¶_i = (Ï†_upper - Ï†_lower) / P * LV  [POSITIVE, ascent]
        âˆ‚LL/âˆ‚Ï„_k = (Ï†_upper or -Ï†_lower) / P  [POSITIVE, ascent]

    âš ï¸ CRITICAL: This function returns POSITIVE gradients (âˆ‡LL).
                 The negation to -âˆ‡LL (for minimization) happens ONLY at the top-level wrapper.

    Args:
        gpu_measurement_model: GPU ì¸¡ì •ëª¨ë¸
        ind_data: ê°œì¸ ë°ì´í„° (ëª¨ë“  ì„ íƒ ìƒí™©)
        lvs_list: ê° drawì˜ ì ì¬ë³€ìˆ˜ ê°’ [{lv_name: value}, ...]
        params: ì¸¡ì •ëª¨ë¸ íŒŒë¼ë¯¸í„°
        weights: Importance weights (n_draws,)
        iteration_logger: ë¡œê±° (optional)
        log_level: ë¡œê¹… ë ˆë²¨ ('MINIMAL', 'MODERATE', 'DETAILED')

    Returns:
        Dict[str, Dict]: {lv_name: {'grad_zeta': ..., 'grad_tau': ...}} or
                         {lv_name: {'grad_zeta': ..., 'grad_sigma_sq': ...}}
                         Each gradient is POSITIVE (âˆ‚LL/âˆ‚param) - Ascent direction
    """
    if not CUPY_AVAILABLE:
        raise RuntimeError("CuPy not available")

    n_draws = len(lvs_list)
    weights_gpu = cp.asarray(weights)  # (n_draws,)
    gradients = {}

    if iteration_logger and log_level in ['MODERATE', 'DETAILED']:
        iteration_logger.info("\n[ì¸¡ì •ëª¨ë¸ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°]")

    # ê° ì ì¬ë³€ìˆ˜ë³„ë¡œ ì²˜ë¦¬
    for lv_idx, lv_name in enumerate(params.keys()):
        zeta = params[lv_name]['zeta']
        n_indicators = len(zeta)

        # âœ… measurement_method í™•ì¸ (ëª¨ë¸ ê°ì²´ì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°)
        measurement_method = getattr(gpu_measurement_model.models[lv_name], 'measurement_method', 'ordered_probit')
        config = gpu_measurement_model.models[lv_name].config

        if iteration_logger and log_level == 'DETAILED' and lv_idx == 0:
            iteration_logger.info(f"  ì ì¬ë³€ìˆ˜: {lv_name}")
            iteration_logger.info(f"    - ì¸¡ì • ë°©ë²•: {measurement_method}")
            iteration_logger.info(f"    - ì§€í‘œ ìˆ˜: {n_indicators}")
            iteration_logger.info(f"    - zeta (ì²˜ìŒ 3ê°œ): {zeta[:min(3, len(zeta))]}")

        # LV ê°’ë“¤ì„ ë°°ì—´ë¡œ ë³€í™˜
        lv_values = np.array([lvs[lv_name] for lvs in lvs_list])
        lv_values_gpu = cp.asarray(lv_values)  # (n_draws,)
        zeta_gpu = cp.asarray(zeta)  # (n_indicators,)

        if iteration_logger and log_level == 'DETAILED' and lv_idx == 0:
            iteration_logger.info(f"    - LV ê°’ ë²”ìœ„: [{float(cp.min(lv_values_gpu)):.4f}, {float(cp.max(lv_values_gpu)):.4f}]")
            iteration_logger.info(f"    - LV ê°’ í‰ê· : {float(cp.mean(lv_values_gpu)):.4f}")

        # ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™” (ê° drawë³„)
        grad_zeta_batch = cp.zeros((n_draws, n_indicators))

        if measurement_method == 'continuous_linear':
            # âœ… Continuous Linear ë°©ì‹
            sigma_sq = params[lv_name]['sigma_sq']
            sigma_sq_gpu = cp.asarray(sigma_sq)  # (n_indicators,)
            grad_sigma_sq_batch = cp.zeros((n_draws, n_indicators))
        else:
            # Ordered Probit ë°©ì‹ (ê¸°ì¡´)
            tau = params[lv_name]['tau']
            n_thresholds = tau.shape[1]
            tau_gpu = cp.asarray(tau)  # (n_indicators, n_thresholds)
            grad_tau_batch = cp.zeros((n_draws, n_indicators, n_thresholds))

        # âœ… ì¸¡ì •ëª¨ë¸ì€ ê°œì¸ ìˆ˜ì¤€ ë°ì´í„°ì´ë¯€ë¡œ ì²« ë²ˆì§¸ í–‰ë§Œ ì‚¬ìš©
        # (DCE long formatìœ¼ë¡œ ì¸í•´ ë™ì¼í•œ ê°’ì´ ì—¬ëŸ¬ í–‰ì— ë³µì œë˜ì–´ ìˆìŒ)
        row = ind_data.iloc[0]

        # ì§€í‘œë³„ë¡œ ì²˜ë¦¬
        for i, indicator in enumerate(config.indicators):
            if indicator not in row.index:
                continue

            y = row[indicator]
            if pd.isna(y):
                continue

            if measurement_method == 'continuous_linear':
                # âœ… Continuous Linear: Y = Î¶ * LV + Îµ, Îµ ~ N(0, ÏƒÂ²)
                # log L = -0.5 * log(2Ï€ * ÏƒÂ²) - 0.5 * (y - Î¶*LV)Â² / ÏƒÂ²

                # ì˜ˆì¸¡ê°’: y_pred = Î¶_i * LV
                y_pred = zeta_gpu[i] * lv_values_gpu  # (n_draws,)

                # ì”ì°¨: residual = y - y_pred
                residual = y - y_pred  # (n_draws,)

                # ìƒì„¸ ë¡œê¹… (ì²« ë²ˆì§¸ LV, ì²« ë²ˆì§¸ ì§€í‘œë§Œ)
                if iteration_logger and log_level == 'DETAILED' and lv_idx == 0 and i == 0:
                    iteration_logger.info(f"\n    [ì§€í‘œ {indicator} ê³„ì‚° ì˜ˆì‹œ]")
                    iteration_logger.info(f"      - ê´€ì¸¡ê°’ y: {y:.4f}")
                    iteration_logger.info(f"      - zeta[{i}]: {float(zeta_gpu[i]):.6f}")
                    iteration_logger.info(f"      - sigma_sq[{i}]: {float(sigma_sq_gpu[i]):.6f}")
                    iteration_logger.info(f"      - LV (draw 0): {float(lv_values_gpu[0]):.4f}")
                    iteration_logger.info(f"      - ì˜ˆì¸¡ê°’ (draw 0): {float(y_pred[0]):.4f}")
                    iteration_logger.info(f"      - ì”ì°¨ (draw 0): {float(residual[0]):.4f}")

                # âˆ‚ log L / âˆ‚Î¶_i = (y - Î¶*LV) * LV / ÏƒÂ²
                grad_zeta_batch[:, i] = residual * lv_values_gpu / sigma_sq_gpu[i]

                # âˆ‚ log L / âˆ‚ÏƒÂ²_i = -0.5 / ÏƒÂ² + 0.5 * (y - Î¶*LV)Â² / Ïƒâ´
                grad_sigma_sq_batch[:, i] = -0.5 / sigma_sq_gpu[i] + 0.5 * (residual ** 2) / (sigma_sq_gpu[i] ** 2)

            else:
                # Ordered Probit (ê¸°ì¡´ ë°©ì‹)
                k = int(y) - 1  # 1-5 â†’ 0-4

                # V = zeta_i * LV (broadcasting)
                V = zeta_gpu[i] * lv_values_gpu  # (n_draws,)

                # tau_i for this indicator
                tau_i = tau_gpu[i]  # (n_thresholds,)

                # P(Y=k) ê³„ì‚°
                if k == 0:
                    # P(Y=1) = Î¦(Ï„_1 - V)
                    prob = cp_ndtr(tau_i[0] - V)
                    phi_upper = cp_norm_pdf(tau_i[0] - V)
                    phi_lower = cp.zeros_like(V)
                elif k == config.n_categories - 1:
                    # P(Y=5) = 1 - Î¦(Ï„_4 - V)
                    prob = 1 - cp_ndtr(tau_i[-1] - V)
                    phi_upper = cp.zeros_like(V)
                    phi_lower = cp_norm_pdf(tau_i[-1] - V)
                else:
                    # P(Y=k) = Î¦(Ï„_k - V) - Î¦(Ï„_{k-1} - V)
                    prob = cp_ndtr(tau_i[k] - V) - cp_ndtr(tau_i[k-1] - V)
                    phi_upper = cp_norm_pdf(tau_i[k] - V)
                    phi_lower = cp_norm_pdf(tau_i[k-1] - V)

                # ìˆ˜ì¹˜ ì•ˆì •ì„±
                prob = cp.clip(prob, 1e-10, 1 - 1e-10)

                # ğŸ”´ SIGN FIX: Removed the negative sign to return POSITIVE gradient
                # âˆ‚ log L / âˆ‚Î¶_i = (Ï†_upper - Ï†_lower) / P * LV  [POSITIVE, ascent]
                # Old (WRONG): grad_zeta_batch[:, i] = (phi_upper - phi_lower) / prob * (-lv_values_gpu)
                # New (CORRECT): Remove the minus sign
                grad_zeta_batch[:, i] = (phi_upper - phi_lower) / prob * lv_values_gpu

                # âˆ‚ log L / âˆ‚Ï„_k
                if k == 0:
                    grad_tau_batch[:, i, 0] = phi_upper / prob
                elif k == config.n_categories - 1:
                    grad_tau_batch[:, i, -1] = -phi_lower / prob
                else:
                    grad_tau_batch[:, i, k] = phi_upper / prob
                    grad_tau_batch[:, i, k-1] = -phi_lower / prob

        # âœ… ìˆ˜ì •: ê°€ì¤‘í‰ê·  ì ìš© (ë‹¨ìˆœ í•©ì‚°ì´ ì•„ë‹˜)
        # grad_weighted = Î£_r w_r * grad_r
        grad_zeta_weighted = cp.sum(weights_gpu[:, None] * grad_zeta_batch, axis=0)

        if iteration_logger and log_level == 'DETAILED' and lv_idx == 0:
            iteration_logger.info(f"\n    [ê°€ì¤‘í‰ê·  ì ìš© ì „í›„]")
            iteration_logger.info(f"      - grad_zeta_batch (draw 0, ì²˜ìŒ 3ê°œ): {[float(x) for x in grad_zeta_batch[0, :min(3, n_indicators)]]}")
            iteration_logger.info(f"      - weights (ì²˜ìŒ 5ê°œ): {[float(x) for x in weights_gpu[:5]]}")
            iteration_logger.info(f"      - grad_zeta_weighted (ì²˜ìŒ 3ê°œ): {[float(x) for x in grad_zeta_weighted[:min(3, n_indicators)]]}")

        # NaN ì²´í¬
        if cp.any(cp.isnan(grad_zeta_weighted)):
            logger.warning(f"NaN detected in grad_zeta for {lv_name}")
            grad_zeta_weighted = cp.nan_to_num(grad_zeta_weighted, nan=0.0)

        if measurement_method == 'continuous_linear':
            grad_sigma_sq_weighted = cp.sum(weights_gpu[:, None] * grad_sigma_sq_batch, axis=0)

            if iteration_logger and log_level == 'DETAILED' and lv_idx == 0:
                iteration_logger.info(f"      - grad_sigma_sq_weighted (ì²˜ìŒ 3ê°œ): {[float(x) for x in grad_sigma_sq_weighted[:min(3, n_indicators)]]}")

            if cp.any(cp.isnan(grad_sigma_sq_weighted)):
                logger.warning(f"NaN detected in grad_sigma_sq for {lv_name}")
                grad_sigma_sq_weighted = cp.nan_to_num(grad_sigma_sq_weighted, nan=0.0)
        else:
            grad_tau_weighted = cp.sum(weights_gpu[:, None, None] * grad_tau_batch, axis=0)

            if cp.any(cp.isnan(grad_tau_weighted)):
                logger.warning(f"NaN detected in grad_tau for {lv_name}")
                grad_tau_weighted = cp.nan_to_num(grad_tau_weighted, nan=0.0)

        # Gradient clipping
        grad_zeta_weighted = cp.clip(grad_zeta_weighted, -1e6, 1e6)

        # âœ… fix_first_loading ê³ ë ¤: ì²« ë²ˆì§¸ loadingì´ ê³ ì •ë˜ë©´ gradient ì œì™¸
        fix_first_loading = getattr(config, 'fix_first_loading', True)
        if fix_first_loading:
            # ì²« ë²ˆì§¸ zetaëŠ” 1.0ìœ¼ë¡œ ê³ ì • (gradient ì œì™¸)
            grad_zeta_final = cp.asnumpy(grad_zeta_weighted[1:])
        else:
            grad_zeta_final = cp.asnumpy(grad_zeta_weighted)

        # GPUì—ì„œ CPUë¡œ ì „ì†¡
        if measurement_method == 'continuous_linear':
            grad_sigma_sq_weighted = cp.clip(grad_sigma_sq_weighted, -1e6, 1e6)
            # ğŸ”´ SIGN: Store POSITIVE gradients (âˆ‚LL/âˆ‚Î¶, âˆ‚LL/âˆ‚ÏƒÂ²)
            gradients[lv_name] = {
                'grad_zeta': grad_zeta_final,
                'grad_sigma_sq': cp.asnumpy(grad_sigma_sq_weighted)
            }

            if iteration_logger and log_level in ['MODERATE', 'DETAILED']:
                iteration_logger.info(f"\n  [{lv_name}] ìµœì¢… ê·¸ë˜ë””ì–¸íŠ¸:")
                iteration_logger.info(f"    - grad_zeta: ë²”ìœ„=[{float(cp.min(grad_zeta_weighted)):.4f}, {float(cp.max(grad_zeta_weighted)):.4f}], norm={float(cp.linalg.norm(grad_zeta_weighted)):.4f}")
                iteration_logger.info(f"    - grad_sigma_sq: ë²”ìœ„=[{float(cp.min(grad_sigma_sq_weighted)):.4f}, {float(cp.max(grad_sigma_sq_weighted)):.4f}], norm={float(cp.linalg.norm(grad_sigma_sq_weighted)):.4f}")
        else:
            grad_tau_weighted = cp.clip(grad_tau_weighted, -1e6, 1e6)
            # ğŸ”´ SIGN: Store POSITIVE gradients (âˆ‚LL/âˆ‚Î¶, âˆ‚LL/âˆ‚Ï„)
            gradients[lv_name] = {
                'grad_zeta': grad_zeta_final,
                'grad_tau': cp.asnumpy(grad_tau_weighted)
            }

            if iteration_logger and log_level in ['MODERATE', 'DETAILED']:
                iteration_logger.info(f"\n  [{lv_name}] ìµœì¢… ê·¸ë˜ë””ì–¸íŠ¸:")
                iteration_logger.info(f"    - grad_zeta: ë²”ìœ„=[{float(cp.min(grad_zeta_weighted)):.4f}, {float(cp.max(grad_zeta_weighted)):.4f}], norm={float(cp.linalg.norm(grad_zeta_weighted)):.4f}")
                iteration_logger.info(f"    - grad_tau: ë²”ìœ„=[{float(cp.min(grad_tau_weighted)):.4f}, {float(cp.max(grad_tau_weighted)):.4f}], norm={float(cp.linalg.norm(grad_tau_weighted)):.4f}")

    # ğŸŸ¢ SIGN CHECK: Returns POSITIVE gradients (âˆ‚LL/âˆ‚Î¶, âˆ‚LL/âˆ‚ÏƒÂ² or âˆ‚LL/âˆ‚Ï„)
    # All measurement model gradients are in the ASCENT direction (âˆ‡LL)
    return gradients


def compute_structural_gradient_batch_gpu(
    ind_data: pd.DataFrame,
    lvs_list: List[Dict[str, float]],
    exo_draws_list: List[np.ndarray],
    params: Dict,
    covariates: List[str],
    endogenous_lv: str,
    exogenous_lvs: List[str],
    weights: np.ndarray,
    error_variance: float = 1.0,
    is_hierarchical: bool = False,
    hierarchical_paths: List[Dict] = None,
    gpu_measurement_model=None,
    choice_model=None,
    iteration_logger=None,
    log_level: str = 'DETAILED'
) -> Dict[str, np.ndarray]:
    """
    êµ¬ì¡°ëª¨ë¸ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ GPU ë°°ì¹˜ë¡œ ê³„ì‚° (ì²´ì¸ë£° ì—­ì „íŒŒ ì ìš©)

    âœ… ì˜¬ë°”ë¥¸ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°:
    âˆ‚LL/âˆ‚Î³_HC_to_PB = Î£_r w_r Ã— âˆ‚LL_r/âˆ‚Î³_HC_to_PB

    âˆ‚LL_r/âˆ‚Î³_HC_to_PB = âˆ‚LL_measurement/âˆ‚PB Ã— âˆ‚PB/âˆ‚Î³_HC_to_PB
                        + âˆ‚LL_choice/âˆ‚PB Ã— âˆ‚PB/âˆ‚Î³_HC_to_PB

    where:
    âˆ‚PB/âˆ‚Î³_HC_to_PB = HC (ì˜ˆì¸¡ë³€ìˆ˜ ê°’)

    Args:
        ind_data: ê°œì¸ ë°ì´í„°
        lvs_list: ê° drawì˜ ì ì¬ë³€ìˆ˜ ê°’
        exo_draws_list: ê° drawì˜ ì™¸ìƒ draws
        params: ì „ì²´ íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬ {'measurement': ..., 'structural': ..., 'choice': ...}
        covariates: ê³µë³€ëŸ‰ ë¦¬ìŠ¤íŠ¸
        endogenous_lv: ë‚´ìƒ LV ì´ë¦„ (ë³‘ë ¬ êµ¬ì¡°ì—ì„œë§Œ ì‚¬ìš©)
        exogenous_lvs: ì™¸ìƒ LV ì´ë¦„ ë¦¬ìŠ¤íŠ¸ (ë³‘ë ¬ êµ¬ì¡°ì—ì„œë§Œ ì‚¬ìš©)
        weights: Importance weights (n_draws,)
        error_variance: ì˜¤ì°¨ ë¶„ì‚°
        is_hierarchical: ê³„ì¸µì  êµ¬ì¡° ì—¬ë¶€
        hierarchical_paths: ê³„ì¸µì  ê²½ë¡œ ì •ë³´
        gpu_measurement_model: GPU ì¸¡ì •ëª¨ë¸ (ì—­ì „íŒŒìš©)
        choice_model: ì„ íƒëª¨ë¸ (ì—­ì „íŒŒìš©)
        iteration_logger: ë¡œê±° (optional)
        log_level: ë¡œê¹… ë ˆë²¨ ('MINIMAL', 'MODERATE', 'DETAILED')

    Returns:
        ë³‘ë ¬ êµ¬ì¡°: {'grad_gamma_lv': ..., 'grad_gamma_x': ...}
        ê³„ì¸µì  êµ¬ì¡°: {'grad_gamma_{pred}_to_{target}': ...}
    """
    if not CUPY_AVAILABLE:
        raise RuntimeError("CuPy not available")

    n_draws = len(lvs_list)
    weights_gpu = cp.asarray(weights)  # (n_draws,)

    if iteration_logger and log_level in ['MODERATE', 'DETAILED']:
        iteration_logger.info("\n[êµ¬ì¡°ëª¨ë¸ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°]")
        iteration_logger.info(f"  êµ¬ì¡° ìœ í˜•: {'ê³„ì¸µì ' if is_hierarchical else 'ë³‘ë ¬'}")

    if is_hierarchical:
        # âœ… ê³„ì¸µì  êµ¬ì¡°: ê° ê²½ë¡œë³„ë¡œ gradient ê³„ì‚°
        gradients = {}

        for path_idx, path in enumerate(hierarchical_paths):
            target = path['target']
            predictors = path['predictors']
            param_key = f"gamma_{predictors[0]}_to_{target}"

            # íŒŒë¼ë¯¸í„° ì¶”ì¶œ
            gamma = params['structural'][param_key]  # âœ… êµ¬ì¡°ëª¨ë¸ íŒŒë¼ë¯¸í„°ì—ì„œ ì¶”ì¶œ

            if iteration_logger and log_level == 'DETAILED' and path_idx == 0:
                iteration_logger.info(f"\n  [ê²½ë¡œ {path_idx + 1}] {predictors[0]} â†’ {target}")
                iteration_logger.info(f"    - gamma: {gamma:.6f}")

            # LV ê°’ ì¶”ì¶œ
            target_values = np.array([lvs[target] for lvs in lvs_list])
            pred_values = np.array([lvs[predictors[0]] for lvs in lvs_list])

            # GPUë¡œ ì „ì†¡
            target_gpu = cp.asarray(target_values)  # (n_draws,)
            pred_gpu = cp.asarray(pred_values)  # (n_draws,)
            gamma_gpu = cp.asarray(gamma)  # ìŠ¤ì¹¼ë¼

            # ì˜ˆì¸¡ê°’ ê³„ì‚°: target = gamma * predictor + error
            mu = gamma_gpu * pred_gpu  # (n_draws,)

            # ì”ì°¨
            residual = target_gpu - mu  # (n_draws,)

            if iteration_logger and log_level == 'DETAILED' and path_idx == 0:
                iteration_logger.info(f"    - predictor (draw 0): {float(pred_gpu[0]):.4f}")
                iteration_logger.info(f"    - target (draw 0): {float(target_gpu[0]):.4f}")
                iteration_logger.info(f"    - ì˜ˆì¸¡ê°’ Î¼ (draw 0): {float(mu[0]):.4f}")
                iteration_logger.info(f"    - ì”ì°¨ (draw 0): {float(residual[0]):.4f}")
                iteration_logger.info(f"    - ì”ì°¨ ë²”ìœ„: [{float(cp.min(residual)):.4f}, {float(cp.max(residual)):.4f}]")
                # âœ… ì¶”ê°€ ë””ë²„ê¹…: ëª¨ë“  drawsì˜ target ê°’ í™•ì¸
                iteration_logger.info(f"    - target ê°’ (ì²˜ìŒ 5ê°œ draws): {target_values[:5]}")
                iteration_logger.info(f"    - predictor ê°’ (ì²˜ìŒ 5ê°œ draws): {pred_values[:5]}")
                iteration_logger.info(f"    - ì”ì°¨ (ì²˜ìŒ 5ê°œ draws): {cp.asnumpy(residual[:5])}")

            # âœ… ì˜¬ë°”ë¥¸ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° (ì²´ì¸ë£° ì—­ì „íŒŒ)
            # âˆ‚LL/âˆ‚Î³ = Î£_r w_r Ã— (âˆ‚LL_measurement/âˆ‚target + âˆ‚LL_choice/âˆ‚target) Ã— predictor_r

            # 1. âˆ‚LL_measurement/âˆ‚target ê³„ì‚°
            grad_ll_meas_wrt_target = compute_measurement_grad_wrt_lv_gpu(
                gpu_measurement_model,
                ind_data,
                lvs_list,
                params['measurement'],
                target
            )
            grad_ll_meas_wrt_target_gpu = cp.asarray(grad_ll_meas_wrt_target)  # (n_draws,)

            # 2. âˆ‚LL_choice/âˆ‚target ê³„ì‚°
            choice_attributes = list(params['choice'].keys())
            choice_attributes = [k.replace('beta_', '') for k in choice_attributes if k.startswith('beta_')]

            grad_ll_choice_wrt_target = compute_choice_grad_wrt_lv_gpu(
                ind_data,
                lvs_list,
                params['choice'],
                target,
                choice_attributes
            )
            grad_ll_choice_wrt_target_gpu = cp.asarray(grad_ll_choice_wrt_target)  # (n_draws,)

            # 3. ì´ ê·¸ë˜ë””ì–¸íŠ¸: âˆ‚LL/âˆ‚target
            grad_ll_wrt_target = grad_ll_meas_wrt_target_gpu + grad_ll_choice_wrt_target_gpu  # (n_draws,)

            # 4. ì²´ì¸ë£°: âˆ‚LL/âˆ‚Î³ = Î£_r w_r Ã— (âˆ‚LL/âˆ‚target)_r Ã— (âˆ‚target/âˆ‚Î³)_r
            # âˆ‚target/âˆ‚Î³ = predictor
            grad_gamma = cp.sum(weights_gpu * grad_ll_wrt_target * pred_gpu)  # ìŠ¤ì¹¼ë¼

            if iteration_logger and log_level == 'DETAILED' and path_idx == 0:
                iteration_logger.info(f"\n    [ì—­ì „íŒŒ ê·¸ë˜ë””ì–¸íŠ¸]")
                iteration_logger.info(f"      - âˆ‚LL_meas/âˆ‚{target} (draw 0): {float(grad_ll_meas_wrt_target_gpu[0]):.6f}")
                iteration_logger.info(f"      - âˆ‚LL_choice/âˆ‚{target} (draw 0): {float(grad_ll_choice_wrt_target_gpu[0]):.6f}")
                iteration_logger.info(f"      - âˆ‚LL/âˆ‚{target} (draw 0): {float(grad_ll_wrt_target[0]):.6f}")
                iteration_logger.info(f"      - âˆ‚{target}/âˆ‚Î³ (draw 0): {float(pred_gpu[0]):.6f}")

            # NaN ì²´í¬
            if cp.isnan(grad_gamma):
                logger.warning(f"NaN detected in grad_{param_key}")
                grad_gamma = cp.asarray(0.0)

            # Gradient clipping
            grad_gamma = cp.clip(grad_gamma, -1e6, 1e6)

            gradients[f'grad_{param_key}'] = cp.asnumpy(grad_gamma).item()

            if iteration_logger and log_level in ['MODERATE', 'DETAILED']:
                iteration_logger.info(f"    - grad_{param_key} (ì—­ì „íŒŒ): {gradients[f'grad_{param_key}']:.6f}")

        return gradients

    else:
        # ë³‘ë ¬ êµ¬ì¡° (ì²´ì¸ë£° ì—­ì „íŒŒ ì ìš©)
        n_exo = len(exogenous_lvs)
        n_cov = len(covariates)

        gamma_lv = params['structural']['gamma_lv']
        gamma_x = params['structural']['gamma_x']

        # ë°°ì—´ë¡œ ë³€í™˜
        lv_endo_values = np.array([lvs[endogenous_lv] for lvs in lvs_list])
        exo_lv_matrix = np.array([[lvs[lv_name] for lv_name in exogenous_lvs] for lvs in lvs_list])

        # ê³µë³€ëŸ‰ (ëª¨ë“  drawì—ì„œ ë™ì¼)
        first_row = ind_data.iloc[0]
        X = np.array([first_row[cov] if cov in first_row.index and not pd.isna(first_row[cov]) else 0.0 for cov in covariates])

        # GPUë¡œ ì „ì†¡
        lv_endo_gpu = cp.asarray(lv_endo_values)  # (n_draws,)
        exo_lv_gpu = cp.asarray(exo_lv_matrix)  # (n_draws, n_exo)
        X_gpu = cp.asarray(X)  # (n_cov,)

        # âœ… ì˜¬ë°”ë¥¸ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° (ì²´ì¸ë£° ì—­ì „íŒŒ)
        # âˆ‚LL/âˆ‚Î³ = Î£_r w_r Ã— (âˆ‚LL_measurement/âˆ‚endo + âˆ‚LL_choice/âˆ‚endo) Ã— âˆ‚endo/âˆ‚Î³

        # 1. âˆ‚LL_measurement/âˆ‚endo ê³„ì‚°
        grad_ll_meas_wrt_endo = compute_measurement_grad_wrt_lv_gpu(
            gpu_measurement_model,
            ind_data,
            lvs_list,
            params['measurement'],
            endogenous_lv
        )
        grad_ll_meas_wrt_endo_gpu = cp.asarray(grad_ll_meas_wrt_endo)  # (n_draws,)

        # 2. âˆ‚LL_choice/âˆ‚endo ê³„ì‚°
        choice_attributes = list(params['choice'].keys())
        choice_attributes = [k.replace('beta_', '') for k in choice_attributes if k.startswith('beta_')]

        grad_ll_choice_wrt_endo = compute_choice_grad_wrt_lv_gpu(
            ind_data,
            lvs_list,
            params['choice'],
            endogenous_lv,
            choice_attributes
        )
        grad_ll_choice_wrt_endo_gpu = cp.asarray(grad_ll_choice_wrt_endo)  # (n_draws,)

        # 3. ì´ ê·¸ë˜ë””ì–¸íŠ¸: âˆ‚LL/âˆ‚endo
        grad_ll_wrt_endo = grad_ll_meas_wrt_endo_gpu + grad_ll_choice_wrt_endo_gpu  # (n_draws,)

        # 4. ì²´ì¸ë£°: âˆ‚LL/âˆ‚Î³_lv = Î£_r w_r Ã— (âˆ‚LL/âˆ‚endo)_r Ã— (âˆ‚endo/âˆ‚Î³_lv)_r
        # âˆ‚endo/âˆ‚Î³_lv = exo_lv
        grad_gamma_lv = cp.dot(exo_lv_gpu.T, weights_gpu * grad_ll_wrt_endo)  # (n_exo,)

        # âˆ‚LL/âˆ‚Î³_x = Î£_r w_r Ã— (âˆ‚LL/âˆ‚endo)_r Ã— (âˆ‚endo/âˆ‚Î³_x)_r
        # âˆ‚endo/âˆ‚Î³_x = X
        grad_gamma_x = cp.sum(weights_gpu * grad_ll_wrt_endo) * X_gpu  # (n_cov,)

        # NaN ì²´í¬
        if cp.any(cp.isnan(grad_gamma_lv)):
            logger.warning("NaN detected in grad_gamma_lv")
            grad_gamma_lv = cp.nan_to_num(grad_gamma_lv, nan=0.0)

        if cp.any(cp.isnan(grad_gamma_x)):
            logger.warning("NaN detected in grad_gamma_x")
            grad_gamma_x = cp.nan_to_num(grad_gamma_x, nan=0.0)

        # Gradient clipping
        grad_gamma_lv = cp.clip(grad_gamma_lv, -1e6, 1e6)
        grad_gamma_x = cp.clip(grad_gamma_x, -1e6, 1e6)

        return {
            'grad_gamma_lv': cp.asnumpy(grad_gamma_lv),
            'grad_gamma_x': cp.asnumpy(grad_gamma_x)
        }


def compute_choice_gradient_batch_gpu(
    ind_data: pd.DataFrame,
    lvs_list: List[Dict[str, float]],
    params: Dict,
    endogenous_lv: str,
    choice_attributes: List[str],
    weights: np.ndarray,
    moderators: List[str] = None,
    iteration_logger=None,
    log_level: str = 'DETAILED'
) -> Dict[str, np.ndarray]:
    """
    ì„ íƒëª¨ë¸ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ GPU ë°°ì¹˜ë¡œ ê³„ì‚° (ê°€ì¤‘í‰ê·  + ë°°ì¹˜ ì²˜ë¦¬)

    âœ… ì¡°ì ˆíš¨ê³¼ ì§€ì› ì¶”ê°€

    CPU êµ¬í˜„ (gradient_calculator.pyì˜ ChoiceGradient)ì„ ë”°ë¥´ë©´ì„œ:
    1. Importance weighting ì ìš©
    2. GPU ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì„±ëŠ¥ í–¥ìƒ (for loop ì œê±°)

    Args:
        ind_data: ê°œì¸ì˜ ì„ íƒ ë°ì´í„°
        lvs_list: ê° drawì˜ ì ì¬ë³€ìˆ˜ ê°’
        params: ì„ íƒëª¨ë¸ íŒŒë¼ë¯¸í„°
        endogenous_lv: ë‚´ìƒ LV ì´ë¦„ (main LV)
        choice_attributes: ì„ íƒ ì†ì„± ë¦¬ìŠ¤íŠ¸
        weights: Importance weights (n_draws,)
        moderators: ì¡°ì ˆë³€ìˆ˜ LV ì´ë¦„ ë¦¬ìŠ¤íŠ¸ (optional)
        iteration_logger: ë¡œê±° (optional)
        log_level: ë¡œê¹… ë ˆë²¨ ('MINIMAL', 'MODERATE', 'DETAILED')

    Returns:
        ê¸°ë³¸: {'grad_intercept': ..., 'grad_beta': ..., 'grad_lambda': ...}
        ì¡°ì ˆíš¨ê³¼: {'grad_intercept': ..., 'grad_beta': ..., 'grad_lambda_main': ..., 'grad_lambda_mod_{moderator}': ...}
    """
    if not CUPY_AVAILABLE:
        raise RuntimeError("CuPy not available")

    n_draws = len(lvs_list)
    n_choice_situations = len(ind_data)
    n_attributes = len(choice_attributes)

    if iteration_logger and log_level in ['MODERATE', 'DETAILED']:
        iteration_logger.info("\n[ì„ íƒëª¨ë¸ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°]")
        iteration_logger.info(f"  ì„ íƒ ìƒí™© ìˆ˜: {n_choice_situations}")

    # âœ… ASC (Alternative Specific Constants) ì¶”ì¶œ
    # paramsì—ëŠ” 'asc_sugar', 'asc_sugar_free' ê°™ì€ í‚¤ê°€ ìˆìŒ
    asc_keys = sorted([k for k in params.keys() if k.startswith('asc_')])
    if asc_keys:
        # ASC ê¸°ë°˜ multinomial logit
        asc_sugar = params.get('asc_sugar', params.get('asc_A', 0.0))
        asc_sugar_free = params.get('asc_sugar_free', params.get('asc_B', 0.0))
    else:
        # Binary probit (intercept ì‚¬ìš©)
        asc_sugar = params.get('intercept', 0.0)
        asc_sugar_free = 0.0

    # âœ… Beta íŒŒë¼ë¯¸í„° (ë°°ì—´ ë˜ëŠ” ê°œë³„ í‚¤)
    if 'beta' in params:
        beta = params['beta']
    else:
        # ê°œë³„ beta í‚¤ì—ì„œ ë°°ì—´ ìƒì„± (choice_attributes ìˆœì„œëŒ€ë¡œ)
        beta_keys = sorted([k for k in params.keys() if k.startswith('beta_')])
        if beta_keys:
            beta = np.array([params[k] for k in beta_keys])
        else:
            beta = np.array([])

    # âœ… Theta íŒŒë¼ë¯¸í„° (ëŒ€ì•ˆë³„ LV ê³„ìˆ˜) ìë™ ì¶”ì¶œ
    theta_params = {}
    for key in params.keys():
        if key.startswith('theta_'):
            theta_params[key] = params[key]

    # âœ… Gamma íŒŒë¼ë¯¸í„° (LV-Attribute ìƒí˜¸ì‘ìš©) ìë™ ì¶”ì¶œ
    gamma_interactions = {}
    for key in params.keys():
        if key.startswith('gamma_') and '_to_' not in key:
            # LV-Attribute ìƒí˜¸ì‘ìš©: gamma_{alt}_{lv}_{attr}
            # êµ¬ì¡°ëª¨ë¸ íŒŒë¼ë¯¸í„° (gamma_{lv1}_to_{lv2})ëŠ” ì œì™¸
            gamma_interactions[key] = params[key]

    if iteration_logger and log_level == 'DETAILED':
        iteration_logger.info(f"  ìœ ì—°í•œ ë¦¬ìŠ¤íŠ¸ ê¸°ë°˜ ëª¨ë¸:")
        iteration_logger.info(f"    - asc_sugar: {asc_sugar:.6f}")
        iteration_logger.info(f"    - asc_sugar_free: {asc_sugar_free:.6f}")
        iteration_logger.info(f"    - beta: {beta[:min(3, len(beta))]}")
        if theta_params:
            for theta_key, theta_val in theta_params.items():
                iteration_logger.info(f"    - {theta_key}: {theta_val:.6f}")
        if gamma_interactions:
            for gamma_key, gamma_val in gamma_interactions.items():
                iteration_logger.info(f"    - {gamma_key}: {gamma_val:.6f}")

    weights_gpu = cp.asarray(weights)  # (n_draws,)

    # âœ… LV ê°’ë“¤ ìë™ ì¶”ì¶œ (theta íŒŒë¼ë¯¸í„°ì—ì„œ LV ì´ë¦„ ì¶”ì¶œ)
    lv_values = {}
    lv_names_from_theta = set()
    for key in theta_params.keys():
        # theta_sugar_nutrition_knowledge -> nutrition_knowledge
        parts = key.split('_')
        if len(parts) >= 3:  # theta_{alt}_{lv_name}
            lv_name = '_'.join(parts[2:])  # alt ì´í›„ ëª¨ë“  ë¶€ë¶„
            lv_names_from_theta.add(lv_name)

    for lv_name in lv_names_from_theta:
        if lv_name in lvs_list[0]:  # ì²« ë²ˆì§¸ drawì— LVê°€ ìˆëŠ”ì§€ í™•ì¸
            lv_values[lv_name] = np.array([lvs[lv_name] for lvs in lvs_list])

    # ì„ íƒ ë³€ìˆ˜ ì°¾ê¸°
    choice_var = None
    for col in ['choice', 'chosen', 'choice_binary']:
        if col in ind_data.columns:
            choice_var = col
            break

    if choice_var is None:
        raise ValueError("ì„ íƒ ë³€ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    # ì†ì„± ë°ì´í„°ì™€ ì„ íƒ ì¤€ë¹„
    attributes_matrix = []
    choices = []

    for idx in range(n_choice_situations):
        row = ind_data.iloc[idx]
        attr_values = [row[attr] if attr in row.index and not pd.isna(row[attr]) else 0.0 for attr in choice_attributes]
        attributes_matrix.append(attr_values)
        choices.append(row[choice_var])

    attributes_matrix = np.array(attributes_matrix)  # (n_situations, n_attributes)
    choices = np.array(choices)  # (n_situations,)

    # GPUë¡œ ì „ì†¡
    attr_gpu = cp.asarray(attributes_matrix)  # (n_situations, n_attributes)
    choices_gpu = cp.asarray(choices)  # (n_situations,)
    beta_gpu = cp.asarray(beta)  # (n_attributes,)

    # âœ… ìœ ì—°í•œ ë¦¬ìŠ¤íŠ¸ ê¸°ë°˜: LV GPU ì „ì†¡
    lv_gpu = {}
    for lv_name, lv_vals in lv_values.items():
        lv_gpu[lv_name] = cp.asarray(lv_vals)  # (n_draws,)

    # âœ… ê°œì„ : ë°°ì¹˜ ì²˜ë¦¬ (for loop ì œê±°)
    # Broadcastingì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  drawsë¥¼ ë™ì‹œì— ì²˜ë¦¬

    # attr_batch: (1, n_situations, n_attributes)
    attr_batch = attr_gpu[None, :, :]

    # V_batch: (n_draws, n_situations)
    # V = ASC + Î²'X (multinomial logit) ë˜ëŠ” intercept + Î²'X (binary probit)
    if len(beta) > 0:
        V_batch = asc_sugar + cp.dot(attr_batch, beta_gpu[:, None]).squeeze(-1)
    else:
        V_batch = cp.full((len(weights), n_choice_situations), asc_sugar)

    # âœ… Theta íŒŒë¼ë¯¸í„°ë¡œ LV íš¨ê³¼ ì¶”ê°€: V += Î£(Î¸_i * LV_i)
    # theta_paramsì—ì„œ LV ì´ë¦„ê³¼ ê³„ìˆ˜ ì¶”ì¶œ
    for theta_key, theta_val in theta_params.items():
        # theta_sugar_nutrition_knowledge -> nutrition_knowledge
        parts = theta_key.split('_')
        if len(parts) >= 3:  # theta_{alt}_{lv_name}
            lv_name = '_'.join(parts[2:])  # alt ì´í›„ ëª¨ë“  ë¶€ë¶„
            if lv_name in lv_gpu:
                lv_batch = lv_gpu[lv_name][:, None]  # (n_draws, 1)
                V_batch = V_batch + theta_val * lv_batch

    # Î¦(V): (n_draws, n_situations)
    prob_batch = cp_ndtr(V_batch)
    prob_batch = cp.clip(prob_batch, 1e-10, 1 - 1e-10)

    # Ï†(V): (n_draws, n_situations)
    phi_batch = cp_norm_pdf(V_batch)

    # ì‹¤ì œ ì„ íƒì— ë”°ë¼: (n_draws, n_situations)
    prob_final_batch = cp.where(choices_gpu[None, :] == 1, prob_batch, 1 - prob_batch)

    # Mills ratio: (n_draws, n_situations)
    mills_batch = phi_batch / prob_final_batch

    # Sign: (n_draws, n_situations)
    sign_batch = cp.where(choices_gpu[None, :] == 1, 1.0, -1.0)

    if iteration_logger and log_level == 'DETAILED':
        iteration_logger.info(f"\n  [ì¤‘ê°„ ê³„ì‚° ê°’ (draw 0, situation 0)]")
        iteration_logger.info(f"    - V: {float(V_batch[0, 0]):.4f}")
        iteration_logger.info(f"    - Î¦(V): {float(prob_batch[0, 0]):.4f}")
        iteration_logger.info(f"    - Ï†(V): {float(phi_batch[0, 0]):.4f}")
        iteration_logger.info(f"    - choice: {int(choices_gpu[0])}")
        iteration_logger.info(f"    - Mills ratio: {float(mills_batch[0, 0]):.4f}")

    # Weighted mills: (n_draws, n_situations)
    weighted_mills = weights_gpu[:, None] * sign_batch * mills_batch

    # âœ… ìˆ˜ì •: ê°€ì¤‘í‰ê·  ì ìš©
    # âˆ‚V/âˆ‚intercept = 1
    grad_intercept = cp.sum(weighted_mills).item()

    # âˆ‚V/âˆ‚Î² = X
    # (n_situations, n_attributes).T @ (n_draws, n_situations).T
    # = (n_attributes, n_situations) @ (n_situations, n_draws)
    # = (n_attributes, n_draws) â†’ sum over draws
    grad_beta = cp.dot(attr_gpu.T, weighted_mills.T).sum(axis=1)  # (n_attributes,)

    # âœ… Theta gradient ê³„ì‚°: âˆ‚V/âˆ‚Î¸_i = LV_i
    # theta_paramsì—ì„œ LV ì´ë¦„ ì¶”ì¶œí•˜ì—¬ gradient ê³„ì‚°
    grad_lambda = {}
    for theta_key in theta_params.keys():
        # theta_sugar_nutrition_knowledge -> nutrition_knowledge
        parts = theta_key.split('_')
        if len(parts) >= 3:  # theta_{alt}_{lv_name}
            lv_name = '_'.join(parts[2:])  # alt ì´í›„ ëª¨ë“  ë¶€ë¶„
            if lv_name in lv_gpu:
                lv_batch = lv_gpu[lv_name][:, None]  # (n_draws, 1)
                grad_lambda[theta_key] = cp.sum(weighted_mills * lv_batch).item()

    # âœ… ìœ ì—°í•œ ë¦¬ìŠ¤íŠ¸ ê¸°ë°˜: Gamma gradient ê³„ì‚° (LV-Attribute ìƒí˜¸ì‘ìš©)
    # âˆ‚V/âˆ‚Î³_ij = LV_i Ã— X_j
    grad_gamma = {}

    if iteration_logger and log_level == 'DETAILED':
        iteration_logger.info(f"\n  [Gamma Gradient ê³„ì‚°]")
        iteration_logger.info(f"    - gamma_interactions í‚¤: {list(gamma_interactions.keys())}")
        iteration_logger.info(f"    - lv_gpu í‚¤: {list(lv_gpu.keys())}")
        iteration_logger.info(f"    - choice_attributes: {choice_attributes}")

    for gamma_key in gamma_interactions.keys():
        # gamma_purchase_intention_health_label â†’ lv_name='purchase_intention', attr_name='health_label'
        parts = gamma_key.replace('gamma_', '').rsplit('_', 1)
        if iteration_logger and log_level == 'DETAILED':
            iteration_logger.info(f"    - {gamma_key}: parts={parts}")

        if len(parts) == 2:
            lv_name, attr_name = parts
            if iteration_logger and log_level == 'DETAILED':
                iteration_logger.info(f"      lv_name={lv_name}, attr_name={attr_name}")
                iteration_logger.info(f"      lv_name in lv_gpu: {lv_name in lv_gpu}")
                iteration_logger.info(f"      attr_name in choice_attributes: {attr_name in choice_attributes}")

            if lv_name in lv_gpu and attr_name in choice_attributes:
                lv_batch = lv_gpu[lv_name][:, None]  # (n_draws, 1)
                attr_idx = choice_attributes.index(attr_name)
                attr_values = attr_gpu[:, attr_idx]  # (n_situations,)
                # (n_draws, n_situations) * (n_draws, 1) * (1, n_situations)
                interaction_batch = lv_batch * attr_values[None, :]  # (n_draws, n_situations)
                grad_gamma[gamma_key] = cp.sum(weighted_mills * interaction_batch).item()

                if iteration_logger and log_level == 'DETAILED':
                    iteration_logger.info(f"      âœ… grad_{gamma_key} ê³„ì‚° ì™„ë£Œ: {grad_gamma[gamma_key]:.6f}")

    # NaN ì²´í¬
    if np.isnan(grad_intercept):
        logger.warning("NaN detected in grad_intercept")
        grad_intercept = 0.0

    if cp.any(cp.isnan(grad_beta)):
        logger.warning("NaN detected in grad_beta")
        grad_beta = cp.nan_to_num(grad_beta, nan=0.0)

    # âœ… ìœ ì—°í•œ ë¦¬ìŠ¤íŠ¸ ê¸°ë°˜: Lambda NaN ì²´í¬
    for lv_name in grad_lambda.keys():
        if np.isnan(grad_lambda[lv_name]):
            logger.warning(f"NaN detected in grad_lambda_{lv_name}")
            grad_lambda[lv_name] = 0.0

    # âœ… ìœ ì—°í•œ ë¦¬ìŠ¤íŠ¸ ê¸°ë°˜: Gamma NaN ì²´í¬
    for gamma_key in grad_gamma.keys():
        if np.isnan(grad_gamma[gamma_key]):
            logger.warning(f"NaN detected in grad_{gamma_key}")
            grad_gamma[gamma_key] = 0.0

    # Gradient clipping
    grad_intercept = np.clip(grad_intercept, -1e6, 1e6)
    grad_beta = cp.clip(grad_beta, -1e6, 1e6)

    # âœ… ìœ ì—°í•œ ë¦¬ìŠ¤íŠ¸ ê¸°ë°˜: Lambda clipping
    for lv_name in grad_lambda.keys():
        grad_lambda[lv_name] = np.clip(grad_lambda[lv_name], -1e6, 1e6)

    # âœ… ìœ ì—°í•œ ë¦¬ìŠ¤íŠ¸ ê¸°ë°˜: Gamma clipping
    for gamma_key in grad_gamma.keys():
        grad_gamma[gamma_key] = np.clip(grad_gamma[gamma_key], -1e6, 1e6)

    if iteration_logger and log_level in ['MODERATE', 'DETAILED']:
        iteration_logger.info(f"\n  [ìµœì¢… ê·¸ë˜ë””ì–¸íŠ¸]")
        iteration_logger.info(f"    - grad_intercept: {grad_intercept:.6f}")
        iteration_logger.info(f"    - grad_beta: {cp.asnumpy(grad_beta)[:min(3, len(grad_beta))]}")
        if grad_lambda:
            for lv_name, grad_val in grad_lambda.items():
                iteration_logger.info(f"    - grad_lambda_{lv_name}: {grad_val:.6f}")
        else:
            iteration_logger.info(f"    - grad_lambda: ì—†ìŒ (Base Model)")
        if grad_gamma:
            for gamma_key, grad_val in grad_gamma.items():
                iteration_logger.info(f"    - grad_{gamma_key}: {grad_val:.6f}")

    # âœ… ìœ ì—°í•œ ë¦¬ìŠ¤íŠ¸ ê¸°ë°˜: ê²°ê³¼ ë°˜í™˜
    result = {
        'grad_intercept': grad_intercept,
        'grad_beta': cp.asnumpy(grad_beta)
    }
    for lv_name, grad_val in grad_lambda.items():
        result[f'grad_lambda_{lv_name}'] = grad_val
    for gamma_key, grad_val in grad_gamma.items():
        result[f'grad_{gamma_key}'] = grad_val

    return result


def compute_all_individuals_gradients_batch_gpu(
    gpu_measurement_model,
    all_ind_data: List[pd.DataFrame],
    all_ind_draws: np.ndarray,
    params_dict: Dict,
    measurement_model,
    structural_model,
    choice_model,
    iteration_logger=None,
    log_level: str = 'MINIMAL'
) -> List[Dict]:
    """
    ëª¨ë“  ê°œì¸ì˜ gradientë¥¼ GPU batchë¡œ ë™ì‹œ ê³„ì‚° (ê°œì¸ë³„ ìˆœì°¨ + draws GPU batch)

    âš ï¸ ì´ í•¨ìˆ˜ëŠ” ê°œì¸ë³„ ìˆœì°¨ ì²˜ë¦¬ì…ë‹ˆë‹¤.
    ì™„ì „ GPU BatchëŠ” compute_all_individuals_gradients_full_batch_gpu ì‚¬ìš©

    Args:
        gpu_measurement_model: GPU ì¸¡ì •ëª¨ë¸
        all_ind_data: ëª¨ë“  ê°œì¸ì˜ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ [DataFrame_1, ..., DataFrame_N]
        all_ind_draws: ëª¨ë“  ê°œì¸ì˜ draws (N, n_draws, n_dims)
        params_dict: íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬
        measurement_model: ì¸¡ì •ëª¨ë¸
        structural_model: êµ¬ì¡°ëª¨ë¸
        choice_model: ì„ íƒëª¨ë¸
        iteration_logger: ë¡œê±°
        log_level: ë¡œê¹… ë ˆë²¨

    Returns:
        ê°œì¸ë³„ gradient ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ [grad_dict_1, ..., grad_dict_N]
    """
    if not CUPY_AVAILABLE:
        raise RuntimeError("CuPy not available")

    n_individuals = len(all_ind_data)
    n_draws = all_ind_draws.shape[1]

    if iteration_logger and log_level in ['MODERATE', 'DETAILED']:
        iteration_logger.info(
            f"\n{'='*80}\n"
            f"GPU Batch Gradient ê³„ì‚° (ê°œì¸ë³„ ìˆœì°¨)\n"
            f"{'='*80}\n"
            f"  ê°œì¸ ìˆ˜: {n_individuals}ëª…\n"
            f"  Draws per individual: {n_draws}ê°œ\n"
            f"  ì´ ê³„ì‚°: {n_individuals} Ã— {n_draws} = {n_individuals * n_draws}ê°œ\n"
            f"{'='*80}"
        )

    # ê³„ì¸µì  êµ¬ì¡° ì§€ì›
    is_hierarchical = hasattr(structural_model, 'is_hierarchical') and structural_model.is_hierarchical

    if is_hierarchical:
        n_first_order = len(structural_model.exogenous_lvs)
        n_higher_order = len(structural_model.get_higher_order_lvs())
    else:
        n_exo = structural_model.n_exo

    # ëª¨ë“  ê°œì¸ì˜ gradient ì €ì¥
    all_individual_gradients = []

    # ê°œì¸ë³„ë¡œ ì²˜ë¦¬ (ê° ê°œì¸ ë‚´ë¶€ëŠ” GPU batch)
    for ind_idx, (ind_data, ind_draws) in enumerate(zip(all_ind_data, all_ind_draws)):
        # ëª¨ë“  drawsì˜ LV ê°’ ë¯¸ë¦¬ ê³„ì‚°
        lvs_list = []
        exo_draws_list = []

        for draw_idx in range(n_draws):
            if is_hierarchical:
                # ê³„ì¸µì  êµ¬ì¡°
                first_order_draws = ind_draws[draw_idx, :n_first_order]
                higher_order_errors = ind_draws[draw_idx, n_first_order:]

                higher_order_lvs = structural_model.get_higher_order_lvs()
                error_dict = {lv_name: higher_order_errors[i] for i, lv_name in enumerate(higher_order_lvs)}

                latent_vars = structural_model.predict(
                    ind_data, first_order_draws, params_dict['structural'],
                    endo_draw=None, higher_order_draws=error_dict
                )
                exo_draws_list.append(first_order_draws)
            else:
                # ë³‘ë ¬ êµ¬ì¡°
                exo_draws = ind_draws[draw_idx, :n_exo]
                endo_draw = ind_draws[draw_idx, n_exo]

                latent_vars = structural_model.predict(
                    ind_data, exo_draws, params_dict['structural'], endo_draw
                )
                exo_draws_list.append(exo_draws)

            lvs_list.append(latent_vars)

        # 1. ê²°í•© likelihood ê³„ì‚° (GPU batch)
        ll_batch = compute_joint_likelihood_batch_gpu(
            gpu_measurement_model,
            ind_data,
            lvs_list,
            ind_draws,
            params_dict,
            structural_model,
            choice_model
        )

        # 2. Importance weights ê³„ì‚° (GPU)
        weights = compute_importance_weights_gpu(ll_batch, individual_id=ind_idx)

        # 3. ê°€ì¤‘í‰ê·  gradient ê³„ì‚° (GPU batch)
        grad_meas = compute_measurement_gradient_batch_gpu(
            gpu_measurement_model,
            ind_data,
            lvs_list,
            params_dict['measurement'],
            weights,
            iteration_logger=None,  # ê°œë³„ ë¡œê¹… ë¹„í™œì„±í™”
            log_level='MINIMAL'
        )

        grad_struct = compute_structural_gradient_batch_gpu(
            ind_data,
            lvs_list,
            exo_draws_list,
            params_dict,  # âœ… ì „ì²´ íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬ ì „ë‹¬ (ì—­ì „íŒŒìš©)
            structural_model.covariates,
            structural_model.endogenous_lv if not is_hierarchical else None,
            structural_model.exogenous_lvs if not is_hierarchical else None,
            weights,
            error_variance=1.0,
            is_hierarchical=is_hierarchical,
            hierarchical_paths=structural_model.hierarchical_paths if is_hierarchical else None,
            gpu_measurement_model=gpu_measurement_model,  # âœ… ì—­ì „íŒŒìš©
            choice_model=choice_model,  # âœ… ì—­ì „íŒŒìš©
            iteration_logger=None,
            log_level='MINIMAL'
        )

        # ì„ íƒëª¨ë¸ gradient
        if hasattr(choice_model.config, 'moderators') and choice_model.config.moderators:
            # ì¡°ì ˆíš¨ê³¼ ëª¨ë¸
            grad_choice = compute_choice_gradient_batch_gpu(
                ind_data,
                lvs_list,
                params_dict['choice'],
                choice_model.config.main_lv,
                choice_model.config.choice_attributes,
                weights,
                moderators=choice_model.config.moderators,
                iteration_logger=None,
                log_level='MINIMAL'
            )
        else:
            # ê¸°ë³¸ ëª¨ë¸
            grad_choice = compute_choice_gradient_batch_gpu(
                ind_data,
                lvs_list,
                params_dict['choice'],
                structural_model.endogenous_lv,
                choice_model.config.choice_attributes,
                weights,
                moderators=None,
                iteration_logger=None,
                log_level='MINIMAL'
            )

        # ê°œì¸ë³„ gradient ì €ì¥
        ind_grad_dict = {
            'measurement': grad_meas,
            'structural': grad_struct,
            'choice': grad_choice
        }

        all_individual_gradients.append(ind_grad_dict)

        # ì§„í–‰ ìƒí™© ë¡œê¹… (10% ë‹¨ìœ„)
        if iteration_logger and log_level in ['MODERATE', 'DETAILED']:
            if (ind_idx + 1) % max(1, n_individuals // 10) == 0:
                progress = (ind_idx + 1) / n_individuals * 100
                iteration_logger.info(f"  ì§„í–‰: {ind_idx + 1}/{n_individuals} ({progress:.0f}%)")

    if iteration_logger and log_level in ['MODERATE', 'DETAILED']:
        iteration_logger.info(
            f"{'='*80}\n"
            f"ì™„ì „ GPU Batch Gradient ê³„ì‚° ì™„ë£Œ: {n_individuals}ëª…\n"
            f"{'='*80}"
        )

    return all_individual_gradients

def compute_all_individuals_likelihood_full_batch_gpu(
    gpu_measurement_model,
    all_ind_data: List[pd.DataFrame],
    all_ind_draws: np.ndarray,
    params_dict: Dict,
    structural_model,
    choice_model,
    iteration_logger=None,
    log_level: str = 'MINIMAL',
    use_scaling: bool = False  # âœ… ìŠ¤ì¼€ì¼ë§ ë¹„í™œì„±í™” (ê¸°ë³¸ê°’)
) -> float:
    """
    ëª¨ë“  ê°œì¸ì˜ ìš°ë„ë¥¼ ì™„ì „ GPU batchë¡œ ë™ì‹œ ê³„ì‚°

    ğŸš€ ì™„ì „ GPU Batch: Nëª… Ã— R drawsë¥¼ ë™ì‹œ ì²˜ë¦¬

    Args:
        gpu_measurement_model: GPU ì¸¡ì •ëª¨ë¸
        all_ind_data: ëª¨ë“  ê°œì¸ì˜ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ [DataFrame_1, ..., DataFrame_N]
        all_ind_draws: ëª¨ë“  ê°œì¸ì˜ draws (N, n_draws, n_dims)
        params_dict: íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬
        structural_model: êµ¬ì¡°ëª¨ë¸
        choice_model: ì„ íƒëª¨ë¸
        iteration_logger: ë¡œê±°
        log_level: ë¡œê¹… ë ˆë²¨
        use_scaling: bool = Trueì´ë©´ ì¸¡ì •ëª¨ë¸ ìš°ë„ë¥¼ ì§€í‘œ ìˆ˜ë¡œ ë‚˜ëˆ” (ìµœì í™”ìš©),
                           Falseë©´ ì›ë³¸ ìš°ë„ ì‚¬ìš© (AIC/BIC ê³„ì‚°ìš©)

    Returns:
        ì „ì²´ ë¡œê·¸ìš°ë„ (ìŠ¤ì¹¼ë¼)
    """
    if not CUPY_AVAILABLE:
        raise RuntimeError("CuPy not available")

    import time
    from scipy.special import logsumexp

    n_individuals = len(all_ind_data)
    n_draws = all_ind_draws.shape[1]

    if iteration_logger and log_level in ['MODERATE', 'DETAILED']:
        iteration_logger.info(
            f"\n{'='*80}\n"
            f"ğŸš€ ì™„ì „ GPU Batch ìš°ë„ ê³„ì‚°\n"
            f"{'='*80}\n"
            f"  ê°œì¸ ìˆ˜: {n_individuals}ëª…\n"
            f"  Draws per individual: {n_draws}ê°œ\n"
            f"  ì´ ê³„ì‚°: {n_individuals} Ã— {n_draws} = {n_individuals * n_draws}ê°œ ë™ì‹œ ì²˜ë¦¬\n"
            f"{'='*80}"
        )

    total_start = time.time()

    # Step 1: ëª¨ë“  ê°œì¸ Ã— ëª¨ë“  drawsì˜ LV ê³„ì‚° (ê¸°ì¡´ gradient í•¨ìˆ˜ì™€ ë™ì¼)
    lv_start = time.time()
    all_lvs_list = []  # (N, R) ë¦¬ìŠ¤íŠ¸

    for ind_idx, (ind_data, ind_draws) in enumerate(zip(all_ind_data, all_ind_draws)):
        ind_lvs_list = []

        for draw_idx in range(n_draws):
            draw = ind_draws[draw_idx]

            # êµ¬ì¡°ëª¨ë¸: LV ì˜ˆì¸¡
            if hasattr(structural_model, 'is_hierarchical') and structural_model.is_hierarchical:
                # ê³„ì¸µì  êµ¬ì¡°
                n_first_order = len(structural_model.exogenous_lvs)
                exo_draws = draw[:n_first_order]

                # 2ì°¨+ LV ì˜¤ì°¨í•­
                higher_order_draws = {}
                higher_order_lvs = structural_model.get_higher_order_lvs()
                for i, lv_name in enumerate(higher_order_lvs):
                    higher_order_draws[lv_name] = draw[n_first_order + i]

                lv = structural_model.predict(
                    ind_data, exo_draws, params_dict['structural'],
                    higher_order_draws=higher_order_draws
                )
            elif hasattr(structural_model, 'endogenous_lv'):
                # ë³‘ë ¬ êµ¬ì¡°
                n_exo = structural_model.n_exo
                exo_draws = draw[:n_exo]
                endo_draw = draw[n_exo]
                lv = structural_model.predict(ind_data, exo_draws, params_dict['structural'], endo_draw)
            else:
                # ë‹¨ì¼ ì ì¬ë³€ìˆ˜
                lv = structural_model.predict(ind_data, params_dict['structural'], draw)

            ind_lvs_list.append(lv)

        all_lvs_list.append(ind_lvs_list)

    lv_time = time.time() - lv_start

    if iteration_logger and log_level in ['MODERATE', 'DETAILED']:
        iteration_logger.info(f"  LV ê³„ì‚° ì™„ë£Œ ({lv_time:.3f}ì´ˆ)")

    # Step 2: ê°œì¸ë³„ ìš°ë„ ê³„ì‚° (ê° ê°œì¸ì˜ R drawsë¥¼ GPU ë°°ì¹˜ë¡œ ì²˜ë¦¬)
    likelihood_start = time.time()
    total_ll = 0.0

    # ğŸ“Š ì „ì²´ ìš°ë„ ì„±ë¶„ ëˆ„ì  (ë¡œê¹…ìš©)
    total_ll_measurement = 0.0
    total_ll_choice = 0.0

    # ğŸ” ì¸¡ì •ëª¨ë¸ ì§€í‘œ ìˆ˜ ê³„ì‚° (ìŠ¤ì¼€ì¼ë§ìš©)
    n_measurement_indicators = 0
    if hasattr(gpu_measurement_model, 'models'):
        for lv_name, model in gpu_measurement_model.models.items():
            n_measurement_indicators += len(model.config.indicators)

    # ìŠ¤ì¼€ì¼ë§ ê°€ì¤‘ì¹˜ ê³„ì‚°
    measurement_weight = 1.0 / n_measurement_indicators if (use_scaling and n_measurement_indicators > 0) else 1.0

    for ind_idx, (ind_data, ind_lvs_list, ind_draws) in enumerate(zip(all_ind_data, all_lvs_list, all_ind_draws)):
        # ê¸°ì¡´ gpu_batch_utils í•¨ìˆ˜ í™œìš©
        from . import gpu_batch_utils

        # ì¸¡ì •ëª¨ë¸ ìš°ë„ (GPU ë°°ì¹˜)
        ll_measurement_raw = gpu_batch_utils.compute_measurement_batch_gpu(
            gpu_measurement_model,
            ind_data,
            ind_lvs_list,
            params_dict['measurement']
        )

        # âœ… ì¸¡ì •ëª¨ë¸ ìš°ë„ ìŠ¤ì¼€ì¼ë§ (ê°€ì¤‘ì¹˜ ì ìš©)
        ll_measurement = ll_measurement_raw * measurement_weight

        # ì„ íƒëª¨ë¸ ìš°ë„ (GPU ë°°ì¹˜)
        ll_choice = gpu_batch_utils.compute_choice_batch_gpu(
            ind_data,
            ind_lvs_list,
            params_dict['choice'],
            choice_model
        )

        # ê²°í•© ìš°ë„ (R,)
        # âœ… êµ¬ì¡°ëª¨ë¸ ìš°ë„ëŠ” í¬í•¨í•˜ì§€ ì•ŠìŒ (êµ¬ì¡°ëª¨ë¸ì€ LV ìƒì„±ë§Œ ë‹´ë‹¹)
        draw_lls = ll_measurement + ll_choice

        # ğŸ“Š ì „ì²´ ìš°ë„ ì„±ë¶„ ëˆ„ì  (ê°œì¸ë³„ í‰ê· )
        person_ll_measurement = logsumexp(ll_measurement) - np.log(n_draws)
        person_ll_choice = logsumexp(ll_choice) - np.log(n_draws)

        total_ll_measurement += person_ll_measurement
        total_ll_choice += person_ll_choice

        # ìœ í•œì„± ì²´í¬
        non_finite_mask = ~np.isfinite(draw_lls)
        if np.any(non_finite_mask):
            non_finite_indices = np.where(non_finite_mask)[0]
            print(f"\n{'='*80}")
            print(f"âŒ ê°œì¸ {ind_idx+1}ì—ì„œ ë¹„ìœ í•œ ìš°ë„ ë°œê²¬!")
            print(f"{'='*80}")
            print(f"ë¹„ìœ í•œ draws ìˆ˜: {len(non_finite_indices)}/{n_draws}")
            print(f"ë¹„ìœ í•œ draw ì¸ë±ìŠ¤: {non_finite_indices[:10]}...")  # ì²˜ìŒ 10ê°œë§Œ
            print(f"ë¹„ìœ í•œ ìš°ë„ ê°’: {draw_lls[non_finite_indices[:10]]}")
            print(f"\nìš°ë„ ì„±ë¶„ (ì²« ë²ˆì§¸ ë¹„ìœ í•œ draw):")
            bad_idx = non_finite_indices[0]
            print(f"  ll_measurement[{bad_idx}]: {ll_measurement[bad_idx]:.4f}")
            print(f"  ll_choice[{bad_idx}]: {ll_choice[bad_idx]:.4f}")
            print(f"  draw_ll[{bad_idx}]: {draw_lls[bad_idx]}")
            print(f"{'='*80}\n")
            raise ValueError(f"ê°œì¸ {ind_idx+1}ì—ì„œ ë¹„ìœ í•œ ìš°ë„ ë°œê²¬!")

        # ê°œì¸ ìš°ë„: log(1/R * sum(exp(draw_lls)))
        person_ll = logsumexp(draw_lls) - np.log(n_draws)
        total_ll += person_ll

    likelihood_time = time.time() - likelihood_start
    total_time = time.time() - total_start

    if iteration_logger and log_level in ['MODERATE', 'DETAILED']:
        # ğŸ“Š ì „ì²´ ìš°ë„ ì„±ë¶„ ë¡œê¹…
        iteration_logger.info(
            f"\n{'='*80}\n"
            f"ğŸ“Š ìš°ë„ ê³„ì‚° ì™„ë£Œ\n"
            f"{'='*80}\n"
            f"  ì´ ì‹œê°„: {total_time:.3f}ì´ˆ (LV: {lv_time:.3f}ì´ˆ, ìš°ë„: {likelihood_time:.3f}ì´ˆ)\n"
            f"\n"
            f"  ì „ì²´ ë¡œê·¸ìš°ë„: {total_ll:.4f}\n"
            f"\n"
            f"  ğŸ“ˆ ëª¨ë¸ë³„ ìš°ë„ ì„±ë¶„:\n"
            f"    ì¸¡ì •ëª¨ë¸: {total_ll_measurement:.4f} ({100*abs(total_ll_measurement)/abs(total_ll):.1f}%)\n"
            f"    ì„ íƒëª¨ë¸: {total_ll_choice:.4f} ({100*abs(total_ll_choice)/abs(total_ll):.1f}%)\n"
            f"{'='*80}"
        )

    return total_ll


def compute_all_individuals_gradients_full_batch_gpu(
    gpu_measurement_model,
    all_ind_data: List[pd.DataFrame],
    all_ind_draws: np.ndarray,
    params_dict: Dict,
    measurement_model,
    structural_model,
    choice_model,
    iteration_logger=None,
    log_level: str = 'MINIMAL',
    use_scaling: bool = False  # âœ… ì¸¡ì •ëª¨ë¸ ìš°ë„ ìŠ¤ì¼€ì¼ë§ ì‚¬ìš© ì—¬ë¶€
) -> List[Dict]:
    """
    ëª¨ë“  ê°œì¸ì˜ gradientë¥¼ ì™„ì „ GPU batchë¡œ ë™ì‹œ ê³„ì‚°

    ğŸš€ ì™„ì „ GPU Batch: Nëª… Ã— R draws Ã— P paramsë¥¼ ë™ì‹œ ê³„ì‚°

    Args:
        gpu_measurement_model: GPU ì¸¡ì •ëª¨ë¸
        all_ind_data: ëª¨ë“  ê°œì¸ì˜ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ [DataFrame_1, ..., DataFrame_N]
        all_ind_draws: ëª¨ë“  ê°œì¸ì˜ draws (N, n_draws, n_dims)
        params_dict: íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬
        measurement_model: ì¸¡ì •ëª¨ë¸
        structural_model: êµ¬ì¡°ëª¨ë¸
        choice_model: ì„ íƒëª¨ë¸
        iteration_logger: ë¡œê±°
        log_level: ë¡œê¹… ë ˆë²¨
        use_scaling: ì¸¡ì •ëª¨ë¸ ìš°ë„ ìŠ¤ì¼€ì¼ë§ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: False)

    Returns:
        ê°œì¸ë³„ gradient ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ [grad_dict_1, ..., grad_dict_N]
    """
    if not CUPY_AVAILABLE:
        raise RuntimeError("CuPy not available")

    import time

    n_individuals = len(all_ind_data)
    n_draws = all_ind_draws.shape[1]

    total_start = time.time()

    # Step 1: ë°ì´í„° ì¤€ë¹„
    prep_start = time.time()
    n_rows = len(all_ind_data[0])
    prep_time = time.time() - prep_start

    # Step 2: GPUë¡œ ë°ì´í„° ì „ì†¡
    transfer_start = time.time()
    all_draws_gpu = cp.asarray(all_ind_draws)
    transfer_time = time.time() - transfer_start

    # Step 3: ì™„ì „ GPU Batchë¡œ ëª¨ë“  ê°œì¸ Ã— ëª¨ë“  drawsì˜ LV ê³„ì‚°

    lv_start = time.time()

    # ëª¨ë“  ê°œì¸ Ã— ëª¨ë“  drawsì˜ LV ê³„ì‚°
    # Shape: (N, R, n_lvs)
    all_lvs_list = []  # List of List[Dict]: (N, R)

    is_hierarchical = hasattr(structural_model, 'is_hierarchical') and structural_model.is_hierarchical

    for ind_idx, (ind_data, ind_draws) in enumerate(zip(all_ind_data, all_ind_draws)):
        ind_lvs_list = []

        for draw_idx in range(n_draws):
            draw = ind_draws[draw_idx]

            # LV ê³„ì‚° (CPU - structural_model.predict)
            if is_hierarchical:
                # ê³„ì¸µì  êµ¬ì¡°: exo_drawsì™€ higher_order_draws ë¶„ë¦¬
                n_first_order = len(structural_model.exogenous_lvs)
                exo_draws = draw[:n_first_order]

                # 2ì°¨+ LV ì˜¤ì°¨í•­
                higher_order_draws = {}
                higher_order_lvs = structural_model.get_higher_order_lvs()
                for i, lv_name in enumerate(higher_order_lvs):
                    higher_order_draws[lv_name] = draw[n_first_order + i]

                latent_vars = structural_model.predict(
                    ind_data.iloc[0],
                    exo_draws,
                    params_dict['structural'],
                    higher_order_draws=higher_order_draws
                )
            else:
                # ë³‘ë ¬ êµ¬ì¡° (í•˜ìœ„ í˜¸í™˜)
                latent_vars = structural_model.predict(
                    ind_data.iloc[0],
                    draw,
                    params_dict['structural']
                )

            ind_lvs_list.append(latent_vars)

        all_lvs_list.append(ind_lvs_list)

    lv_time = time.time() - lv_start

    # Step 4: LVë¥¼ 3D ë°°ì—´ë¡œ ë³€í™˜ (N, R, n_lvs)
    convert_start = time.time()

    # LV ì´ë¦„ ìˆœì„œ ì •ì˜
    lv_names = list(params_dict['measurement'].keys())
    n_lvs = len(lv_names)

    # 3D ë°°ì—´ ìƒì„±: (N, R, n_lvs)
    all_lvs_array = np.zeros((n_individuals, n_draws, n_lvs))

    for ind_idx, ind_lvs_list in enumerate(all_lvs_list):
        for draw_idx, lvs_dict in enumerate(ind_lvs_list):
            for lv_idx, lv_name in enumerate(lv_names):
                all_lvs_array[ind_idx, draw_idx, lv_idx] = lvs_dict[lv_name]

    convert_time = time.time() - convert_start

    # Step 5: ì™„ì „ GPU Batchë¡œ ëª¨ë“  ê°œì¸ Ã— ëª¨ë“  drawsì˜ gradient ê³„ì‚°
    grad_start = time.time()

    # ê· ë“± ê°€ì¤‘ì¹˜ (N, R)
    all_weights = np.ones((n_individuals, n_draws)) / n_draws

    # âœ… ì¸¡ì •ëª¨ë¸ ìš°ë„ ìŠ¤ì¼€ì¼ë§ ê°€ì¤‘ì¹˜ ê³„ì‚°
    # Forward passì™€ ë™ì¼í•œ ìŠ¤ì¼€ì¼ë§ì„ Backward passì—ë„ ì ìš©
    measurement_weight = 1.0
    if use_scaling:
        n_measurement_indicators = 0
        if hasattr(gpu_measurement_model, 'models'):
            for lv_name, model in gpu_measurement_model.models.items():
                n_measurement_indicators += len(model.config.indicators)

        if n_measurement_indicators > 0:
            measurement_weight = 1.0 / n_measurement_indicators

            if iteration_logger and log_level in ['MODERATE', 'DETAILED']:
                iteration_logger.info(
                    f"\n{'='*80}\n"
                    f"ğŸ“Š Gradient ìŠ¤ì¼€ì¼ë§ ì„¤ì •\n"
                    f"{'='*80}\n"
                    f"  ì¸¡ì •ëª¨ë¸ ì§€í‘œ ìˆ˜: {n_measurement_indicators}ê°œ\n"
                    f"  ì¸¡ì •ëª¨ë¸ ê°€ì¤‘ì¹˜ (Ï‰): {measurement_weight:.6f}\n"
                    f"  âˆ‡LL_total = âˆ‡LL_choice + {measurement_weight:.6f} Ã— âˆ‡LL_measurement\n"
                    f"{'='*80}"
                )

    # ğŸš€ ì™„ì „ GPU Batch: Nëª… Ã— R draws Ã— P paramsë¥¼ ë™ì‹œ ê³„ì‚°
    # ì¸¡ì •ëª¨ë¸, êµ¬ì¡°ëª¨ë¸, ì„ íƒëª¨ë¸ gradientë¥¼ í•œ ë²ˆì— ê³„ì‚°
    all_individual_gradients = compute_full_batch_gradients_gpu(
        gpu_measurement_model,
        all_ind_data,
        all_lvs_array,
        all_ind_draws,
        params_dict,
        all_weights,
        structural_model,
        choice_model,
        lv_names,
        iteration_logger=iteration_logger,
        log_level=log_level,
        measurement_weight=measurement_weight  # âœ… ìŠ¤ì¼€ì¼ë§ ê°€ì¤‘ì¹˜ ì „ë‹¬
    )

    grad_time = time.time() - grad_start

    if iteration_logger and log_level in ['MODERATE', 'DETAILED']:
        iteration_logger.info(
            f"  Gradient ê³„ì‚° ì™„ë£Œ ({grad_time:.3f}ì´ˆ)"
        )

    total_time = time.time() - total_start

    if iteration_logger and log_level in ['MODERATE', 'DETAILED']:
        iteration_logger.info(
            f"\n{'='*80}\n"
            f"ğŸ“Š Gradient ê³„ì‚° ì™„ë£Œ\n"
            f"{'='*80}\n"
            f"  ì´ ì‹œê°„: {total_time:.3f}ì´ˆ (LV: {lv_time:.3f}ì´ˆ, Grad: {grad_time:.3f}ì´ˆ)\n"
            f"{'='*80}"
        )

    return all_individual_gradients


def compute_full_batch_gradients_gpu(
    gpu_measurement_model,
    all_ind_data: List[pd.DataFrame],
    all_lvs_array: np.ndarray,  # (N, R, n_lvs)
    all_ind_draws: np.ndarray,  # (N, R, n_dims)
    params_dict: Dict,
    all_weights: np.ndarray,  # (N, R)
    structural_model,
    choice_model,
    lv_names: List[str],
    iteration_logger=None,
    log_level: str = 'MINIMAL',
    measurement_weight: float = 1.0  # âœ… ì¸¡ì •ëª¨ë¸ ìš°ë„ ìŠ¤ì¼€ì¼ë§ ê°€ì¤‘ì¹˜
) -> List[Dict]:
    """
    ì™„ì „ GPU Batch: Nëª… Ã— R draws Ã— P paramsë¥¼ ë™ì‹œ ê³„ì‚°

    Args:
        gpu_measurement_model: GPU ì¸¡ì •ëª¨ë¸
        all_ind_data: ëª¨ë“  ê°œì¸ ë°ì´í„° (Nê°œ)
        all_lvs_array: ëª¨ë“  LV ê°’ (N, R, n_lvs)
        all_ind_draws: ëª¨ë“  draws (N, R, n_dims)
        params_dict: íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬
        all_weights: ê°€ì¤‘ì¹˜ (N, R)
        structural_model: êµ¬ì¡°ëª¨ë¸
        choice_model: ì„ íƒëª¨ë¸
        lv_names: LV ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        iteration_logger: ë¡œê±°
        log_level: ë¡œê¹… ë ˆë²¨
        measurement_weight: ì¸¡ì •ëª¨ë¸ ìš°ë„ ìŠ¤ì¼€ì¼ë§ ê°€ì¤‘ì¹˜ (ê¸°ë³¸ê°’: 1.0)

    Returns:
        ê°œì¸ë³„ gradient ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ (Nê°œ)
    """
    n_individuals, n_draws, n_lvs = all_lvs_array.shape

    # GPUë¡œ ì „ì†¡
    all_lvs_gpu = cp.asarray(all_lvs_array)  # (N, R, n_lvs)
    all_weights_gpu = cp.asarray(all_weights)  # (N, R)

    # âœ… ë™ì‹œì¶”ì •: ì¸¡ì •ëª¨ë¸ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ì œì™¸ (ê³ ì • íŒŒë¼ë¯¸í„°)
    # ì¸¡ì •ëª¨ë¸ ê·¸ë˜ë””ì–¸íŠ¸ëŠ” ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¡œ ì„¤ì •
    meas_grads = {}

    # 1. êµ¬ì¡°ëª¨ë¸ Gradient (ì™„ì „ Batch - ì²´ì¸ë£° ì—­ì „íŒŒ)
    # âœ… measurement_weight ì „ë‹¬: Forwardì™€ Backwardì˜ ìŠ¤ì¼€ì¼ë§ ì¼ì¹˜
    struct_grads = compute_structural_full_batch_gpu(
        all_ind_data,
        all_lvs_gpu,
        params_dict,  # âœ… ì „ì²´ íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬ ì „ë‹¬
        all_weights_gpu,
        structural_model,
        choice_model,
        gpu_measurement_model,
        lv_names,
        iteration_logger,
        log_level,
        measurement_weight=measurement_weight  # âœ… ìŠ¤ì¼€ì¼ë§ ê°€ì¤‘ì¹˜ ì „ë‹¬
    )

    # 3. ì„ íƒëª¨ë¸ Gradient (ì™„ì „ Batch)
    choice_grads = compute_choice_full_batch_gpu(
        all_ind_data,
        all_lvs_gpu,
        params_dict['choice'],
        all_weights_gpu,
        choice_model,
        lv_names,
        iteration_logger,
        log_level
    )

    # ê°œì¸ë³„ gradient ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
    all_individual_gradients = []
    for ind_idx in range(n_individuals):
        # ì¸¡ì •ëª¨ë¸: {lv_name: {'grad_zeta': array, 'grad_sigma_sq': array}}
        meas_dict = {}
        for lv_name in meas_grads:
            meas_dict[lv_name] = {
                'grad_zeta': meas_grads[lv_name]['grad_zeta'][ind_idx],
                'grad_sigma_sq': meas_grads[lv_name]['grad_sigma_sq'][ind_idx]
            }

        # êµ¬ì¡°ëª¨ë¸: {param_name: scalar}
        struct_dict = {key: struct_grads[key][ind_idx].item() if hasattr(struct_grads[key][ind_idx], 'item') else struct_grads[key][ind_idx] for key in struct_grads}

        # ì„ íƒëª¨ë¸: {'grad_intercept': scalar, 'grad_beta': array, ...}
        choice_dict = {}
        for key in choice_grads:
            val = choice_grads[key][ind_idx]
            # grad_betaëŠ” ë°°ì—´ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ìœ ì§€
            if key == 'grad_beta':
                choice_dict[key] = val
            elif hasattr(val, 'item'):
                choice_dict[key] = val.item()
            else:
                choice_dict[key] = val

        ind_grad_dict = {
            'measurement': meas_dict,
            'structural': struct_dict,
            'choice': choice_dict
        }
        all_individual_gradients.append(ind_grad_dict)

    return all_individual_gradients


def compute_measurement_full_batch_gpu(
    gpu_measurement_model,
    all_ind_data: List[pd.DataFrame],
    all_lvs_gpu,  # CuPy array (N, R, n_lvs)
    params: Dict,
    all_weights_gpu,  # CuPy array (N, R)
    lv_names: List[str],
    iteration_logger=None,
    log_level: str = 'MINIMAL'
) -> Dict:
    """
    ì¸¡ì •ëª¨ë¸ Gradient - ì™„ì „ GPU Batch

    âš ï¸ ì£¼ì˜: ë™ì‹œì¶”ì •ì—ì„œëŠ” ì´ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ (ì¸¡ì •ëª¨ë¸ ê³ ì •)
    ì´ í•¨ìˆ˜ëŠ” ìˆœì°¨ì¶”ì • ë˜ëŠ” CFAì—ì„œë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤.

    Returns:
        {lv_name: {'grad_zeta': (N, n_indicators), 'grad_sigma_sq': (N, n_indicators)}}
    """
    n_individuals, n_draws, n_lvs = all_lvs_gpu.shape

    gradients = {}

    for lv_idx, lv_name in enumerate(lv_names):
        zeta = params[lv_name]['zeta']
        sigma_sq = params[lv_name]['sigma_sq']
        n_indicators = len(zeta)

        config = gpu_measurement_model.models[lv_name].config

        # ëª¨ë“  ê°œì¸ì˜ ê´€ì¸¡ê°’ ì¶”ì¶œ (N, n_indicators)
        all_y = np.zeros((n_individuals, n_indicators))
        for ind_idx, ind_data in enumerate(all_ind_data):
            row = ind_data.iloc[0]
            for i, indicator in enumerate(config.indicators):
                if indicator in row.index and not pd.isna(row[indicator]):
                    all_y[ind_idx, i] = row[indicator]

        all_y_gpu = cp.asarray(all_y)  # (N, n_indicators)
        zeta_gpu = cp.asarray(zeta)  # (n_indicators,)
        sigma_sq_gpu = cp.asarray(sigma_sq)  # (n_indicators,)

        # LV ê°’ ì¶”ì¶œ: (N, R)
        lv_values_gpu = all_lvs_gpu[:, :, lv_idx]

        # Gradient ì´ˆê¸°í™”
        grad_zeta_all = cp.zeros((n_individuals, n_indicators))
        grad_sigma_sq_all = cp.zeros((n_individuals, n_indicators))

        # ê° ì§€í‘œë³„ë¡œ ê³„ì‚°
        for i in range(n_indicators):
            # ì˜ˆì¸¡ê°’: (N, R)
            y_pred = zeta_gpu[i] * lv_values_gpu

            # ì”ì°¨: (N, R)
            residual = all_y_gpu[:, i:i+1] - y_pred

            # Gradient (ê° draw): (N, R)
            grad_zeta_batch = residual * lv_values_gpu / sigma_sq_gpu[i]
            grad_sigma_sq_batch = -0.5 / sigma_sq_gpu[i] + 0.5 * (residual ** 2) / (sigma_sq_gpu[i] ** 2)

            # ê°€ì¤‘í‰ê· : (N,)
            grad_zeta_all[:, i] = cp.sum(all_weights_gpu * grad_zeta_batch, axis=1)
            grad_sigma_sq_all[:, i] = cp.sum(all_weights_gpu * grad_sigma_sq_batch, axis=1)

        # âœ… fix_first_loading ê³ ë ¤: ì²« ë²ˆì§¸ loadingì´ ê³ ì •ë˜ë©´ gradient ì œì™¸
        fix_first_loading = getattr(config, 'fix_first_loading', True)
        if fix_first_loading:
            # ì²« ë²ˆì§¸ zetaëŠ” 1.0ìœ¼ë¡œ ê³ ì • (gradient ì œì™¸)
            grad_zeta_final = cp.asnumpy(grad_zeta_all[:, 1:])  # (N, n_indicators-1)
        else:
            grad_zeta_final = cp.asnumpy(grad_zeta_all)  # (N, n_indicators)

        gradients[lv_name] = {
            'zeta': grad_zeta_final,
            'sigma_sq': cp.asnumpy(grad_sigma_sq_all)
        }

    return gradients


def compute_structural_full_batch_gpu(
    all_ind_data: List[pd.DataFrame],
    all_lvs_gpu,  # CuPy array (N, R, n_lvs)
    params_dict: Dict,  # âœ… ì „ì²´ íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬
    all_weights_gpu,  # CuPy array (N, R)
    structural_model,
    choice_model,
    gpu_measurement_model,
    lv_names: List[str],
    iteration_logger=None,
    log_level: str = 'MINIMAL',
    measurement_weight: float = 1.0  # âœ… ì¸¡ì •ëª¨ë¸ ìš°ë„ ìŠ¤ì¼€ì¼ë§ ê°€ì¤‘ì¹˜
) -> Dict:
    """
    êµ¬ì¡°ëª¨ë¸ Gradient - ì™„ì „ GPU Batch (ì²´ì¸ë£° ì—­ì „íŒŒ)

    ğŸ”´ SIGN PROTOCOL (Level 3 - Kernel):
    ==========================================
    This function computes and returns the POSITIVE GRADIENT (âˆ‡LL) - the ASCENT direction.

    Mathematical Formula:
        âˆ‚LL/âˆ‚Î³ = Î£_r w_r Ã— (Ï‰ Ã— âˆ‚LL_measurement/âˆ‚target + âˆ‚LL_choice/âˆ‚target) Ã— âˆ‚target/âˆ‚Î³

    Where:
        - âˆ‚LL/âˆ‚Î³ > 0 indicates the direction that INCREASES log-likelihood
        - Ï‰ (measurement_weight) must match the Forward pass scaling

    âš ï¸ CRITICAL: This function returns POSITIVE gradients (âˆ‡LL).
                 The negation to -âˆ‡LL (for minimization) happens ONLY at the top-level wrapper.

    Args:
        measurement_weight: ì¸¡ì •ëª¨ë¸ ìš°ë„ ìŠ¤ì¼€ì¼ë§ ê°€ì¤‘ì¹˜ (ê¸°ë³¸ê°’: 1.0)
                          Forward passì—ì„œ ì‚¬ìš©í•œ ê°’ê³¼ ë™ì¼í•´ì•¼ í•¨

    Returns:
        Dict[str, np.ndarray]: {param_name: positive_gradient_array (N,)}
                               Each value is âˆ‚LL/âˆ‚param (POSITIVE, ascent direction)
    """
    n_individuals, n_draws, n_lvs = all_lvs_gpu.shape

    # ğŸ”´ SIGN PROTOCOL: All gradients stored here are POSITIVE (âˆ‡LL)
    positive_loglike_gradients = {}

    # ê³„ì¸µì  êµ¬ì¡°ì¸ ê²½ìš°
    if hasattr(structural_model, 'is_hierarchical') and structural_model.is_hierarchical:

        for path in structural_model.hierarchical_paths:
            target = path['target']
            predictor = path['predictors'][0]  # ë‹¨ì¼ predictor ê°€ì •
            param_key = f"gamma_{predictor}_to_{target}"

            # LV ì¸ë±ìŠ¤ ì°¾ê¸°
            target_idx = lv_names.index(target)
            pred_idx = lv_names.index(predictor)

            # ëª¨ë“  ê°œì¸ì˜ ê·¸ë˜ë””ì–¸íŠ¸ ì €ì¥: (N,)
            # ğŸ”´ SIGN: This will store âˆ‚LL/âˆ‚Î³ (POSITIVE gradient)
            all_positive_grad_gamma = cp.zeros(n_individuals)

            # ê°œì¸ë³„ë¡œ ì—­ì „íŒŒ ê³„ì‚° (ê° ê°œì¸ì˜ R drawsëŠ” GPU ë°°ì¹˜)
            for ind_idx, ind_data in enumerate(all_ind_data):
                # ì´ ê°œì¸ì˜ LV ê°’: (R,)
                target_values_gpu = all_lvs_gpu[ind_idx, :, target_idx]
                pred_values_gpu = all_lvs_gpu[ind_idx, :, pred_idx]
                weights_gpu = all_weights_gpu[ind_idx, :]

                # LV ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ ìƒì„± (ì—­ì „íŒŒ í•¨ìˆ˜ìš©)
                lvs_list = []
                for draw_idx in range(n_draws):
                    lvs_dict = {lv_name: float(all_lvs_gpu[ind_idx, draw_idx, lv_idx])
                                for lv_idx, lv_name in enumerate(lv_names)}
                    lvs_list.append(lvs_dict)

                # 1. âˆ‚LL_measurement/âˆ‚target ê³„ì‚°
                # ğŸ”´ SIGN: This returns POSITIVE gradient (âˆ‚LL_meas/âˆ‚target)
                positive_grad_ll_meas_wrt_target = compute_measurement_grad_wrt_lv_gpu(
                    gpu_measurement_model,
                    ind_data,
                    lvs_list,
                    params_dict['measurement'],
                    target
                )
                positive_grad_ll_meas_wrt_target_gpu = cp.asarray(positive_grad_ll_meas_wrt_target)  # (R,)

                # 2. âˆ‚LL_choice/âˆ‚target ê³„ì‚°
                # ğŸ”´ SIGN: This returns POSITIVE gradient (âˆ‚LL_choice/âˆ‚target)
                choice_attributes = [k.replace('beta_', '') for k in params_dict['choice'].keys() if k.startswith('beta_')]

                positive_grad_ll_choice_wrt_target = compute_choice_grad_wrt_lv_gpu(
                    ind_data,
                    lvs_list,
                    params_dict['choice'],
                    target,
                    choice_attributes
                )
                positive_grad_ll_choice_wrt_target_gpu = cp.asarray(positive_grad_ll_choice_wrt_target)  # (R,)

                # 3. ì´ ê·¸ë˜ë””ì–¸íŠ¸: âˆ‚LL/âˆ‚target (ìŠ¤ì¼€ì¼ë§ ì ìš©!)
                # âœ… Forward: LL_total = LL_choice + Ï‰ Ã— LL_measurement
                # âœ… Backward: âˆ‡LL_total = âˆ‡LL_choice + Ï‰ Ã— âˆ‡LL_measurement
                # ğŸ”´ SIGN: Sum of POSITIVE gradients = POSITIVE gradient
                positive_grad_ll_wrt_target = (measurement_weight * positive_grad_ll_meas_wrt_target_gpu +
                                              positive_grad_ll_choice_wrt_target_gpu)  # (R,)

                # 4. ì²´ì¸ë£°: âˆ‚LL/âˆ‚Î³ = Î£_r w_r Ã— (âˆ‚LL/âˆ‚target)_r Ã— (âˆ‚target/âˆ‚Î³)_r
                # âˆ‚target/âˆ‚Î³ = predictor
                # ğŸ”´ SIGN: Chain rule preserves sign â†’ POSITIVE gradient
                positive_grad_gamma = cp.sum(weights_gpu * positive_grad_ll_wrt_target * pred_values_gpu)

                all_positive_grad_gamma[ind_idx] = positive_grad_gamma

            # ì ‘ë‘ì‚¬ ì—†ì´ ì €ì¥
            # ğŸ”´ SIGN: Store POSITIVE gradient (âˆ‚LL/âˆ‚Î³)
            positive_loglike_gradients[param_key] = cp.asnumpy(all_positive_grad_gamma)

    # ğŸ”´ SIGN PROTOCOL: Return POSITIVE gradients (âˆ‡LL) - Ascent direction
    return positive_loglike_gradients


def compute_choice_full_batch_gpu(
    all_ind_data: List[pd.DataFrame],
    all_lvs_gpu,  # CuPy array (N, R, 5)
    params: Dict,
    all_weights_gpu,  # CuPy array (N, R)
    choice_model,
    lv_names: List[str],
    iteration_logger=None,
    log_level: str = 'MINIMAL'
) -> Dict:
    """
    ì„ íƒëª¨ë¸ Gradient - ì™„ì „ GPU Batch

    ğŸ”´ SIGN PROTOCOL (Level 3 - Kernel):
    ==========================================
    This function computes and returns the POSITIVE GRADIENT (âˆ‡LL) - the ASCENT direction.

    Mathematical Formulas:
    ----------------------
    Binary Probit:
        âˆ‚LL/âˆ‚Î¸ = Î£ w_r * sign * mills_ratio * x  [POSITIVE, ascent]
        where sign = +1 if chosen, -1 if not chosen

    Multinomial Logit:
        âˆ‚LL/âˆ‚Î¸ = Î£ w_r * (y - P) * x  [POSITIVE, ascent]
        where y = 1 if chosen, 0 otherwise; P = choice probability

    âš ï¸ CRITICAL: This function returns POSITIVE gradients (âˆ‡LL).
                 The negation to -âˆ‡LL (for minimization) happens ONLY at the top-level wrapper.

    Returns:
        Dict[str, np.ndarray]: {param_name: gradient_array (N,)}
                               Each gradient is âˆ‚LL/âˆ‚param (POSITIVE, ascent direction)
    """
    n_individuals, n_draws, n_lvs = all_lvs_gpu.shape

    # âœ… ëª¨ë¸ íƒ€ì… í™•ì¸: ASC ê¸°ë°˜ multinomial logit vs binary probit
    use_alternative_specific = 'asc_sugar' in params or 'asc_A' in params

    # íŒŒë¼ë¯¸í„° ì¶”ì¶œ
    # âœ… Beta íŒŒë¼ë¯¸í„° (ë°°ì—´ ë˜ëŠ” ê°œë³„ í‚¤)
    if 'beta' in params:
        beta = params['beta']
    else:
        # ê°œë³„ beta í‚¤ì—ì„œ ë°°ì—´ ìƒì„± (choice_attributes ìˆœì„œëŒ€ë¡œ)
        if hasattr(choice_model, 'choice_attributes'):
            beta = np.array([params.get(f'beta_{attr}', 0.0) for attr in choice_model.choice_attributes])
        else:
            # choice_attributesê°€ ì—†ìœ¼ë©´ ì•ŒíŒŒë²³ ìˆœì„œë¡œ
            beta_keys = sorted([k for k in params.keys() if k.startswith('beta_')])
            beta = np.array([params[k] for k in beta_keys])

    n_attributes = len(beta)

    if use_alternative_specific:
        # âœ… Multinomial Logit with ASC
        asc_sugar = params.get('asc_sugar', params.get('asc_A', 0.0))
        asc_sugar_free = params.get('asc_sugar_free', params.get('asc_B', 0.0))

        # ëŒ€ì•ˆë³„ LV ê³„ìˆ˜ (theta_*)
        # âœ… ë” ê¸´ prefixë¥¼ ë¨¼ì € ì²´í¬ (theta_sugar_free_ ë¨¼ì €, theta_sugar_ ë‚˜ì¤‘ì—)
        theta_params = {}  # {(alt, lv_name): theta_value}
        for key in params:
            if key.startswith('theta_sugar_free_'):
                lv_name = key.replace('theta_sugar_free_', '')
                theta_params[('sugar_free', lv_name)] = params[key]
            elif key.startswith('theta_sugar_'):
                lv_name = key.replace('theta_sugar_', '')
                theta_params[('sugar', lv_name)] = params[key]
            elif key.startswith('theta_B_'):
                lv_name = key.replace('theta_B_', '')
                theta_params[('B', lv_name)] = params[key]
            elif key.startswith('theta_A_'):
                lv_name = key.replace('theta_A_', '')
                theta_params[('A', lv_name)] = params[key]

        # ëŒ€ì•ˆë³„ LV-Attribute ìƒí˜¸ì‘ìš© (gamma_*)
        # âœ… choice_attributesë¥¼ ì‚¬ìš©í•˜ì—¬ ì •í™•íˆ íŒŒì‹±
        choice_attributes = choice_model.config.choice_attributes
        gamma_interactions = {}  # {(alt, lv_name, attr_name): gamma_value}

        for key in params:
            if not key.startswith('gamma_') or '_to_' in key:
                continue

            # gamma_sugar_free_purchase_intention_health_label í˜•ì‹
            # â†’ alt='sugar_free', lv='purchase_intention', attr='health_label'

            if key.startswith('gamma_sugar_free_'):
                remainder = key.replace('gamma_sugar_free_', '')
                alt_name = 'sugar_free'
            elif key.startswith('gamma_sugar_'):
                remainder = key.replace('gamma_sugar_', '')
                alt_name = 'sugar'
            elif key.startswith('gamma_B_'):
                remainder = key.replace('gamma_B_', '')
                alt_name = 'B'
            elif key.startswith('gamma_A_'):
                remainder = key.replace('gamma_A_', '')
                alt_name = 'A'
            else:
                continue

            # ì†ì„± ì´ë¦„ ì°¾ê¸° (choice_attributesì—ì„œ)
            attr_name = None
            for attr in choice_attributes:
                if remainder.endswith('_' + attr):
                    attr_name = attr
                    lv_name = remainder[:-(len(attr) + 1)]  # '_attr' ì œê±°
                    break

            if attr_name and lv_name:
                gamma_interactions[(alt_name, lv_name, attr_name)] = params[key]

        all_lvs_as_main = False
        moderation_enabled = False

    else:
        # âœ… Binary Probit with intercept
        intercept = params['intercept']

        # âœ… ëª¨ë“  LV ì£¼íš¨ê³¼ vs ì¡°ì ˆíš¨ê³¼ vs ê¸°ë³¸ ëª¨ë¸ í™•ì¸
        lambda_lv_keys = [key for key in params.keys() if key.startswith('lambda_') and key not in ['lambda_main']]

        all_lvs_as_main = len(lambda_lv_keys) > 1
        moderation_enabled = 'lambda_main' in params

        # ğŸ” ë””ë²„ê¹…: params í‚¤ í™•ì¸
        if iteration_logger:
            iteration_logger.info(f"[GPU Choice Gradient] params í‚¤: {list(params.keys())}")
            iteration_logger.info(f"[GPU Choice Gradient] lambda_lv_keys: {lambda_lv_keys}")
            iteration_logger.info(f"[GPU Choice Gradient] all_lvs_as_main: {all_lvs_as_main}")

        if all_lvs_as_main:
            # ëª¨ë“  LV ì£¼íš¨ê³¼ ëª¨ë¸
            lambda_lvs = {}
            for key in lambda_lv_keys:
                lv_name = key.replace('lambda_', '')
                lambda_lvs[lv_name] = params[key]
        elif moderation_enabled:
            # ì¡°ì ˆíš¨ê³¼ ëª¨ë¸
            lambda_main = params['lambda_main']
            lambda_mod = {}
            for key in params:
                if key.startswith('lambda_mod_'):
                    mod_lv_name = key.replace('lambda_mod_', '')
                    lambda_mod[mod_lv_name] = params[key]
            main_lv = choice_model.config.main_lv
        else:
            # ê¸°ë³¸ ëª¨ë¸
            lambda_lv = params['lambda']
            # main_lv ì°¾ê¸°
            if hasattr(choice_model.config, 'main_lv'):
                main_lv = choice_model.config.main_lv
            else:
                main_lv = 'purchase_intention'  # ê¸°ë³¸ê°’

    choice_attributes = choice_model.config.choice_attributes

    if use_alternative_specific:
        # âœ… Multinomial Logit Gradient ê³„ì‚°
        return _compute_multinomial_logit_gradient_gpu(
            all_ind_data=all_ind_data,
            all_lvs_gpu=all_lvs_gpu,
            params=params,
            all_weights_gpu=all_weights_gpu,
            choice_model=choice_model,
            lv_names=lv_names,
            asc_sugar=asc_sugar,
            asc_sugar_free=asc_sugar_free,
            beta=beta,
            theta_params=theta_params,
            gamma_interactions=gamma_interactions,
            iteration_logger=iteration_logger
        )

    # âœ… Binary Probit Gradient ê³„ì‚° (ê¸°ì¡´ ë¡œì§)
    # ëª¨ë“  ê°œì¸ì˜ ì„ íƒ ë°ì´í„° ì¶”ì¶œ
    n_situations = len(all_ind_data[0])
    all_choices = np.zeros((n_individuals, n_situations))
    all_attributes = np.zeros((n_individuals, n_situations, n_attributes))

    # ì„ íƒ ë³€ìˆ˜ ì°¾ê¸°
    choice_var = None
    for col in ['choice', 'chosen', 'choice_binary']:
        if col in all_ind_data[0].columns:
            choice_var = col
            break

    for ind_idx, ind_data in enumerate(all_ind_data):
        for sit_idx in range(n_situations):
            row = ind_data.iloc[sit_idx]
            all_choices[ind_idx, sit_idx] = row[choice_var]
            for attr_idx, attr in enumerate(choice_attributes):
                if attr in row.index and not pd.isna(row[attr]):
                    all_attributes[ind_idx, sit_idx, attr_idx] = row[attr]

    # GPUë¡œ ì „ì†¡
    all_choices_gpu = cp.asarray(all_choices)  # (N, 18)
    all_attr_gpu = cp.asarray(all_attributes)  # (N, 18, 3)
    beta_gpu = cp.asarray(beta)  # (3,)

    # ì†ì„± ë°°ì¹˜: (N, 1, 18, 3)
    attr_batch = all_attr_gpu[:, None, :, :]

    # íš¨ìš© ê³„ì‚°: (N, R, 18)
    # V = intercept + Î²'X
    V_batch = intercept + cp.sum(attr_batch * beta_gpu[None, None, None, :], axis=-1)

    if all_lvs_as_main:
        # âœ… ëª¨ë“  LV ì£¼íš¨ê³¼: V += Î£(Î»_i * LV_i)
        for lv_name, lambda_val in lambda_lvs.items():
            lv_idx = lv_names.index(lv_name)
            lv_batch = all_lvs_gpu[:, :, lv_idx:lv_idx+1]  # (N, R, 1)
            V_batch = V_batch + lambda_val * lv_batch
    elif moderation_enabled:
        # ì¡°ì ˆíš¨ê³¼: V += Î»_main * PI + Î£ Î»_mod_k * (PI Ã— LV_k)
        main_lv_idx = lv_names.index(main_lv)
        main_lv_gpu = all_lvs_gpu[:, :, main_lv_idx]
        main_lv_batch = main_lv_gpu[:, :, None]  # (N, R, 1)

        V_batch = V_batch + lambda_main * main_lv_batch

        for mod_lv_name, lambda_mod_val in lambda_mod.items():
            mod_lv_idx = lv_names.index(mod_lv_name)
            mod_lv_batch = all_lvs_gpu[:, :, mod_lv_idx:mod_lv_idx+1]  # (N, R, 1)
            interaction = main_lv_batch * mod_lv_batch  # (N, R, 1)
            V_batch = V_batch + lambda_mod_val * interaction
    else:
        # ê¸°ë³¸ ëª¨ë¸: V += Î» * LV
        main_lv_idx = lv_names.index(main_lv)
        main_lv_gpu = all_lvs_gpu[:, :, main_lv_idx]
        main_lv_batch = main_lv_gpu[:, :, None]  # (N, R, 1)
        V_batch = V_batch + lambda_lv * main_lv_batch

    # í™•ë¥  ê³„ì‚°: (N, R, 18)
    prob_batch = cp_ndtr(V_batch)
    prob_batch = cp.clip(prob_batch, 1e-10, 1 - 1e-10)
    phi_batch = cp_norm_pdf(V_batch)

    # ì‹¤ì œ ì„ íƒì— ë”°ë¼: (N, R, 18)
    choices_batch = all_choices_gpu[:, None, :]  # (N, 1, 18)
    prob_final = cp.where(choices_batch == 1, prob_batch, 1 - prob_batch)

    # Mills ratio: (N, R, 18)
    mills_batch = phi_batch / prob_final
    sign_batch = cp.where(choices_batch == 1, 1.0, -1.0)

    # Weighted mills: (N, R, 18)
    weighted_mills = all_weights_gpu[:, :, None] * sign_batch * mills_batch

    # Gradient ê³„ì‚°
    gradients = {}

    # intercept: (N,)
    gradients['intercept'] = cp.asnumpy(cp.sum(weighted_mills, axis=(1, 2)))

    # beta: (N, 3)
    grad_beta = cp.sum(weighted_mills[:, :, :, None] * attr_batch, axis=(1, 2))
    gradients['beta'] = cp.asnumpy(grad_beta)

    if all_lvs_as_main:
        # âœ… ëª¨ë“  LV ì£¼íš¨ê³¼: lambda_{lv_name}
        for lv_name in lambda_lvs.keys():
            lv_idx = lv_names.index(lv_name)
            lv_batch = all_lvs_gpu[:, :, lv_idx:lv_idx+1]  # (N, R, 1)
            grad_lambda_lv = cp.sum(weighted_mills * lv_batch, axis=(1, 2))
            gradients[f'lambda_{lv_name}'] = cp.asnumpy(grad_lambda_lv)

        # âœ… LV-Attribute ìƒí˜¸ì‘ìš©: gamma_{lv_name}_{attr_name}
        # paramsì—ì„œ gamma_ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        for key in params.keys():
            if key.startswith('gamma_') and '_to_' not in key:
                # gamma_purchase_intention_health_label â†’ lv_name='purchase_intention', attr_name='health_label'
                # choice_attributesë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒì‹±
                gamma_str = key.replace('gamma_', '')
                lv_name = None
                attr_name = None

                # ê° ì†ì„± ì´ë¦„ìœ¼ë¡œ ëë‚˜ëŠ”ì§€ í™•ì¸
                for attr in choice_attributes:
                    if gamma_str.endswith('_' + attr):
                        attr_name = attr
                        lv_name = gamma_str[:-(len(attr) + 1)]  # '_attr' ì œê±°
                        break

                if lv_name and attr_name and lv_name in lv_names:
                    lv_idx = lv_names.index(lv_name)
                    attr_idx = choice_attributes.index(attr_name)
                    lv_batch = all_lvs_gpu[:, :, lv_idx]  # (N, R)
                    attr_values = all_attr_gpu[:, :, attr_idx]  # (N, 18)
                    # (N, R, 18) = (N, R, 1) * (N, 1, 18)
                    interaction = lv_batch[:, :, None] * attr_values[:, None, :]  # (N, R, 18)
                    grad_gamma = cp.sum(weighted_mills * interaction, axis=(1, 2))
                    gradients[key] = cp.asnumpy(grad_gamma)
    elif moderation_enabled:
        # ì¡°ì ˆíš¨ê³¼ ëª¨ë¸
        main_lv_idx = lv_names.index(main_lv)
        main_lv_gpu = all_lvs_gpu[:, :, main_lv_idx]
        main_lv_batch = main_lv_gpu[:, :, None]  # (N, R, 1)

        # lambda_main: (N,)
        gradients['lambda_main'] = cp.asnumpy(cp.sum(weighted_mills * main_lv_batch, axis=(1, 2)))

        # lambda_mod: (N,) for each moderator
        for mod_lv_name in lambda_mod.keys():
            mod_lv_idx = lv_names.index(mod_lv_name)
            mod_lv_batch = all_lvs_gpu[:, :, mod_lv_idx:mod_lv_idx+1]  # (N, R, 1)
            interaction = main_lv_batch * mod_lv_batch  # (N, R, 1)
            grad_lambda_mod = cp.sum(weighted_mills * interaction, axis=(1, 2))
            gradients[f'lambda_mod_{mod_lv_name}'] = cp.asnumpy(grad_lambda_mod)
    else:
        # ê¸°ë³¸ ëª¨ë¸
        main_lv_idx = lv_names.index(main_lv)
        main_lv_gpu = all_lvs_gpu[:, :, main_lv_idx]
        main_lv_batch = main_lv_gpu[:, :, None]  # (N, R, 1)

        # lambda: (N,)
        gradients['lambda'] = cp.asnumpy(cp.sum(weighted_mills * main_lv_batch, axis=(1, 2)))

    # ğŸŸ¢ SIGN CHECK: Returns POSITIVE gradients (âˆ‚LL/âˆ‚Î², âˆ‚LL/âˆ‚Î», etc.)
    # Binary Probit gradients are in the ASCENT direction (âˆ‡LL)
    return gradients


def _compute_multinomial_logit_gradient_gpu(
    all_ind_data: List[pd.DataFrame],
    all_lvs_gpu,  # CuPy array (N, R, n_lvs)
    params: Dict,
    all_weights_gpu,  # CuPy array (N, R)
    choice_model,
    lv_names: List[str],
    asc_sugar: float,
    asc_sugar_free: float,
    beta: np.ndarray,
    theta_params: Dict,  # {(alt, lv_name): theta_value}
    gamma_interactions: Dict,  # {(alt, lv_name, attr_name): gamma_value}
    iteration_logger=None
) -> Dict:
    """
    Multinomial Logit Gradient - ì™„ì „ GPU Batch

    ğŸ”´ SIGN PROTOCOL (Level 3 - Kernel):
    ==========================================
    This function computes and returns the POSITIVE GRADIENT (âˆ‡LL) - the ASCENT direction.

    Mathematical Formula:
    ---------------------
    âˆ‚LL/âˆ‚Î¸ = Î£_n Î£_r w_r * (y_ni - P_ni) * x_ni  [POSITIVE, ascent]

    ì—¬ê¸°ì„œ:
    - y_ni: ëŒ€ì•ˆ iê°€ ì„ íƒë˜ì—ˆìœ¼ë©´ 1, ì•„ë‹ˆë©´ 0
    - P_ni: ëŒ€ì•ˆ iì˜ ì„ íƒ í™•ë¥  (softmax)
    - x_ni: ëŒ€ì•ˆ iì˜ ì†ì„± (ë˜ëŠ” LV)
    - w_r: importance weight

    âš ï¸ CRITICAL: The formula (y - P) gives POSITIVE gradient (âˆ‡LL).
                 DO NOT change to (P - y) which would give NEGATIVE gradient!
                 The negation to -âˆ‡LL (for minimization) happens ONLY at the top-level wrapper.

    Returns:
        Dict[str, np.ndarray]: {param_name: gradient_array (N,)}
                               Each gradient is âˆ‚LL/âˆ‚param (POSITIVE, ascent direction)
    """
    n_individuals, n_draws, n_lvs = all_lvs_gpu.shape
    choice_attributes = choice_model.config.choice_attributes
    n_attributes = len(beta)

    # ë°ì´í„° ì¶”ì¶œ: sugar_content ê¸°ì¤€
    # ê° ê°œì¸ì˜ ë°ì´í„°ëŠ” choice set ë‹¨ìœ„ë¡œ êµ¬ì„± (3ê°œ í–‰ = 1 choice set)
    # ì˜ˆ: 18ê°œ í–‰ = 6 choice sets
    n_rows_per_ind = len(all_ind_data[0])
    n_choice_sets = n_rows_per_ind // 3  # 3ê°œ ëŒ€ì•ˆ

    # ëª¨ë“  ê°œì¸ì˜ ì„ íƒ ë°ì´í„° ì¶”ì¶œ
    # sugar_contents: (N, n_choice_sets, 3) - ê° choice setì˜ 3ê°œ ëŒ€ì•ˆì˜ sugar_content
    # choices: (N, n_choice_sets) - ì„ íƒëœ ëŒ€ì•ˆ ì¸ë±ìŠ¤ (0=ì¼ë°˜ë‹¹, 1=ë¬´ì„¤íƒ•, 2=opt-out)
    # attributes: (N, n_choice_sets, 3, n_attr) - ê° ëŒ€ì•ˆì˜ ì†ì„±

    sugar_contents_list = []
    choices_list = []
    attributes_list = []

    for ind_idx, ind_data in enumerate(all_ind_data):
        ind_sugar_contents = []
        ind_choices = []
        ind_attributes = []

        for cs_idx in range(n_choice_sets):
            # 3ê°œ í–‰ ì¶”ì¶œ
            start_row = cs_idx * 3
            choice_set = ind_data.iloc[start_row:start_row+3]

            # sugar_content ì¶”ì¶œ
            sc_values = []
            attrs = []
            chosen_alt = -1

            for alt_idx, (_, row) in enumerate(choice_set.iterrows()):
                sc = row.get('sugar_content', np.nan)
                if pd.isna(sc):
                    sc_values.append('opt_out')
                else:
                    sc_values.append(sc)

                # ì†ì„± ì¶”ì¶œ
                attr_vec = np.zeros(n_attributes)
                for attr_idx, attr in enumerate(choice_attributes):
                    if attr in row.index and not pd.isna(row[attr]):
                        attr_vec[attr_idx] = row[attr]
                attrs.append(attr_vec)

                # ì„ íƒ í™•ì¸
                if row.get('choice', 0) == 1 or row.get('chosen', 0) == 1:
                    chosen_alt = alt_idx

            ind_sugar_contents.append(sc_values)
            ind_choices.append(chosen_alt)
            ind_attributes.append(attrs)

        sugar_contents_list.append(ind_sugar_contents)
        choices_list.append(ind_choices)
        attributes_list.append(ind_attributes)

    # NumPy ë°°ì—´ë¡œ ë³€í™˜
    choices = np.array(choices_list)  # (N, n_choice_sets)
    attributes = np.array(attributes_list)  # (N, n_choice_sets, 3, n_attr)

    # GPUë¡œ ì „ì†¡
    choices_gpu = cp.asarray(choices)  # (N, n_choice_sets)
    attributes_gpu = cp.asarray(attributes)  # (N, n_choice_sets, 3, n_attr)
    beta_gpu = cp.asarray(beta)  # (n_attr,)

    # íš¨ìš© ê³„ì‚°: (N, R, n_choice_sets, 3)
    # V[n, r, cs, alt] = ASC[alt] + beta' * X[n, cs, alt] + theta[alt, lv] * LV[n, r, lv] + ...

    V_batch = cp.zeros((n_individuals, n_draws, n_choice_sets, 3))

    # ê° choice set, ê° ëŒ€ì•ˆì— ëŒ€í•´ íš¨ìš© ê³„ì‚°
    for cs_idx in range(n_choice_sets):
        for ind_idx in range(n_individuals):
            sc_values = sugar_contents_list[ind_idx][cs_idx]

            for alt_idx in range(3):
                sc = sc_values[alt_idx]

                # ASC
                # âœ… ë°ì´í„°ì—ëŠ” 'ì•Œë°˜ë‹¹'ìœ¼ë¡œ ì €ì¥ë¨ ('ì¼ë°˜ë‹¹' ì•„ë‹˜!)
                if sc == 'ì•Œë°˜ë‹¹' or sc == 'ì¼ë°˜ë‹¹':
                    asc = asc_sugar
                    alt_name = 'sugar'
                elif sc == 'ë¬´ì„¤íƒ•':
                    asc = asc_sugar_free
                    alt_name = 'sugar_free'
                else:  # opt-out
                    asc = 0.0
                    alt_name = 'opt_out'

                # ì†ì„± íš¨ê³¼: beta' * X
                attr_vec = attributes_gpu[ind_idx, cs_idx, alt_idx, :]  # (n_attr,)
                attr_effect = cp.sum(beta_gpu * attr_vec)

                # ê¸°ë³¸ íš¨ìš©
                V_batch[ind_idx, :, cs_idx, alt_idx] = asc + attr_effect

                # ì ì¬ë³€ìˆ˜ ì£¼íš¨ê³¼: theta * LV
                if alt_name != 'opt_out':
                    for lv_name in lv_names:
                        theta_key = (alt_name, lv_name)
                        if theta_key in theta_params:
                            theta = theta_params[theta_key]
                            lv_idx = lv_names.index(lv_name)
                            lv_values = all_lvs_gpu[ind_idx, :, lv_idx]  # (R,)
                            V_batch[ind_idx, :, cs_idx, alt_idx] += theta * lv_values

                # ìƒí˜¸ì‘ìš©: gamma * LV * Attribute
                if alt_name != 'opt_out':
                    for (gamma_alt, gamma_lv, gamma_attr), gamma_val in gamma_interactions.items():
                        if gamma_alt == alt_name:
                            lv_idx = lv_names.index(gamma_lv)
                            attr_idx = choice_attributes.index(gamma_attr)
                            lv_values = all_lvs_gpu[ind_idx, :, lv_idx]  # (R,)
                            attr_value = attributes_gpu[ind_idx, cs_idx, alt_idx, attr_idx]
                            V_batch[ind_idx, :, cs_idx, alt_idx] += gamma_val * lv_values * attr_value

    # í™•ë¥  ê³„ì‚°: Softmax
    # P[n, r, cs, alt] = exp(V[n, r, cs, alt]) / Î£_j exp(V[n, r, cs, j])
    exp_V = cp.exp(V_batch)  # (N, R, n_choice_sets, 3)
    sum_exp_V = cp.sum(exp_V, axis=3, keepdims=True)  # (N, R, n_choice_sets, 1)
    P_batch = exp_V / sum_exp_V  # (N, R, n_choice_sets, 3)
    P_batch = cp.clip(P_batch, 1e-10, 1 - 1e-10)

    # ì„ íƒ ì§€ì‹œì: y[n, cs, alt]
    y_batch = cp.zeros((n_individuals, n_choice_sets, 3))
    for ind_idx in range(n_individuals):
        for cs_idx in range(n_choice_sets):
            chosen_alt = choices[ind_idx, cs_idx]
            if 0 <= chosen_alt < 3:
                y_batch[ind_idx, cs_idx, chosen_alt] = 1.0

    y_batch_gpu = cp.asarray(y_batch)  # (N, n_choice_sets, 3)

    # Gradient ê³„ì‚°: (y - P) * x
    # diff: (N, R, n_choice_sets, 3)
    diff = y_batch_gpu[:, None, :, :] - P_batch  # (N, 1, n_choice_sets, 3) - (N, R, n_choice_sets, 3)

    # Weighted diff: (N, R, n_choice_sets, 3)
    weighted_diff = all_weights_gpu[:, :, None, None] * diff  # (N, R, 1, 1) * (N, R, n_choice_sets, 3)

    gradients = {}

    # ASC gradients
    # asc_sugar: sum over (ì¼ë°˜ë‹¹ ëŒ€ì•ˆ)
    # asc_sugar_free: sum over (ë¬´ì„¤íƒ• ëŒ€ì•ˆ)
    grad_asc_sugar = cp.zeros(n_individuals)
    grad_asc_sugar_free = cp.zeros(n_individuals)

    for ind_idx in range(n_individuals):
        for cs_idx in range(n_choice_sets):
            sc_values = sugar_contents_list[ind_idx][cs_idx]
            for alt_idx in range(3):
                sc = sc_values[alt_idx]
                # âœ… ë°ì´í„°ì—ëŠ” 'ì•Œë°˜ë‹¹'ìœ¼ë¡œ ì €ì¥ë¨ ('ì¼ë°˜ë‹¹' ì•„ë‹˜!)
                if sc == 'ì•Œë°˜ë‹¹' or sc == 'ì¼ë°˜ë‹¹':
                    grad_asc_sugar[ind_idx] += cp.sum(weighted_diff[ind_idx, :, cs_idx, alt_idx])
                elif sc == 'ë¬´ì„¤íƒ•':
                    grad_asc_sugar_free[ind_idx] += cp.sum(weighted_diff[ind_idx, :, cs_idx, alt_idx])

    gradients['asc_sugar'] = cp.asnumpy(grad_asc_sugar)
    gradients['asc_sugar_free'] = cp.asnumpy(grad_asc_sugar_free)

    # Beta gradients: (N, n_attr)
    # âˆ‚LL/âˆ‚Î²_k = Î£_n Î£_r Î£_cs Î£_alt w_r * (y - P) * X[alt, k]
    grad_beta = cp.zeros((n_individuals, n_attributes))
    for attr_idx in range(n_attributes):
        attr_values = attributes_gpu[:, :, :, attr_idx]  # (N, n_choice_sets, 3)
        # (N, R, n_choice_sets, 3) * (N, 1, n_choice_sets, 3)
        grad_beta[:, attr_idx] = cp.sum(
            weighted_diff * attr_values[:, None, :, :],
            axis=(1, 2, 3)
        )
    gradients['beta'] = cp.asnumpy(grad_beta)

    # Theta gradients: (N,) for each (alt, lv)
    for (alt_name, lv_name), theta_val in theta_params.items():
        grad_theta = cp.zeros(n_individuals)
        lv_idx = lv_names.index(lv_name)

        for ind_idx in range(n_individuals):
            for cs_idx in range(n_choice_sets):
                sc_values = sugar_contents_list[ind_idx][cs_idx]
                for alt_idx in range(3):
                    sc = sc_values[alt_idx]
                    # âœ… ë°ì´í„°ì—ëŠ” 'ì•Œë°˜ë‹¹'ìœ¼ë¡œ ì €ì¥ë¨ ('ì¼ë°˜ë‹¹' ì•„ë‹˜!)
                    if ((sc == 'ì•Œë°˜ë‹¹' or sc == 'ì¼ë°˜ë‹¹') and alt_name == 'sugar') or \
                       (sc == 'ë¬´ì„¤íƒ•' and alt_name == 'sugar_free'):
                        lv_values = all_lvs_gpu[ind_idx, :, lv_idx]  # (R,)
                        grad_theta[ind_idx] += cp.sum(
                            weighted_diff[ind_idx, :, cs_idx, alt_idx] * lv_values
                        )

        gradients[f'theta_{alt_name}_{lv_name}'] = cp.asnumpy(grad_theta)

    # Gamma gradients: (N,) for each (alt, lv, attr)
    for (alt_name, lv_name, attr_name), gamma_val in gamma_interactions.items():
        grad_gamma = cp.zeros(n_individuals)
        lv_idx = lv_names.index(lv_name)
        attr_idx = choice_attributes.index(attr_name)

        for ind_idx in range(n_individuals):
            for cs_idx in range(n_choice_sets):
                sc_values = sugar_contents_list[ind_idx][cs_idx]
                for alt_idx in range(3):
                    sc = sc_values[alt_idx]
                    # âœ… ë°ì´í„°ì—ëŠ” 'ì•Œë°˜ë‹¹'ìœ¼ë¡œ ì €ì¥ë¨ ('ì¼ë°˜ë‹¹' ì•„ë‹˜!)
                    if ((sc == 'ì•Œë°˜ë‹¹' or sc == 'ì¼ë°˜ë‹¹') and alt_name == 'sugar') or \
                       (sc == 'ë¬´ì„¤íƒ•' and alt_name == 'sugar_free'):
                        lv_values = all_lvs_gpu[ind_idx, :, lv_idx]  # (R,)
                        attr_value = attributes_gpu[ind_idx, cs_idx, alt_idx, attr_idx]
                        grad_gamma[ind_idx] += cp.sum(
                            weighted_diff[ind_idx, :, cs_idx, alt_idx] * lv_values * attr_value
                        )

        gradients[f'gamma_{alt_name}_{lv_name}_{attr_name}'] = cp.asnumpy(grad_gamma)

    # ğŸŸ¢ SIGN CHECK: Returns POSITIVE gradients (âˆ‚LL/âˆ‚ASC, âˆ‚LL/âˆ‚Î², âˆ‚LL/âˆ‚Î¸, âˆ‚LL/âˆ‚Î³)
    # Multinomial Logit gradients use (y - P) formula, which gives ASCENT direction (âˆ‡LL)
    return gradients


