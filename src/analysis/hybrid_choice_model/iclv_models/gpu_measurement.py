"""
GPU-accelerated Ordered Probit Measurement Model

CuPyë¥¼ ì‚¬ìš©í•˜ì—¬ ì¸¡ì •ëª¨ë¸ì˜ í•µì‹¬ ì—°ì‚°ì„ GPUì—ì„œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

import numpy as np
import pandas as pd
from typing import Dict, Optional
import logging

logger = logging.getLogger(__name__)

# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
try:
    import cupy as cp
    from cupyx.scipy.special import ndtr  # í‘œì¤€ì •ê·œë¶„í¬ CDF
    GPU_AVAILABLE = True
    logger.info("âœ… CuPy ë¡œë“œ ì„±ê³µ - GPU ê°€ì† ì‚¬ìš© ê°€ëŠ¥")
except ImportError as e:
    GPU_AVAILABLE = False
    logger.warning(f"âš ï¸ CuPy ë¡œë“œ ì‹¤íŒ¨ - CPU ëª¨ë“œë¡œ ì‘ë™: {e}")
    cp = None
    ndtr = None


class GPUOrderedProbitMeasurement:
    """
    GPU ê°€ì† Ordered Probit ì¸¡ì •ëª¨ë¸
    
    ì „ëµ: í•µì‹¬ ì—°ì‚°(ì •ê·œë¶„í¬ CDF)ë§Œ GPU ì‚¬ìš©
    - ê°„ë‹¨í•œ êµ¬í˜„
    - ìµœì†Œí•œì˜ ì½”ë“œ ë³€ê²½
    - CPU-GPU ì „ì†¡ ìµœì†Œí™”
    """
    
    def __init__(self, config, use_gpu: bool = True):
        """
        ì´ˆê¸°í™”
        
        Args:
            config: MeasurementConfig ê°ì²´
            use_gpu: GPU ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
        """
        self.config = config
        self.n_indicators = len(config.indicators)
        self.n_categories = config.n_categories
        self.n_thresholds = config.n_categories - 1
        
        # GPU ì‚¬ìš© ì„¤ì •
        self.use_gpu = use_gpu and GPU_AVAILABLE
        
        if self.use_gpu:
            self.xp = cp  # CuPy
            self.norm_cdf = ndtr  # GPU CDF
            logger.info(f"ğŸš€ GPU ëª¨ë“œ í™œì„±í™”: {self.n_indicators}ê°œ ì§€í‘œ")
        else:
            self.xp = np  # NumPy
            from scipy.stats import norm
            self.norm_cdf = norm.cdf  # CPU CDF
            logger.info(f"ğŸ’» CPU ëª¨ë“œ: {self.n_indicators}ê°œ ì§€í‘œ")
        
        self.fitted = False
    
    def _compute_ordered_probit_prob(self, y_value: int, linear_pred: float, 
                                     tau_indicator: np.ndarray) -> float:
        """
        Ordered Probit í™•ë¥  ê³„ì‚° (GPU ê°€ì†)
        
        P(Y = k) = Î¦(Ï„_k - V) - Î¦(Ï„_{k-1} - V)
        
        Args:
            y_value: ê´€ì¸¡ê°’ (1, 2, 3, 4, 5)
            linear_pred: ì„ í˜• ì˜ˆì¸¡ê°’ (Î¶ * LV)
            tau_indicator: í•´ë‹¹ ì§€í‘œì˜ ì„ê³„ê°’ (n_thresholds,)
        
        Returns:
            í™•ë¥ ê°’
        """
        k = int(y_value) - 1  # 0-based index
        
        # ìƒí•œ/í•˜í•œ ê³„ì‚°
        if k == 0:
            # ì²« ë²ˆì§¸ ë²”ì£¼: P(Y=1) = Î¦(Ï„_1 - V)
            upper = tau_indicator[0] - linear_pred
            if self.use_gpu:
                upper_gpu = self.xp.array(upper)
                prob = float(self.norm_cdf(upper_gpu).get())
            else:
                prob = self.norm_cdf(upper)
        elif k == self.n_categories - 1:
            # ë§ˆì§€ë§‰ ë²”ì£¼: P(Y=5) = 1 - Î¦(Ï„_4 - V)
            lower = tau_indicator[-1] - linear_pred
            if self.use_gpu:
                lower_gpu = self.xp.array(lower)
                prob = float(1.0 - self.norm_cdf(lower_gpu).get())
            else:
                prob = 1.0 - self.norm_cdf(lower)
        else:
            # ì¤‘ê°„ ë²”ì£¼: P(Y=k) = Î¦(Ï„_k - V) - Î¦(Ï„_{k-1} - V)
            upper = tau_indicator[k] - linear_pred
            lower = tau_indicator[k-1] - linear_pred
            if self.use_gpu:
                bounds_gpu = self.xp.array([upper, lower])
                cdf_vals = self.norm_cdf(bounds_gpu)
                prob = float((cdf_vals[0] - cdf_vals[1]).get())
            else:
                prob = self.norm_cdf(upper) - self.norm_cdf(lower)
        
        return max(prob, 1e-10)
    
    def log_likelihood(self, data: pd.DataFrame, latent_var: float,
                      params: Dict[str, np.ndarray]) -> float:
        """
        ë¡œê·¸ìš°ë„ ê³„ì‚° (GPU ê°€ì†)
        
        Args:
            data: ê´€ì¸¡ì§€í‘œ ë°ì´í„°
            latent_var: ì ì¬ë³€ìˆ˜ ê°’
            params: íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬
                - 'zeta': ìš”ì¸ì ì¬ëŸ‰ (n_indicators,)
                - 'tau': ì„ê³„ê°’ (n_indicators, n_thresholds)
        
        Returns:
            ë¡œê·¸ìš°ë„ ê°’
        """
        zeta = params['zeta']
        tau = params['tau']
        
        total_ll = 0.0
        first_row = data.iloc[0]
        
        # ê° ì§€í‘œì— ëŒ€í•´
        for i, indicator in enumerate(self.config.indicators):
            if indicator not in first_row.index:
                continue
            
            y_value = first_row[indicator]
            
            if pd.isna(y_value):
                continue
            
            # ì„ í˜• ì˜ˆì¸¡: V = Î¶ * LV
            linear_pred = zeta[i] * latent_var
            
            # Ordered Probit í™•ë¥  (GPU ê°€ì†)
            prob = self._compute_ordered_probit_prob(
                y_value, linear_pred, tau[i]
            )
            
            # ë¡œê·¸ìš°ë„ ëˆ„ì 
            total_ll += np.log(prob)
        
        return total_ll
    
    def initialize_parameters(self) -> Dict[str, np.ndarray]:
        """íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”"""
        params = {
            'zeta': np.ones(self.n_indicators),
            'tau': np.zeros((self.n_indicators, self.n_thresholds))
        }
        
        # ì„ê³„ê°’ ì´ˆê¸°í™” (ê· ë“± ê°„ê²©)
        for i in range(self.n_indicators):
            params['tau'][i] = np.linspace(-2, 2, self.n_thresholds)
        
        return params


class GPUBatchOrderedProbitMeasurement:
    """
    GPU ë°°ì¹˜ ì²˜ë¦¬ Ordered Probit ì¸¡ì •ëª¨ë¸
    
    ì „ëµ: ì—¬ëŸ¬ ê°œì¸ì„ í•œë²ˆì— GPUë¡œ ì²˜ë¦¬
    - ìµœëŒ€ GPU í™œìš©
    - CPU-GPU ì „ì†¡ ìµœì†Œí™”
    - ë†’ì€ ì†ë„ í–¥ìƒ
    """
    
    def __init__(self, config, use_gpu: bool = True):
        """
        ì´ˆê¸°í™”
        
        Args:
            config: MeasurementConfig ê°ì²´
            use_gpu: GPU ì‚¬ìš© ì—¬ë¶€
        """
        self.config = config
        self.n_indicators = len(config.indicators)
        self.n_categories = config.n_categories
        self.n_thresholds = config.n_categories - 1
        
        self.use_gpu = use_gpu and GPU_AVAILABLE
        
        if self.use_gpu:
            self.xp = cp
            logger.info(f"ğŸš€ GPU ë°°ì¹˜ ëª¨ë“œ: {self.n_indicators}ê°œ ì§€í‘œ")
        else:
            self.xp = np
            logger.info(f"ğŸ’» CPU ë°°ì¹˜ ëª¨ë“œ: {self.n_indicators}ê°œ ì§€í‘œ")
        
        self.fitted = False
    
    def log_likelihood_batch(self, data_batch: np.ndarray, 
                            latent_vars_batch: np.ndarray,
                            params: Dict[str, np.ndarray]) -> np.ndarray:
        """
        ë°°ì¹˜ ë¡œê·¸ìš°ë„ ê³„ì‚° (GPU ê°€ì†)
        
        Args:
            data_batch: (n_persons, n_indicators) - ê´€ì¸¡ê°’
            latent_vars_batch: (n_persons,) - ì ì¬ë³€ìˆ˜
            params: íŒŒë¼ë¯¸í„°
        
        Returns:
            (n_persons,) - ê° ê°œì¸ì˜ ë¡œê·¸ìš°ë„
        """
        if not self.use_gpu:
            # CPU ëª¨ë“œ: ìˆœì°¨ ì²˜ë¦¬
            return self._log_likelihood_batch_cpu(
                data_batch, latent_vars_batch, params
            )
        
        # GPUë¡œ ì „ì†¡
        data_gpu = self.xp.array(data_batch)  # (n_persons, n_indicators)
        lv_gpu = self.xp.array(latent_vars_batch)  # (n_persons,)
        zeta_gpu = self.xp.array(params['zeta'])  # (n_indicators,)
        tau_gpu = self.xp.array(params['tau'])  # (n_indicators, n_thresholds)
        
        n_persons = data_gpu.shape[0]
        
        # ì„ í˜• ì˜ˆì¸¡: (n_persons, n_indicators)
        linear_pred = self.xp.outer(lv_gpu, zeta_gpu)
        
        # ë¡œê·¸ìš°ë„ ì´ˆê¸°í™”
        ll_batch = self.xp.zeros(n_persons)
        
        # ê° ì§€í‘œì— ëŒ€í•´
        for i in range(self.n_indicators):
            y_values = data_gpu[:, i]  # (n_persons,)
            linear_pred_i = linear_pred[:, i]  # (n_persons,)
            tau_i = tau_gpu[i]  # (n_thresholds,)
            
            # ê° ë²”ì£¼ì— ëŒ€í•´ í™•ë¥  ê³„ì‚°
            for k in range(self.n_categories):
                mask = (y_values == (k + 1))  # í•´ë‹¹ ë²”ì£¼ì¸ ê°œì¸ë“¤
                
                if self.xp.sum(mask) == 0:
                    continue
                
                # í™•ë¥  ê³„ì‚°
                if k == 0:
                    upper = tau_i[0] - linear_pred_i[mask]
                    prob = ndtr(upper)
                elif k == self.n_categories - 1:
                    lower = tau_i[-1] - linear_pred_i[mask]
                    prob = 1.0 - ndtr(lower)
                else:
                    upper = tau_i[k] - linear_pred_i[mask]
                    lower = tau_i[k-1] - linear_pred_i[mask]
                    prob = ndtr(upper) - ndtr(lower)
                
                # ë¡œê·¸ìš°ë„ ëˆ„ì 
                ll_batch[mask] += self.xp.log(self.xp.maximum(prob, 1e-10))
        
        # CPUë¡œ ë°˜í™˜
        return self.xp.asnumpy(ll_batch)
    
    def _log_likelihood_batch_cpu(self, data_batch: np.ndarray,
                                   latent_vars_batch: np.ndarray,
                                   params: Dict[str, np.ndarray]) -> np.ndarray:
        """CPU ë°°ì¹˜ ì²˜ë¦¬"""
        from scipy.stats import norm
        
        n_persons = data_batch.shape[0]
        ll_batch = np.zeros(n_persons)
        
        zeta = params['zeta']
        tau = params['tau']
        
        for person_idx in range(n_persons):
            lv = latent_vars_batch[person_idx]
            
            for i in range(self.n_indicators):
                y_value = data_batch[person_idx, i]
                
                if np.isnan(y_value):
                    continue
                
                linear_pred = zeta[i] * lv
                k = int(y_value) - 1
                
                if k == 0:
                    prob = norm.cdf(tau[i, 0] - linear_pred)
                elif k == self.n_categories - 1:
                    prob = 1.0 - norm.cdf(tau[i, -1] - linear_pred)
                else:
                    prob = norm.cdf(tau[i, k] - linear_pred) - \
                           norm.cdf(tau[i, k-1] - linear_pred)
                
                ll_batch[person_idx] += np.log(max(prob, 1e-10))
        
        return ll_batch
    
    def initialize_parameters(self) -> Dict[str, np.ndarray]:
        """íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”"""
        params = {
            'zeta': np.ones(self.n_indicators),
            'tau': np.zeros((self.n_indicators, self.n_thresholds))
        }
        
        for i in range(self.n_indicators):
            params['tau'][i] = np.linspace(-2, 2, self.n_thresholds)
        
        return params

