"""
GPU-Accelerated Ordered Probit Measurement Model for ICLV

CuPyë¥¼ ì‚¬ìš©í•˜ì—¬ GPUì—ì„œ ì¸¡ì •ëª¨ë¸ ìš°ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
"""

import numpy as np
import pandas as pd
from typing import Dict, Optional
import logging

logger = logging.getLogger(__name__)

# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
try:
    import os
    # CUDA ê²½ë¡œ ì„¤ì • (Windows)
    cuda_path = os.environ.get('CUDA_PATH', r'C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v13.0')
    if os.path.exists(cuda_path):
        cuda_bin = os.path.join(cuda_path, 'bin')
        if cuda_bin not in os.environ.get('PATH', ''):
            os.environ['PATH'] = cuda_bin + os.pathsep + os.environ.get('PATH', '')

    import cupy as cp
    from cupyx.scipy.special import ndtr  # í‘œì¤€ì •ê·œ CDF

    # GPU ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í…ŒìŠ¤íŠ¸
    try:
        cp.cuda.Device(0).use()
        _ = cp.array([1, 2, 3])  # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
        GPU_AVAILABLE = True
        logger.info("âœ… CuPy ë¡œë“œ ì„±ê³µ - GPU ê°€ì† ì‚¬ìš© ê°€ëŠ¥")
    except Exception as e:
        GPU_AVAILABLE = False
        logger.warning(f"âš ï¸ GPU ì´ˆê¸°í™” ì‹¤íŒ¨ - CPU ëª¨ë“œë¡œ ì‘ë™: {e}")
        cp = None
        ndtr = None

except ImportError as e:
    GPU_AVAILABLE = False
    logger.warning(f"âš ï¸ CuPy ë¯¸ì„¤ì¹˜ - CPU ëª¨ë“œë¡œ ì‘ë™: {e}")
    cp = None
    ndtr = None
except Exception as e:
    GPU_AVAILABLE = False
    logger.warning(f"âš ï¸ GPU ë¡œë“œ ì‹¤íŒ¨ - CPU ëª¨ë“œë¡œ ì‘ë™: {e}")
    cp = None
    ndtr = None


class GPUOrderedProbitMeasurement:
    """
    GPU ê°€ì† Ordered Probit ì¸¡ì •ëª¨ë¸
    
    CuPyë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ê·œë¶„í¬ CDF ê³„ì‚°ì„ GPUì—ì„œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    Model:
        P(Y_i = k) = Î¦(Ï„_k - Î¶*LV) - Î¦(Ï„_{k-1} - Î¶*LV)
    """
    
    def __init__(self, config, use_gpu: bool = True):
        """
        ì´ˆê¸°í™”

        Args:
            config: MeasurementConfig ê°ì²´
            use_gpu: GPU ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
        """
        self.config = config
        self.n_indicators = len(config.indicators)
        self.n_categories = config.n_categories
        self.n_thresholds = config.n_categories - 1
        self.measurement_method = 'ordered_probit'  # âœ… ì¸¡ì • ë°©ë²• ëª…ì‹œ

        # GPU ì‚¬ìš© ì„¤ì •
        self.use_gpu = use_gpu and GPU_AVAILABLE

        if self.use_gpu:
            self.xp = cp
            logger.info(f"ğŸš€ GPU ëª¨ë“œ í™œì„±í™”: {self.n_indicators}ê°œ ì§€í‘œ")
        else:
            self.xp = np
            if use_gpu and not GPU_AVAILABLE:
                logger.warning("âš ï¸ GPU ìš”ì²­ë˜ì—ˆìœ¼ë‚˜ CuPy ë¯¸ì„¤ì¹˜ - CPU ëª¨ë“œ ì‚¬ìš©")
            else:
                logger.info(f"ğŸ’» CPU ëª¨ë“œ: {self.n_indicators}ê°œ ì§€í‘œ")
        
        self.zeta = None
        self.tau = None
        self.fitted = False
    
    def _norm_cdf(self, x):
        """í‘œì¤€ì •ê·œ ëˆ„ì ë¶„í¬í•¨ìˆ˜ (GPU/CPU ìë™ ì„ íƒ)"""
        if self.use_gpu:
            return ndtr(x)
        else:
            from scipy.stats import norm
            return norm.cdf(x)
    
    def log_likelihood(self, data: pd.DataFrame, latent_var: float,
                      params: Dict[str, np.ndarray]) -> float:
        """
        ë¡œê·¸ìš°ë„ ê³„ì‚° (GPU ê°€ì†)
        
        Args:
            data: ê´€ì¸¡ì§€í‘œ ë°ì´í„°
            latent_var: ì ì¬ë³€ìˆ˜ ê°’
            params: íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬
                - 'zeta': ìš”ì¸ì ì¬ëŸ‰ (n_indicators,)
                - 'tau': ì„ê³„ê°’ (n_indicators, n_thresholds)
        
        Returns:
            ë¡œê·¸ìš°ë„ ê°’
        """
        zeta = params['zeta']
        tau = params['tau']
        
        # GPUë¡œ ì „ì†¡ (í•„ìš” ì‹œ)
        if self.use_gpu:
            zeta = cp.asarray(zeta)
            tau = cp.asarray(tau)
        
        total_ll = 0.0
        first_row = data.iloc[0]

        # ê° ì§€í‘œì— ëŒ€í•´
        for i, indicator in enumerate(self.config.indicators):
            if indicator not in first_row.index:
                continue

            # NaN ê°’ ì²˜ë¦¬
            if pd.isna(first_row[indicator]):
                continue

            y_obs = int(first_row[indicator])

            if y_obs < 1 or y_obs > self.n_categories:
                continue
            
            # ì„ í˜• ì˜ˆì¸¡: V = Î¶ * LV
            linear_pred = zeta[i] * latent_var
            
            # ì„ê³„ê°’
            tau_i = tau[i]
            
            # í™•ë¥  ê³„ì‚°: P(Y=k) = Î¦(Ï„_k - V) - Î¦(Ï„_{k-1} - V)
            if y_obs == 1:
                # P(Y=1) = Î¦(Ï„_1 - V)
                upper = tau_i[0] - linear_pred
                prob = self._norm_cdf(upper)
            elif y_obs == self.n_categories:
                # P(Y=K) = 1 - Î¦(Ï„_{K-1} - V)
                lower = tau_i[-1] - linear_pred
                prob = 1.0 - self._norm_cdf(lower)
            else:
                # P(Y=k) = Î¦(Ï„_k - V) - Î¦(Ï„_{k-1} - V)
                upper = tau_i[y_obs - 1] - linear_pred
                lower = tau_i[y_obs - 2] - linear_pred
                prob = self._norm_cdf(upper) - self._norm_cdf(lower)
            
            # GPUì—ì„œ CPUë¡œ ë³€í™˜ (í•„ìš” ì‹œ)
            if self.use_gpu:
                prob = float(cp.asnumpy(prob))
            
            # ë¡œê·¸ìš°ë„ ëˆ„ì 
            prob = max(prob, 1e-10)
            total_ll += np.log(prob)
        
        return total_ll
    
    def log_likelihood_batch(self, data_batch: np.ndarray, latent_vars: np.ndarray,
                            params: Dict[str, np.ndarray]) -> np.ndarray:
        """
        ë°°ì¹˜ ë¡œê·¸ìš°ë„ ê³„ì‚° (GPU ìµœì í™”)
        
        ì—¬ëŸ¬ ê°œì¸/drawsë¥¼ í•œë²ˆì— ì²˜ë¦¬í•˜ì—¬ GPU íš¨ìœ¨ ê·¹ëŒ€í™”
        
        Args:
            data_batch: (n_batch, n_indicators) ê´€ì¸¡ ë°ì´í„°
            latent_vars: (n_batch,) ì ì¬ë³€ìˆ˜ ê°’ë“¤
            params: íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬
        
        Returns:
            (n_batch,) ë¡œê·¸ìš°ë„ ë°°ì—´
        """
        if not self.use_gpu:
            # CPU ëª¨ë“œ: ìˆœì°¨ ì²˜ë¦¬
            lls = []
            for i in range(len(latent_vars)):
                data_dict = {ind: data_batch[i, j] 
                           for j, ind in enumerate(self.config.indicators)}
                data_df = pd.DataFrame([data_dict])
                ll = self.log_likelihood(data_df, latent_vars[i], params)
                lls.append(ll)
            return np.array(lls)
        
        # GPU ëª¨ë“œ: ë°°ì¹˜ ì²˜ë¦¬
        zeta = cp.asarray(params['zeta'])  # (n_indicators,)
        tau = cp.asarray(params['tau'])    # (n_indicators, n_thresholds)
        data_gpu = cp.asarray(data_batch)  # (n_batch, n_indicators)
        lv_gpu = cp.asarray(latent_vars)   # (n_batch,)
        
        n_batch = len(latent_vars)
        ll_batch = cp.zeros(n_batch)
        
        # ê° ì§€í‘œì— ëŒ€í•´
        for i in range(self.n_indicators):
            y_obs = data_gpu[:, i].astype(int)  # (n_batch,)
            
            # ì„ í˜• ì˜ˆì¸¡: V = Î¶ * LV
            linear_pred = zeta[i] * lv_gpu  # (n_batch,)
            
            # ì„ê³„ê°’
            tau_i = tau[i]  # (n_thresholds,)
            
            # ê° ë²”ì£¼ì— ëŒ€í•´ í™•ë¥  ê³„ì‚°
            probs = cp.zeros(n_batch)
            
            for k in range(1, self.n_categories + 1):
                mask = (y_obs == k)
                if not cp.any(mask):
                    continue
                
                if k == 1:
                    upper = tau_i[0] - linear_pred[mask]
                    probs[mask] = ndtr(upper)
                elif k == self.n_categories:
                    lower = tau_i[-1] - linear_pred[mask]
                    probs[mask] = 1.0 - ndtr(lower)
                else:
                    upper = tau_i[k - 1] - linear_pred[mask]
                    lower = tau_i[k - 2] - linear_pred[mask]
                    probs[mask] = ndtr(upper) - ndtr(lower)
            
            # ë¡œê·¸ìš°ë„ ëˆ„ì 
            probs = cp.maximum(probs, 1e-10)
            ll_batch += cp.log(probs)
        
        # CPUë¡œ ë°˜í™˜
        return cp.asnumpy(ll_batch)
    
    def initialize_parameters(self) -> Dict[str, np.ndarray]:
        """íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”"""
        zeta = np.ones(self.n_indicators)
        
        tau = np.zeros((self.n_indicators, self.n_thresholds))
        for i in range(self.n_indicators):
            tau[i] = np.linspace(-2, 2, self.n_thresholds)
        
        return {'zeta': zeta, 'tau': tau}
    
    def get_n_parameters(self) -> int:
        """ì´ íŒŒë¼ë¯¸í„° ìˆ˜"""
        return self.n_indicators + (self.n_indicators * self.n_thresholds)


class GPUMultiLatentMeasurement:
    """
    ë‹¤ì¤‘ ì ì¬ë³€ìˆ˜ ì¸¡ì •ëª¨ë¸ (GPU ê°€ì†)

    ì—¬ëŸ¬ ì ì¬ë³€ìˆ˜ì˜ ì¸¡ì •ëª¨ë¸ì„ GPUì—ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """

    def __init__(self, measurement_configs: Dict, use_gpu: bool = True):
        """
        ì´ˆê¸°í™”

        Args:
            measurement_configs: {lv_name: MeasurementConfig} ë”•ì…”ë„ˆë¦¬
            use_gpu: GPU ì‚¬ìš© ì—¬ë¶€
        """
        self.configs = measurement_configs
        self.use_gpu = use_gpu and GPU_AVAILABLE

        # ê° ì ì¬ë³€ìˆ˜ë³„ ì¸¡ì •ëª¨ë¸ ìƒì„±
        self.models = {}
        for lv_name, config in measurement_configs.items():
            # âœ… measurement_methodì— ë”°ë¼ ì ì ˆí•œ ëª¨ë¸ ì„ íƒ
            method = getattr(config, 'measurement_method', 'ordered_probit')

            if method == 'continuous_linear':
                self.models[lv_name] = GPUContinuousLinearMeasurement(config, use_gpu)
            elif method == 'ordered_probit':
                self.models[lv_name] = GPUOrderedProbitMeasurement(config, use_gpu)
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¸¡ì • ë°©ë²•: {method}")

        if self.use_gpu:
            logger.info(f"ğŸš€ GPU ë‹¤ì¤‘ ì¸¡ì •ëª¨ë¸: {len(self.models)}ê°œ ì ì¬ë³€ìˆ˜")
        else:
            logger.info(f"ğŸ’» CPU ë‹¤ì¤‘ ì¸¡ì •ëª¨ë¸: {len(self.models)}ê°œ ì ì¬ë³€ìˆ˜")

    def log_likelihood(self, data: pd.DataFrame, latent_vars: Dict[str, float],
                      params: Dict[str, Dict]) -> float:
        """
        ì „ì²´ ë¡œê·¸ìš°ë„ ê³„ì‚°

        Args:
            data: ê´€ì¸¡ ë°ì´í„°
            latent_vars: {lv_name: lv_value} ì ì¬ë³€ìˆ˜ ê°’ë“¤
            params: {lv_name: {'zeta': ..., 'tau': ...}} íŒŒë¼ë¯¸í„°

        Returns:
            ì „ì²´ ë¡œê·¸ìš°ë„
        """
        total_ll = 0.0

        for lv_name, model in self.models.items():
            if lv_name not in latent_vars or lv_name not in params:
                continue

            ll = model.log_likelihood(data, latent_vars[lv_name], params[lv_name])
            total_ll += ll

        return total_ll

    def log_likelihood_batch(self, data_batch: Dict[str, np.ndarray],
                            latent_vars_batch: Dict[str, np.ndarray],
                            params: Dict[str, Dict]) -> np.ndarray:
        """
        ë°°ì¹˜ ë¡œê·¸ìš°ë„ ê³„ì‚° (GPU ìµœì í™”)

        ëª¨ë“  ê°œì¸ Ã— drawsë¥¼ í•œ ë²ˆì— ì²˜ë¦¬í•˜ì—¬ GPU íš¨ìœ¨ ê·¹ëŒ€í™”

        Args:
            data_batch: {lv_name: (n_batch, n_indicators)} ê´€ì¸¡ ë°ì´í„°
            latent_vars_batch: {lv_name: (n_batch,)} ì ì¬ë³€ìˆ˜ ê°’ë“¤
            params: {lv_name: {'zeta': ..., 'tau': ...}} íŒŒë¼ë¯¸í„°

        Returns:
            (n_batch,) ë¡œê·¸ìš°ë„ ë°°ì—´
        """
        # ì²« ë²ˆì§¸ LVë¡œ ë°°ì¹˜ í¬ê¸° í™•ì¸
        first_lv = list(latent_vars_batch.keys())[0]
        n_batch = len(latent_vars_batch[first_lv])

        if self.use_gpu:
            total_ll = cp.zeros(n_batch)
        else:
            total_ll = np.zeros(n_batch)

        # ê° ì ì¬ë³€ìˆ˜ë³„ ì¸¡ì •ëª¨ë¸ ìš°ë„ ê³„ì‚°
        for lv_idx, (lv_name, model) in enumerate(self.models.items()):
            if lv_name not in latent_vars_batch or lv_name not in params:
                continue

            if lv_name not in data_batch:
                continue

            # ì²« ë²ˆì§¸ LVì— ëŒ€í•´ì„œë§Œ íŒŒë¼ë¯¸í„° ë¡œê¹… (ë””ë²„ê¹…ìš©)
            # if lv_idx == 0:
            #     print(f"  [GPU ì¸¡ì •ëª¨ë¸ ë‚´ë¶€] {lv_name} zeta (ì²˜ìŒ 3ê°œ): {params[lv_name]['zeta'][:3]}")
            #     print(f"  [GPU ì¸¡ì •ëª¨ë¸ ë‚´ë¶€] {lv_name} tau[0] (ì²˜ìŒ 3ê°œ): {params[lv_name]['tau'][0][:3]}")

            # ë°°ì¹˜ ìš°ë„ ê³„ì‚°
            ll_batch = model.log_likelihood_batch(
                data_batch[lv_name],
                latent_vars_batch[lv_name],
                params[lv_name]
            )

            # GPU ëª¨ë“œì¼ ë•Œ NumPy ë°°ì—´ì„ CuPyë¡œ ë³€í™˜
            if self.use_gpu and isinstance(ll_batch, np.ndarray):
                ll_batch = cp.asarray(ll_batch)

            total_ll += ll_batch

        # GPUì—ì„œ CPUë¡œ ë³€í™˜
        if self.use_gpu:
            total_ll = cp.asnumpy(total_ll)

        return total_ll

    def initialize_parameters(self) -> Dict[str, Dict]:
        """ëª¨ë“  ì¸¡ì •ëª¨ë¸ íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”"""
        params = {}
        for lv_name, model in self.models.items():
            params[lv_name] = model.initialize_parameters()
        return params

    def log_likelihood_batch_draws(self, ind_data: pd.DataFrame,
                                    lvs_list: list,
                                    params: Dict[str, Dict]) -> list:
        """
        ê°œì¸ì˜ ì—¬ëŸ¬ drawsì— ëŒ€í•œ ì¸¡ì •ëª¨ë¸ ìš°ë„ ê³„ì‚° (GPU ë°°ì¹˜)

        Args:
            ind_data: ê°œì¸ ë°ì´í„° (1í–‰)
            lvs_list: ê° drawì˜ ì ì¬ë³€ìˆ˜ ê°’ ë¦¬ìŠ¤íŠ¸ [{lv_name: value}, ...]
            params: {lv_name: {'zeta': ..., 'tau': ...}} íŒŒë¼ë¯¸í„°

        Returns:
            ê° drawì˜ ë¡œê·¸ìš°ë„ ë¦¬ìŠ¤íŠ¸
        """
        n_draws = len(lvs_list)

        # ë°°ì¹˜ ë°ì´í„° êµ¬ì„±
        data_batch = {}
        latent_vars_batch = {}

        for lv_name, model in self.models.items():
            # ì§€í‘œ ë°ì´í„° (ëª¨ë“  drawsì— ë™ì¼)
            indicators = model.config.indicators
            ind_values = ind_data[indicators].iloc[0].values
            data_batch[lv_name] = np.tile(ind_values, (n_draws, 1))  # (n_draws, n_indicators)

            # ì ì¬ë³€ìˆ˜ ê°’ (ê° drawë§ˆë‹¤ ë‹¤ë¦„)
            lv_values = np.array([lvs[lv_name] for lvs in lvs_list])
            latent_vars_batch[lv_name] = lv_values  # (n_draws,)

        # ë°°ì¹˜ ìš°ë„ ê³„ì‚°
        ll_batch = self.log_likelihood_batch(data_batch, latent_vars_batch, params)

        return ll_batch.tolist()

    def get_n_parameters(self) -> int:
        """ì´ íŒŒë¼ë¯¸í„° ìˆ˜"""
        total = 0
        for model in self.models.values():
            n_indicators = model.n_indicators
            n_thresholds = model.n_thresholds
            total += n_indicators + (n_indicators * n_thresholds)
        return total


class GPUContinuousLinearMeasurement:
    """
    GPU ê°€ì† ì—°ì†í˜• ì„ í˜• ì¸¡ì •ëª¨ë¸

    CuPyë¥¼ ì‚¬ìš©í•˜ì—¬ GPUì—ì„œ ë¡œê·¸ìš°ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

    Model:
        Y_i = Î¶_i * LV + Îµ_i
        Îµ_i ~ N(0, ÏƒÂ²_i)
    """

    def __init__(self, config, use_gpu: bool = True):
        """
        ì´ˆê¸°í™”

        Args:
            config: MeasurementConfig ê°ì²´
            use_gpu: GPU ì‚¬ìš© ì—¬ë¶€
        """
        self.config = config
        self.n_indicators = len(config.indicators)
        self.use_gpu = use_gpu and GPU_AVAILABLE
        self.measurement_method = 'continuous_linear'  # âœ… ì¸¡ì • ë°©ë²• ëª…ì‹œ

        self.zeta = None
        self.sigma_sq = None
        self.fitted = False

        if self.use_gpu:
            self.xp = cp
            logger.info(f"ğŸš€ GPU ContinuousLinear: {self.n_indicators}ê°œ ì§€í‘œ")
        else:
            self.xp = np
            logger.info(f"ğŸ’» CPU ContinuousLinear: {self.n_indicators}ê°œ ì§€í‘œ")

    def log_likelihood(self, data: pd.DataFrame, latent_var: float,
                      params: Dict[str, np.ndarray]) -> float:
        """
        ë¡œê·¸ìš°ë„ ê³„ì‚° (GPU ê°€ì†)

        Args:
            data: ê´€ì¸¡ì§€í‘œ ë°ì´í„°
            latent_var: ì ì¬ë³€ìˆ˜ ê°’
            params: {'zeta': ..., 'sigma_sq': ...}

        Returns:
            ë¡œê·¸ìš°ë„ ê°’
        """
        zeta = params['zeta']
        sigma_sq = params['sigma_sq']

        if self.use_gpu:
            # GPU ê³„ì‚°
            zeta_gpu = cp.asarray(zeta)
            sigma_sq_gpu = cp.asarray(sigma_sq)
            latent_var_gpu = cp.asarray(latent_var)
        else:
            zeta_gpu = zeta
            sigma_sq_gpu = sigma_sq
            latent_var_gpu = latent_var

        total_ll = 0.0
        first_row = data.iloc[0]

        for i, indicator in enumerate(self.config.indicators):
            if indicator not in first_row.index:
                continue

            y_obs = first_row[indicator]

            if pd.isna(y_obs):
                continue

            # ì˜ˆì¸¡ê°’
            if self.use_gpu:
                y_pred = float(zeta_gpu[i] * latent_var_gpu)
            else:
                y_pred = zeta_gpu[i] * latent_var_gpu

            # ì”ì°¨
            residual = y_obs - y_pred

            # ì •ê·œë¶„í¬ ë¡œê·¸ìš°ë„
            if self.use_gpu:
                ll_i = -0.5 * cp.log(2 * cp.pi * sigma_sq_gpu[i])
                ll_i += -0.5 * (residual ** 2) / sigma_sq_gpu[i]
                ll_i = float(cp.asnumpy(ll_i))
            else:
                ll_i = -0.5 * np.log(2 * np.pi * sigma_sq_gpu[i])
                ll_i += -0.5 * (residual ** 2) / sigma_sq_gpu[i]

            total_ll += ll_i

        return total_ll

    def initialize_parameters(self) -> Dict[str, np.ndarray]:
        """íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”"""
        params = {}

        # ìš”ì¸ì ì¬ëŸ‰
        zeta = np.ones(self.n_indicators)
        if self.config.fix_first_loading:
            zeta[0] = 1.0

        params['zeta'] = zeta

        # ì˜¤ì°¨ë¶„ì‚°
        sigma_sq = np.ones(self.n_indicators) * self.config.initial_error_variance
        params['sigma_sq'] = sigma_sq

        return params

    def log_likelihood_batch(self, data_batch: np.ndarray, latent_vars: np.ndarray,
                            params: Dict[str, np.ndarray]) -> np.ndarray:
        """
        ë°°ì¹˜ ë¡œê·¸ìš°ë„ ê³„ì‚° (GPU ìµœì í™”)

        ì—¬ëŸ¬ ê°œì¸/drawsë¥¼ í•œë²ˆì— ì²˜ë¦¬í•˜ì—¬ GPU íš¨ìœ¨ ê·¹ëŒ€í™”

        Args:
            data_batch: (n_batch, n_indicators) ê´€ì¸¡ ë°ì´í„°
            latent_vars: (n_batch,) ì ì¬ë³€ìˆ˜ ê°’ë“¤
            params: {'zeta': ..., 'sigma_sq': ...}

        Returns:
            (n_batch,) ë¡œê·¸ìš°ë„ ë°°ì—´
        """
        if not self.use_gpu:
            # CPU ëª¨ë“œ: ìˆœì°¨ ì²˜ë¦¬
            lls = []
            for i in range(len(latent_vars)):
                data_dict = {ind: data_batch[i, j]
                           for j, ind in enumerate(self.config.indicators)}
                data_df = pd.DataFrame([data_dict])
                ll = self.log_likelihood(data_df, latent_vars[i], params)
                lls.append(ll)
            return np.array(lls)

        # GPU ëª¨ë“œ: ë°°ì¹˜ ì²˜ë¦¬
        zeta = cp.asarray(params['zeta'])      # (n_indicators,)
        sigma_sq = cp.asarray(params['sigma_sq'])  # (n_indicators,)
        data_gpu = cp.asarray(data_batch)      # (n_batch, n_indicators)
        lv_gpu = cp.asarray(latent_vars)       # (n_batch,)

        n_batch = len(latent_vars)
        ll_batch = cp.zeros(n_batch)

        # ê° ì§€í‘œì— ëŒ€í•´
        for i in range(self.n_indicators):
            y_obs = data_gpu[:, i]  # (n_batch,)

            # ì˜ˆì¸¡ê°’: Y_pred = Î¶ * LV
            y_pred = zeta[i] * lv_gpu  # (n_batch,)

            # ì”ì°¨
            residual = y_obs - y_pred  # (n_batch,)

            # ì •ê·œë¶„í¬ ë¡œê·¸ìš°ë„
            # log p(y|LV) = -0.5 * log(2Ï€ * ÏƒÂ²) - 0.5 * (y - Î¶*LV)Â² / ÏƒÂ²
            ll_i = -0.5 * cp.log(2 * cp.pi * sigma_sq[i])  # ìŠ¤ì¹¼ë¼
            ll_i = ll_i - 0.5 * (residual ** 2) / sigma_sq[i]  # (n_batch,)

            ll_batch = ll_batch + ll_i  # (n_batch,)

        # GPUì—ì„œ CPUë¡œ ë³€í™˜
        return cp.asnumpy(ll_batch)

    def get_n_parameters(self) -> int:
        """íŒŒë¼ë¯¸í„° ìˆ˜ ë°˜í™˜"""
        n_params = 0

        if self.config.fix_first_loading:
            n_params += self.n_indicators - 1
        else:
            n_params += self.n_indicators

        if not self.config.fix_error_variance:
            n_params += self.n_indicators

        return n_params

