"""
Simultaneous Estimation for ICLV Models

ICLV ëª¨ë¸ì˜ ë™ì‹œ ì¶”ì • ì—”ì§„ì…ë‹ˆë‹¤.
Apollo íŒ¨í‚¤ì§€ì˜ ë™ì‹œ ì¶”ì • ë°©ë²•ë¡ ì„ Pythonìœ¼ë¡œ êµ¬í˜„í•©ë‹ˆë‹¤.

ì°¸ì¡°:
- King (2022) - Apollo íŒ¨í‚¤ì§€ ì‚¬ìš©
- Train (2009) - Discrete Choice Methods with Simulation
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Tuple, Callable
from scipy import optimize
from scipy.stats import norm, qmc
from scipy.special import logsumexp
import logging

logger = logging.getLogger(__name__)


class HaltonDrawGenerator:
    """
    Halton ì‹œí€€ìŠ¤ ìƒì„±ê¸°
    
    ì¤€ë‚œìˆ˜(Quasi-random) ì‹œí€€ìŠ¤ë¥¼ ìƒì„±í•˜ì—¬ ì‹œë®¬ë ˆì´ì…˜ ì •í™•ë„ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
    ì¼ë°˜ ë‚œìˆ˜ë³´ë‹¤ ê³µê°„ì„ ë” ê· ë“±í•˜ê²Œ ì»¤ë²„í•©ë‹ˆë‹¤.
    
    ì°¸ì¡°: Apollo íŒ¨í‚¤ì§€ì˜ Halton draws
    """
    
    def __init__(self, n_draws: int, n_individuals: int, 
                 scramble: bool = True, seed: Optional[int] = None):
        """
        Args:
            n_draws: ê°œì¸ë‹¹ draw ìˆ˜
            n_individuals: ê°œì¸ ìˆ˜
            scramble: ìŠ¤í¬ë¨ë¸” ì—¬ë¶€ (ê¶Œì¥)
            seed: ë‚œìˆ˜ ì‹œë“œ
        """
        self.n_draws = n_draws
        self.n_individuals = n_individuals
        self.scramble = scramble
        self.seed = seed
        
        self.draws = None
        self._generate_draws()
    
    def _generate_draws(self):
        """Halton ì‹œí€€ìŠ¤ ìƒì„±"""
        logger.info(f"Halton draws ìƒì„±: {self.n_individuals} ê°œì¸ Ã— {self.n_draws} draws")
        
        # scipyì˜ Halton ì‹œí€€ìŠ¤ ìƒì„±ê¸° ì‚¬ìš©
        sampler = qmc.Halton(d=1, scramble=self.scramble, seed=self.seed)
        
        # ê· ë“±ë¶„í¬ [0,1] ìƒ˜í”Œ ìƒì„±
        uniform_draws = sampler.random(n=self.n_individuals * self.n_draws)
        
        # í‘œì¤€ì •ê·œë¶„í¬ë¡œ ë³€í™˜ (ì—­ëˆ„ì ë¶„í¬í•¨ìˆ˜)
        normal_draws = norm.ppf(uniform_draws)
        
        # (n_individuals, n_draws) í˜•íƒœë¡œ ì¬êµ¬ì„±
        self.draws = normal_draws.reshape(self.n_individuals, self.n_draws)
        
        logger.info(f"Halton draws ìƒì„± ì™„ë£Œ: shape={self.draws.shape}")
    
    def get_draws(self) -> np.ndarray:
        """ìƒì„±ëœ draws ë°˜í™˜"""
        return self.draws
    
    def get_draw_for_individual(self, individual_idx: int) -> np.ndarray:
        """íŠ¹ì • ê°œì¸ì˜ draws ë°˜í™˜"""
        return self.draws[individual_idx, :]


class SimultaneousEstimator:
    """
    ICLV ëª¨ë¸ ë™ì‹œ ì¶”ì •ê¸°
    
    ì¸¡ì •ëª¨ë¸, êµ¬ì¡°ëª¨ë¸, ì„ íƒëª¨ë¸ì„ ë™ì‹œì— ì¶”ì •í•©ë‹ˆë‹¤.
    
    ê²°í•© ìš°ë„í•¨ìˆ˜:
    L = âˆáµ¢ âˆ« P(Choice|LV) Ã— P(Indicators|LV) Ã— P(LV|X) dLV
    
    ì‹œë®¬ë ˆì´ì…˜ ê¸°ë°˜ ì¶”ì •:
    L â‰ˆ âˆáµ¢ (1/R) Î£áµ£ P(Choice|LVáµ£) Ã— P(Indicators|LVáµ£) Ã— P(LVáµ£|X)
    """
    
    def __init__(self, config):
        """
        Args:
            config: ICLVConfig ê°ì²´
        """
        self.config = config
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        
        self.halton_generator = None
        self.data = None
        self.results = None
    
    def estimate(self, data: pd.DataFrame, 
                measurement_model,
                structural_model,
                choice_model) -> Dict:
        """
        ICLV ëª¨ë¸ ë™ì‹œ ì¶”ì •
        
        Args:
            data: í†µí•© ë°ì´í„°
            measurement_model: ì¸¡ì •ëª¨ë¸ ê°ì²´
            structural_model: êµ¬ì¡°ëª¨ë¸ ê°ì²´
            choice_model: ì„ íƒëª¨ë¸ ê°ì²´
        
        Returns:
            ì¶”ì • ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        print("=" * 70, flush=True)
        print("SimultaneousEstimator.estimate() ì‹œì‘", flush=True)
        print("=" * 70, flush=True)
        self.logger.info("ICLV ëª¨ë¸ ë™ì‹œ ì¶”ì • ì‹œì‘")

        self.data = data
        print(f"ë°ì´í„° shape: {data.shape}", flush=True)
        n_individuals = data[self.config.individual_id_column].nunique()
        print(f"ê°œì¸ ìˆ˜: {n_individuals}", flush=True)
        self.logger.info(f"ê°œì¸ ìˆ˜: {n_individuals}")

        # Halton draws ìƒì„±
        print(f"Halton draws ìƒì„± ì‹œì‘... (n_draws={self.config.estimation.n_draws}, n_individuals={n_individuals})", flush=True)
        self.logger.info(f"Halton draws ìƒì„± ì¤‘... (n_draws={self.config.estimation.n_draws})")
        self.halton_generator = HaltonDrawGenerator(
            n_draws=self.config.estimation.n_draws,
            n_individuals=n_individuals,
            scramble=self.config.estimation.scramble_halton
        )
        print("Halton draws ìƒì„± ì™„ë£Œ", flush=True)
        self.logger.info("Halton draws ìƒì„± ì™„ë£Œ")

        # ì´ˆê¸° íŒŒë¼ë¯¸í„° ì„¤ì •
        print("ì´ˆê¸° íŒŒë¼ë¯¸í„° ì„¤ì • ì‹œì‘...", flush=True)
        self.logger.info("ì´ˆê¸° íŒŒë¼ë¯¸í„° ì„¤ì • ì¤‘...")
        initial_params = self._get_initial_parameters(
            measurement_model, structural_model, choice_model
        )
        print(f"ì´ˆê¸° íŒŒë¼ë¯¸í„° ì„¤ì • ì™„ë£Œ (ì´ {len(initial_params)}ê°œ)", flush=True)
        self.logger.info(f"ì´ˆê¸° íŒŒë¼ë¯¸í„° ì„¤ì • ì™„ë£Œ (ì´ {len(initial_params)}ê°œ)")
        
        # ê²°í•© ìš°ë„í•¨ìˆ˜ ì •ì˜ (gradient check ë¡œê¹… ì¶”ê°€)
        iteration_count = [0]  # Mutable counter
        best_ll = [-np.inf]  # Track best log-likelihood

        def negative_log_likelihood(params):
            iteration_count[0] += 1
            ll = self._joint_log_likelihood(
                params, measurement_model, structural_model, choice_model
            )

            # Track best value
            if ll > best_ll[0]:
                best_ll[0] = ll
                improvement = "âœ“ NEW BEST"
            else:
                improvement = ""

            # Log every iteration with more detail
            if iteration_count[0] % 5 == 0 or improvement:
                print(
                    f"Iter {iteration_count[0]:4d}: LL = {ll:12.4f} "
                    f"(Best: {best_ll[0]:12.4f}) {improvement}",
                    flush=True
                )

            return -ll

        # Get parameter bounds
        print("íŒŒë¼ë¯¸í„° bounds ê³„ì‚° ì‹œì‘...", flush=True)
        bounds = self._get_parameter_bounds(
            measurement_model, structural_model, choice_model
        )
        print(f"íŒŒë¼ë¯¸í„° bounds ê³„ì‚° ì™„ë£Œ (ì´ {len(bounds)}ê°œ)", flush=True)

        # ğŸ”´ ì„ì‹œ: Nelder-Meadë¡œ ë³€ê²½ (gradient-free)
        print("=" * 70, flush=True)
        print("ìµœì í™” ì‹œì‘: Nelder-Mead (gradient-free)", flush=True)
        print(f"ì´ˆê¸° íŒŒë¼ë¯¸í„° ê°œìˆ˜: {len(initial_params)}", flush=True)
        print(f"ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜: {self.config.estimation.max_iterations}", flush=True)
        print("=" * 70, flush=True)
        self.logger.info(f"ìµœì í™” ì‹œì‘: Nelder-Mead (gradient-free)")
        self.logger.info(f"ì´ˆê¸° íŒŒë¼ë¯¸í„° ê°œìˆ˜: {len(initial_params)}")
        self.logger.info(f"ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜: {self.config.estimation.max_iterations}")

        result = optimize.minimize(
            negative_log_likelihood,
            initial_params,
            method='Nelder-Mead',  # Gradient-free method
            options={
                'maxiter': self.config.estimation.max_iterations,
                'xatol': 1e-4,
                'fatol': 1e-4,
                'disp': True
            }
        )
        
        if result.success:
            self.logger.info("ìµœì í™” ì„±ê³µ")
        else:
            self.logger.warning(f"ìµœì í™” ì‹¤íŒ¨: {result.message}")
        
        # ê²°ê³¼ ì²˜ë¦¬
        self.results = self._process_results(
            result, measurement_model, structural_model, choice_model
        )
        
        return self.results
    
    def _joint_log_likelihood(self, params: np.ndarray,
                             measurement_model,
                             structural_model,
                             choice_model) -> float:
        """
        ê²°í•© ë¡œê·¸ìš°ë„ ê³„ì‚°

        ì‹œë®¬ë ˆì´ì…˜ ê¸°ë°˜:
        log L â‰ˆ Î£áµ¢ log[(1/R) Î£áµ£ P(Choice|LVáµ£) Ã— P(Indicators|LVáµ£) Ã— P(LVáµ£|X)]
        """
        # print("_joint_log_likelihood ì‹œì‘", flush=True)

        # íŒŒë¼ë¯¸í„° ë¶„í•´
        param_dict = self._unpack_parameters(
            params, measurement_model, structural_model, choice_model
        )

        total_ll = 0.0
        draws = self.halton_generator.get_draws()

        # ê°œì¸ë³„ ìš°ë„ ê³„ì‚°
        individual_ids = self.data[self.config.individual_id_column].unique()
        # print(f"ê°œì¸ ìˆ˜: {len(individual_ids)}", flush=True)

        for i, ind_id in enumerate(individual_ids):
            ind_data = self.data[self.data[self.config.individual_id_column] == ind_id]
            ind_draws = draws[i, :]  # ì´ ê°œì¸ì˜ draws

            # ğŸ”´ ìˆ˜ì •: logsumexpë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆ˜ì¹˜ ì•ˆì •ì„± í™•ë³´
            # King (2022) Apollo ë°©ì‹
            draw_lls = []

            for j, draw in enumerate(ind_draws):
                # êµ¬ì¡°ëª¨ë¸: LV = Î³*X + Î·
                lv = structural_model.predict(ind_data, param_dict['structural'], draw)

                # ì¸¡ì •ëª¨ë¸ ìš°ë„: P(Indicators|LV)
                ll_measurement = measurement_model.log_likelihood(
                    ind_data, lv, param_dict['measurement']
                )

                # Panel Product: ê°œì¸ì˜ ì—¬ëŸ¬ ì„ íƒ ìƒí™©ì— ëŒ€í•œ í™•ë¥ ì„ ê³±í•¨
                choice_set_lls = []
                for idx in range(len(ind_data)):
                    ll_choice_t = choice_model.log_likelihood(
                        ind_data.iloc[idx:idx+1],  # ê° ì„ íƒ ìƒí™©
                        lv,
                        param_dict['choice']
                    )
                    choice_set_lls.append(ll_choice_t)

                # Panel product: log(P1 * P2 * ... * PT) = log(P1) + log(P2) + ... + log(PT)
                ll_choice = sum(choice_set_lls)

                # êµ¬ì¡°ëª¨ë¸ ìš°ë„: P(LV|X) - ì •ê·œë¶„í¬ ê°€ì •
                ll_structural = structural_model.log_likelihood(
                    ind_data, lv, param_dict['structural'], draw
                )

                # ê²°í•© ë¡œê·¸ìš°ë„
                draw_ll = ll_measurement + ll_choice + ll_structural

                # ìˆ˜ì¹˜ ì•ˆì •ì„±: ìœ í•œí•œ ê°’ë§Œ ì‚¬ìš©
                if np.isfinite(draw_ll):
                    draw_lls.append(draw_ll)

            # ğŸ”´ ìˆ˜ì •: logsumexpë¥¼ ì‚¬ìš©í•˜ì—¬ í‰ê·  ê³„ì‚°
            # log[(1/R) Î£áµ£ exp(ll_r)] = logsumexp(ll_r) - log(R)
            if len(draw_lls) == 0:
                person_ll = -1e10  # ìœ íš¨í•œ drawsê°€ ì—†ìœ¼ë©´ ë§¤ìš° ì‘ì€ ê°’
            else:
                person_ll = logsumexp(draw_lls) - np.log(len(draw_lls))

            total_ll += person_ll

        return total_ll

    def _get_parameter_bounds(self, measurement_model,
                              structural_model, choice_model) -> list:
        """
        Parameter bounds for L-BFGS-B

        Returns:
            bounds: [(lower, upper), ...] list
        """
        bounds = []

        # Measurement model parameters
        # - Factor loadings (zeta): [0.1, 10]
        n_indicators = len(self.config.measurement.indicators)
        bounds.extend([(0.1, 10.0)] * n_indicators)

        # - Thresholds (tau): [-10, 10]
        n_thresholds = self.config.measurement.n_categories - 1
        for _ in range(n_indicators):
            bounds.extend([(-10.0, 10.0)] * n_thresholds)

        # Structural model parameters (gamma): unbounded
        n_sociodem = len(self.config.structural.sociodemographics)
        bounds.extend([(None, None)] * n_sociodem)

        # Choice model parameters
        # - Intercept: unbounded
        bounds.append((None, None))

        # - Attribute coefficients (beta): unbounded
        n_attributes = len(self.config.choice.choice_attributes)
        bounds.extend([(None, None)] * n_attributes)

        # - Latent variable coefficient (lambda): unbounded
        bounds.append((None, None))

        # - Sociodemographic coefficients: unbounded
        if self.config.structural.include_in_choice:
            bounds.extend([(None, None)] * n_sociodem)

        return bounds

    def _get_initial_parameters(self, measurement_model,
                                structural_model, choice_model) -> np.ndarray:
        """ì´ˆê¸° íŒŒë¼ë¯¸í„° ì„¤ì •"""
        
        params = []
        
        # ì¸¡ì •ëª¨ë¸ íŒŒë¼ë¯¸í„°
        # - ìš”ì¸ì ì¬ëŸ‰ (zeta)
        n_indicators = len(self.config.measurement.indicators)
        params.extend([1.0] * n_indicators)  # zeta
        
        # - ì„ê³„ê°’ (tau)
        n_thresholds = self.config.measurement.n_categories - 1
        for _ in range(n_indicators):
            params.extend([-2, -1, 1, 2])  # 5ì  ì²™ë„ ê¸°ë³¸ê°’
        
        # êµ¬ì¡°ëª¨ë¸ íŒŒë¼ë¯¸í„° (gamma)
        n_sociodem = len(self.config.structural.sociodemographics)
        params.extend([0.0] * n_sociodem)
        
        # ì„ íƒëª¨ë¸ íŒŒë¼ë¯¸í„°
        # - ì ˆí¸
        params.append(0.0)
        
        # - ì†ì„± ê³„ìˆ˜ (beta)
        n_attributes = len(self.config.choice.choice_attributes)
        params.extend([0.0] * n_attributes)
        
        # - ì ì¬ë³€ìˆ˜ ê³„ìˆ˜ (lambda)
        params.append(1.0)
        
        # - ì‚¬íšŒì¸êµ¬í•™ì  ë³€ìˆ˜ ê³„ìˆ˜ (ì„ íƒëª¨ë¸ì— í¬í•¨ë˜ëŠ” ê²½ìš°)
        if self.config.structural.include_in_choice:
            params.extend([0.0] * n_sociodem)
        
        return np.array(params)
    

    
    def _get_parameter_bounds(self, measurement_model,
                              structural_model, choice_model) -> list:
        """
        Parameter bounds for L-BFGS-B
        
        Returns:
            bounds: [(lower, upper), ...] list
        """
        bounds = []
        
        # Measurement model parameters
        # - Factor loadings (zeta): [0.1, 10]
        n_indicators = len(self.config.measurement.indicators)
        bounds.extend([(0.1, 10.0)] * n_indicators)
        
        # - Thresholds (tau): [-10, 10]
        n_thresholds = self.config.measurement.n_categories - 1
        for _ in range(n_indicators):
            bounds.extend([(-10.0, 10.0)] * n_thresholds)
        
        # Structural model parameters (gamma): unbounded
        n_sociodem = len(self.config.structural.sociodemographics)
        bounds.extend([(None, None)] * n_sociodem)
        
        # Choice model parameters
        # - Intercept: unbounded
        bounds.append((None, None))
        
        # - Attribute coefficients (beta): unbounded
        n_attributes = len(self.config.choice.choice_attributes)
        bounds.extend([(None, None)] * n_attributes)
        
        # - Latent variable coefficient (lambda): unbounded
        bounds.append((None, None))
        
        # - Sociodemographic coefficients: unbounded
        if self.config.structural.include_in_choice:
            bounds.extend([(None, None)] * n_sociodem)
        
        return bounds
    def _unpack_parameters(self, params: np.ndarray,
                          measurement_model,
                          structural_model,
                          choice_model) -> Dict[str, Dict]:
        """íŒŒë¼ë¯¸í„° ë²¡í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        
        idx = 0
        param_dict = {
            'measurement': {},
            'structural': {},
            'choice': {}
        }
        
        # ì¸¡ì •ëª¨ë¸ íŒŒë¼ë¯¸í„°
        n_indicators = len(self.config.measurement.indicators)
        param_dict['measurement']['zeta'] = params[idx:idx+n_indicators]
        idx += n_indicators

        n_thresholds = self.config.measurement.n_categories - 1
        # tauë¥¼ 2D ë°°ì—´ë¡œ ì €ì¥ (n_indicators, n_thresholds)
        tau_list = []
        for i in range(n_indicators):
            tau_list.append(params[idx:idx+n_thresholds])
            idx += n_thresholds
        param_dict['measurement']['tau'] = np.array(tau_list)
        
        # êµ¬ì¡°ëª¨ë¸ íŒŒë¼ë¯¸í„°
        n_sociodem = len(self.config.structural.sociodemographics)
        param_dict['structural']['gamma'] = params[idx:idx+n_sociodem]
        idx += n_sociodem
        
        # ì„ íƒëª¨ë¸ íŒŒë¼ë¯¸í„°
        param_dict['choice']['intercept'] = params[idx]
        idx += 1
        
        n_attributes = len(self.config.choice.choice_attributes)
        param_dict['choice']['beta'] = params[idx:idx+n_attributes]
        idx += n_attributes
        
        param_dict['choice']['lambda'] = params[idx]
        idx += 1
        
        if self.config.structural.include_in_choice:
            param_dict['choice']['beta_sociodem'] = params[idx:idx+n_sociodem]
            idx += n_sociodem
        
        return param_dict
    
    def _process_results(self, optimization_result,
                        measurement_model,
                        structural_model,
                        choice_model) -> Dict:
        """ìµœì í™” ê²°ê³¼ ì²˜ë¦¬"""
        
        param_dict = self._unpack_parameters(
            optimization_result.x,
            measurement_model,
            structural_model,
            choice_model
        )
        
        results = {
            'success': optimization_result.success,
            'message': optimization_result.message,
            'log_likelihood': -optimization_result.fun,
            'n_iterations': optimization_result.nit,
            'parameters': param_dict,
            'raw_params': optimization_result.x,
            
            # ëª¨ë¸ ì í•©ë„
            'n_observations': len(self.data),
            'n_parameters': len(optimization_result.x),
        }
        
        # AIC, BIC ê³„ì‚°
        ll = results['log_likelihood']
        k = results['n_parameters']
        n = results['n_observations']
        
        results['aic'] = -2 * ll + 2 * k
        results['bic'] = -2 * ll + k * np.log(n)
        
        # í‘œì¤€ì˜¤ì°¨ ê³„ì‚° (Hessian ê¸°ë°˜)
        if self.config.estimation.calculate_se:
            try:
                hessian = optimization_result.hess_inv
                if hasattr(hessian, 'todense'):
                    hessian = hessian.todense()
                
                se = np.sqrt(np.diag(hessian))
                results['standard_errors'] = se
                
                # t-í†µê³„ëŸ‰
                results['t_statistics'] = optimization_result.x / se
                
                # p-ê°’
                from scipy.stats import t
                results['p_values'] = 2 * (1 - t.cdf(np.abs(results['t_statistics']), n - k))
                
            except Exception as e:
                self.logger.warning(f"í‘œì¤€ì˜¤ì°¨ ê³„ì‚° ì‹¤íŒ¨: {e}")
        
        return results


def estimate_iclv_simultaneous(data: pd.DataFrame, config,
                               measurement_model,
                               structural_model,
                               choice_model) -> Dict:
    """
    ICLV ëª¨ë¸ ë™ì‹œ ì¶”ì • í—¬í¼ í•¨ìˆ˜
    
    Args:
        data: í†µí•© ë°ì´í„°
        config: ICLVConfig
        measurement_model: ì¸¡ì •ëª¨ë¸
        structural_model: êµ¬ì¡°ëª¨ë¸
        choice_model: ì„ íƒëª¨ë¸
    
    Returns:
        ì¶”ì • ê²°ê³¼
    """
    estimator = SimultaneousEstimator(config)
    return estimator.estimate(data, measurement_model, structural_model, choice_model)

