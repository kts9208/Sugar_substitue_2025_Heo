"""
Simultaneous Estimation for ICLV Models

ICLV ëª¨ë¸ì˜ ë™ì‹œ ì¶”ì • ì—”ì§„ì…ë‹ˆë‹¤.
Apollo íŒ¨í‚¤ì§€ì˜ ë™ì‹œ ì¶”ì • ë°©ë²•ë¡ ì„ Pythonìœ¼ë¡œ êµ¬í˜„í•©ë‹ˆë‹¤.

ì°¸ì¡°:
- King (2022) - Apollo íŒ¨í‚¤ì§€ ì‚¬ìš©
- Train (2009) - Discrete Choice Methods with Simulation
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Tuple, Callable
from scipy import optimize
from scipy.stats import norm, qmc
from scipy.special import logsumexp
import logging

from .gradient_calculator import (
    MeasurementGradient,
    StructuralGradient,
    ChoiceGradient,
    JointGradient
)

logger = logging.getLogger(__name__)


class HaltonDrawGenerator:
    """
    Halton ì‹œí€€ìŠ¤ ìƒì„±ê¸°
    
    ì¤€ë‚œìˆ˜(Quasi-random) ì‹œí€€ìŠ¤ë¥¼ ìƒì„±í•˜ì—¬ ì‹œë®¬ë ˆì´ì…˜ ì •í™•ë„ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
    ì¼ë°˜ ë‚œìˆ˜ë³´ë‹¤ ê³µê°„ì„ ë” ê· ë“±í•˜ê²Œ ì»¤ë²„í•©ë‹ˆë‹¤.
    
    ì°¸ì¡°: Apollo íŒ¨í‚¤ì§€ì˜ Halton draws
    """
    
    def __init__(self, n_draws: int, n_individuals: int, 
                 scramble: bool = True, seed: Optional[int] = None):
        """
        Args:
            n_draws: ê°œì¸ë‹¹ draw ìˆ˜
            n_individuals: ê°œì¸ ìˆ˜
            scramble: ìŠ¤í¬ë¨ë¸” ì—¬ë¶€ (ê¶Œì¥)
            seed: ë‚œìˆ˜ ì‹œë“œ
        """
        self.n_draws = n_draws
        self.n_individuals = n_individuals
        self.scramble = scramble
        self.seed = seed
        
        self.draws = None
        self._generate_draws()
    
    def _generate_draws(self):
        """Halton ì‹œí€€ìŠ¤ ìƒì„±"""
        logger.info(f"Halton draws ìƒì„±: {self.n_individuals} ê°œì¸ Ã— {self.n_draws} draws")
        
        # scipyì˜ Halton ì‹œí€€ìŠ¤ ìƒì„±ê¸° ì‚¬ìš©
        sampler = qmc.Halton(d=1, scramble=self.scramble, seed=self.seed)
        
        # ê· ë“±ë¶„í¬ [0,1] ìƒ˜í”Œ ìƒì„±
        uniform_draws = sampler.random(n=self.n_individuals * self.n_draws)
        
        # í‘œì¤€ì •ê·œë¶„í¬ë¡œ ë³€í™˜ (ì—­ëˆ„ì ë¶„í¬í•¨ìˆ˜)
        normal_draws = norm.ppf(uniform_draws)
        
        # (n_individuals, n_draws) í˜•íƒœë¡œ ì¬êµ¬ì„±
        self.draws = normal_draws.reshape(self.n_individuals, self.n_draws)
        
        logger.info(f"Halton draws ìƒì„± ì™„ë£Œ: shape={self.draws.shape}")
    
    def get_draws(self) -> np.ndarray:
        """ìƒì„±ëœ draws ë°˜í™˜"""
        return self.draws
    
    def get_draw_for_individual(self, individual_idx: int) -> np.ndarray:
        """íŠ¹ì • ê°œì¸ì˜ draws ë°˜í™˜"""
        return self.draws[individual_idx, :]


class SimultaneousEstimator:
    """
    ICLV ëª¨ë¸ ë™ì‹œ ì¶”ì •ê¸°
    
    ì¸¡ì •ëª¨ë¸, êµ¬ì¡°ëª¨ë¸, ì„ íƒëª¨ë¸ì„ ë™ì‹œì— ì¶”ì •í•©ë‹ˆë‹¤.
    
    ê²°í•© ìš°ë„í•¨ìˆ˜:
    L = âˆáµ¢ âˆ« P(Choice|LV) Ã— P(Indicators|LV) Ã— P(LV|X) dLV
    
    ì‹œë®¬ë ˆì´ì…˜ ê¸°ë°˜ ì¶”ì •:
    L â‰ˆ âˆáµ¢ (1/R) Î£áµ£ P(Choice|LVáµ£) Ã— P(Indicators|LVáµ£) Ã— P(LVáµ£|X)
    """
    
    def __init__(self, config):
        """
        Args:
            config: ICLVConfig ê°ì²´
        """
        self.config = config
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")

        self.halton_generator = None
        self.data = None
        self.results = None

        # Gradient calculators (Apollo ë°©ì‹)
        self.measurement_grad = None
        self.structural_grad = None
        self.choice_grad = None
        self.joint_grad = None
        self.use_analytic_gradient = False  # ê¸°ë³¸ê°’: ìˆ˜ì¹˜ì  ê·¸ë˜ë””ì–¸íŠ¸
    
    def estimate(self, data: pd.DataFrame, 
                measurement_model,
                structural_model,
                choice_model) -> Dict:
        """
        ICLV ëª¨ë¸ ë™ì‹œ ì¶”ì •
        
        Args:
            data: í†µí•© ë°ì´í„°
            measurement_model: ì¸¡ì •ëª¨ë¸ ê°ì²´
            structural_model: êµ¬ì¡°ëª¨ë¸ ê°ì²´
            choice_model: ì„ íƒëª¨ë¸ ê°ì²´
        
        Returns:
            ì¶”ì • ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        print("=" * 70, flush=True)
        print("SimultaneousEstimator.estimate() ì‹œì‘", flush=True)
        print("=" * 70, flush=True)
        self.logger.info("ICLV ëª¨ë¸ ë™ì‹œ ì¶”ì • ì‹œì‘")

        self.data = data
        print(f"ë°ì´í„° shape: {data.shape}", flush=True)
        n_individuals = data[self.config.individual_id_column].nunique()
        print(f"ê°œì¸ ìˆ˜: {n_individuals}", flush=True)
        self.logger.info(f"ê°œì¸ ìˆ˜: {n_individuals}")

        # Halton draws ìƒì„±
        print(f"Halton draws ìƒì„± ì‹œì‘... (n_draws={self.config.estimation.n_draws}, n_individuals={n_individuals})", flush=True)
        self.logger.info(f"Halton draws ìƒì„± ì¤‘... (n_draws={self.config.estimation.n_draws})")
        self.halton_generator = HaltonDrawGenerator(
            n_draws=self.config.estimation.n_draws,
            n_individuals=n_individuals,
            scramble=self.config.estimation.scramble_halton
        )
        print("Halton draws ìƒì„± ì™„ë£Œ", flush=True)
        self.logger.info("Halton draws ìƒì„± ì™„ë£Œ")

        # Gradient calculators ì´ˆê¸°í™” (Apollo ë°©ì‹)
        use_gradient = self.config.estimation.optimizer in ['BFGS', 'L-BFGS-B']
        if use_gradient and hasattr(self.config.estimation, 'use_analytic_gradient'):
            self.use_analytic_gradient = self.config.estimation.use_analytic_gradient
        else:
            self.use_analytic_gradient = False

        if self.use_analytic_gradient:
            print("Analytic gradient calculators ì´ˆê¸°í™” (Apollo ë°©ì‹)...", flush=True)
            self.measurement_grad = MeasurementGradient(
                n_indicators=len(self.config.measurement.indicators),
                n_categories=self.config.measurement.n_categories
            )
            self.structural_grad = StructuralGradient(
                n_sociodem=len(self.config.structural.sociodemographics),
                error_variance=1.0
            )
            self.choice_grad = ChoiceGradient(
                n_attributes=len(self.config.choice.choice_attributes)
            )
            self.joint_grad = JointGradient(
                self.measurement_grad,
                self.structural_grad,
                self.choice_grad
            )
            print("Analytic gradient calculators ì´ˆê¸°í™” ì™„ë£Œ", flush=True)

        # ì´ˆê¸° íŒŒë¼ë¯¸í„° ì„¤ì •
        print("ì´ˆê¸° íŒŒë¼ë¯¸í„° ì„¤ì • ì‹œì‘...", flush=True)
        self.logger.info("ì´ˆê¸° íŒŒë¼ë¯¸í„° ì„¤ì • ì¤‘...")
        initial_params = self._get_initial_parameters(
            measurement_model, structural_model, choice_model
        )
        print(f"ì´ˆê¸° íŒŒë¼ë¯¸í„° ì„¤ì • ì™„ë£Œ (ì´ {len(initial_params)}ê°œ)", flush=True)
        self.logger.info(f"ì´ˆê¸° íŒŒë¼ë¯¸í„° ì„¤ì • ì™„ë£Œ (ì´ {len(initial_params)}ê°œ)")
        
        # ê²°í•© ìš°ë„í•¨ìˆ˜ ì •ì˜ (gradient check ë¡œê¹… ì¶”ê°€)
        iteration_count = [0]  # Mutable counter
        best_ll = [-np.inf]  # Track best log-likelihood

        def negative_log_likelihood(params):
            iteration_count[0] += 1
            ll = self._joint_log_likelihood(
                params, measurement_model, structural_model, choice_model
            )

            # Track best value
            if ll > best_ll[0]:
                best_ll[0] = ll
                improvement = "[NEW BEST]"  # ğŸ”´ âœ“ ëŒ€ì‹  ASCII ë¬¸ì ì‚¬ìš©
            else:
                improvement = ""

            # Log every iteration with more detail
            if iteration_count[0] % 5 == 0 or improvement:
                print(
                    f"Iter {iteration_count[0]:4d}: LL = {ll:12.4f} "
                    f"(Best: {best_ll[0]:12.4f}) {improvement}",
                    flush=True
                )

            return -ll

        # Get parameter bounds
        print("íŒŒë¼ë¯¸í„° bounds ê³„ì‚° ì‹œì‘...", flush=True)
        bounds = self._get_parameter_bounds(
            measurement_model, structural_model, choice_model
        )
        print(f"íŒŒë¼ë¯¸í„° bounds ê³„ì‚° ì™„ë£Œ (ì´ {len(bounds)}ê°œ)", flush=True)

        # ìµœì í™” ë°©ë²• ì„ íƒ
        use_gradient = self.config.estimation.optimizer in ['BFGS', 'L-BFGS-B']

        # Gradient í•¨ìˆ˜ ì •ì˜ (Apollo ë°©ì‹)
        def gradient_function(params):
            """Analytic gradient ê³„ì‚° (Apollo ë°©ì‹)"""
            if not self.use_analytic_gradient:
                return None  # ìˆ˜ì¹˜ì  ê·¸ë˜ë””ì–¸íŠ¸ ì‚¬ìš©

            # íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            param_dict = self._unpack_parameters(
                params, measurement_model, structural_model, choice_model
            )

            # Analytic gradient ê³„ì‚°
            grad_dict = self.joint_grad.compute_gradient(
                data=self.data,
                params_dict=param_dict,
                draws=self.halton_generator.get_draws(),
                individual_id_column=self.config.individual_id_column,
                measurement_model=measurement_model,
                structural_model=structural_model,
                choice_model=choice_model,
                indicators=self.config.measurement.indicators,
                sociodemographics=self.config.structural.sociodemographics,
                choice_attributes=self.config.choice.choice_attributes
            )

            # ê·¸ë˜ë””ì–¸íŠ¸ ë²¡í„°ë¡œ ë³€í™˜ (íŒŒë¼ë¯¸í„° ìˆœì„œì™€ ë™ì¼)
            grad_vector = self._pack_gradient(grad_dict, measurement_model, structural_model, choice_model)

            # Negative gradient (minimize -LL)
            return -grad_vector

        print("=" * 70, flush=True)
        if use_gradient:
            print(f"ìµœì í™” ì‹œì‘: {self.config.estimation.optimizer} (gradient-based)", flush=True)
            if self.use_analytic_gradient:
                print("Analytic gradient ì‚¬ìš© (Apollo ë°©ì‹)", flush=True)
            else:
                print("ìˆ˜ì¹˜ì  ê·¸ë˜ë””ì–¸íŠ¸ ì‚¬ìš© (2-point finite difference)", flush=True)
        else:
            print("ìµœì í™” ì‹œì‘: Nelder-Mead (gradient-free)", flush=True)
        print(f"ì´ˆê¸° íŒŒë¼ë¯¸í„° ê°œìˆ˜: {len(initial_params)}", flush=True)
        print(f"ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜: {self.config.estimation.max_iterations}", flush=True)
        print("=" * 70, flush=True)

        if use_gradient:
            self.logger.info(f"ìµœì í™” ì‹œì‘: {self.config.estimation.optimizer} (gradient-based)")
            if self.use_analytic_gradient:
                self.logger.info("Analytic gradient ì‚¬ìš© (Apollo ë°©ì‹)")
            else:
                self.logger.info("ìˆ˜ì¹˜ì  ê·¸ë˜ë””ì–¸íŠ¸ ì‚¬ìš© (2-point finite difference)")

            # BFGS ë˜ëŠ” L-BFGS-B
            result = optimize.minimize(
                negative_log_likelihood,
                initial_params,
                method=self.config.estimation.optimizer,
                jac=gradient_function if self.use_analytic_gradient else '2-point',
                bounds=bounds if self.config.estimation.optimizer == 'L-BFGS-B' else None,
                options={
                    'maxiter': self.config.estimation.max_iterations,
                    'ftol': 1e-6,
                    'gtol': 1e-5,
                    'disp': True
                }
            )
        else:
            self.logger.info(f"ìµœì í™” ì‹œì‘: Nelder-Mead (gradient-free)")

            result = optimize.minimize(
                negative_log_likelihood,
                initial_params,
                method='Nelder-Mead',
                options={
                    'maxiter': self.config.estimation.max_iterations,
                    'xatol': 1e-4,
                    'fatol': 1e-4,
                    'disp': True
                }
            )
        
        if result.success:
            self.logger.info("ìµœì í™” ì„±ê³µ")
        else:
            self.logger.warning(f"ìµœì í™” ì‹¤íŒ¨: {result.message}")
        
        # ê²°ê³¼ ì²˜ë¦¬
        self.results = self._process_results(
            result, measurement_model, structural_model, choice_model
        )
        
        return self.results
    
    def _joint_log_likelihood(self, params: np.ndarray,
                             measurement_model,
                             structural_model,
                             choice_model) -> float:
        """
        ê²°í•© ë¡œê·¸ìš°ë„ ê³„ì‚°

        ì‹œë®¬ë ˆì´ì…˜ ê¸°ë°˜:
        log L â‰ˆ Î£áµ¢ log[(1/R) Î£áµ£ P(Choice|LVáµ£) Ã— P(Indicators|LVáµ£) Ã— P(LVáµ£|X)]
        """
        # print("_joint_log_likelihood ì‹œì‘", flush=True)

        # íŒŒë¼ë¯¸í„° ë¶„í•´
        param_dict = self._unpack_parameters(
            params, measurement_model, structural_model, choice_model
        )

        total_ll = 0.0
        draws = self.halton_generator.get_draws()

        # ê°œì¸ë³„ ìš°ë„ ê³„ì‚°
        individual_ids = self.data[self.config.individual_id_column].unique()
        # print(f"ê°œì¸ ìˆ˜: {len(individual_ids)}", flush=True)

        for i, ind_id in enumerate(individual_ids):
            ind_data = self.data[self.data[self.config.individual_id_column] == ind_id]
            ind_draws = draws[i, :]  # ì´ ê°œì¸ì˜ draws

            # ğŸ”´ ìˆ˜ì •: logsumexpë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆ˜ì¹˜ ì•ˆì •ì„± í™•ë³´
            # King (2022) Apollo ë°©ì‹
            draw_lls = []

            for j, draw in enumerate(ind_draws):
                # êµ¬ì¡°ëª¨ë¸: LV = Î³*X + Î·
                lv = structural_model.predict(ind_data, param_dict['structural'], draw)

                # ì¸¡ì •ëª¨ë¸ ìš°ë„: P(Indicators|LV)
                ll_measurement = measurement_model.log_likelihood(
                    ind_data, lv, param_dict['measurement']
                )

                # Panel Product: ê°œì¸ì˜ ì—¬ëŸ¬ ì„ íƒ ìƒí™©ì— ëŒ€í•œ í™•ë¥ ì„ ê³±í•¨
                choice_set_lls = []
                for idx in range(len(ind_data)):
                    ll_choice_t = choice_model.log_likelihood(
                        ind_data.iloc[idx:idx+1],  # ê° ì„ íƒ ìƒí™©
                        lv,
                        param_dict['choice']
                    )
                    choice_set_lls.append(ll_choice_t)

                # Panel product: log(P1 * P2 * ... * PT) = log(P1) + log(P2) + ... + log(PT)
                ll_choice = sum(choice_set_lls)

                # êµ¬ì¡°ëª¨ë¸ ìš°ë„: P(LV|X) - ì •ê·œë¶„í¬ ê°€ì •
                ll_structural = structural_model.log_likelihood(
                    ind_data, lv, param_dict['structural'], draw
                )

                # ê²°í•© ë¡œê·¸ìš°ë„
                draw_ll = ll_measurement + ll_choice + ll_structural

                # ğŸ”´ ìˆ˜ì •: -infë¥¼ ë§¤ìš° ì‘ì€ ê°’ìœ¼ë¡œ ëŒ€ì²´ (ì—°ì†ì„± í™•ë³´ for gradient)
                if not np.isfinite(draw_ll):
                    draw_ll = -1e10  # -inf ëŒ€ì‹  ë§¤ìš° ì‘ì€ ê°’

                draw_lls.append(draw_ll)

            # ğŸ”´ ìˆ˜ì •: logsumexpë¥¼ ì‚¬ìš©í•˜ì—¬ í‰ê·  ê³„ì‚°
            # log[(1/R) Î£áµ£ exp(ll_r)] = logsumexp(ll_r) - log(R)
            person_ll = logsumexp(draw_lls) - np.log(len(draw_lls))

            total_ll += person_ll

        return total_ll

    def _get_parameter_bounds(self, measurement_model,
                              structural_model, choice_model) -> list:
        """
        Parameter bounds for L-BFGS-B

        Returns:
            bounds: [(lower, upper), ...] list
        """
        bounds = []

        # Measurement model parameters
        # - Factor loadings (zeta): [0.1, 10]
        n_indicators = len(self.config.measurement.indicators)
        bounds.extend([(0.1, 10.0)] * n_indicators)

        # - Thresholds (tau): [-10, 10]
        n_thresholds = self.config.measurement.n_categories - 1
        for _ in range(n_indicators):
            bounds.extend([(-10.0, 10.0)] * n_thresholds)

        # Structural model parameters (gamma): unbounded
        n_sociodem = len(self.config.structural.sociodemographics)
        bounds.extend([(None, None)] * n_sociodem)

        # Choice model parameters
        # - Intercept: unbounded
        bounds.append((None, None))

        # - Attribute coefficients (beta): unbounded
        n_attributes = len(self.config.choice.choice_attributes)
        bounds.extend([(None, None)] * n_attributes)

        # - Latent variable coefficient (lambda): unbounded
        bounds.append((None, None))

        # - Sociodemographic coefficients: unbounded
        if self.config.structural.include_in_choice:
            bounds.extend([(None, None)] * n_sociodem)

        return bounds

    def _get_initial_parameters(self, measurement_model,
                                structural_model, choice_model) -> np.ndarray:
        """ì´ˆê¸° íŒŒë¼ë¯¸í„° ì„¤ì •"""
        
        params = []
        
        # ì¸¡ì •ëª¨ë¸ íŒŒë¼ë¯¸í„°
        # - ìš”ì¸ì ì¬ëŸ‰ (zeta)
        n_indicators = len(self.config.measurement.indicators)
        params.extend([1.0] * n_indicators)  # zeta
        
        # - ì„ê³„ê°’ (tau)
        n_thresholds = self.config.measurement.n_categories - 1
        for _ in range(n_indicators):
            params.extend([-2, -1, 1, 2])  # 5ì  ì²™ë„ ê¸°ë³¸ê°’
        
        # êµ¬ì¡°ëª¨ë¸ íŒŒë¼ë¯¸í„° (gamma)
        n_sociodem = len(self.config.structural.sociodemographics)
        params.extend([0.0] * n_sociodem)
        
        # ì„ íƒëª¨ë¸ íŒŒë¼ë¯¸í„°
        # - ì ˆí¸
        params.append(0.0)
        
        # - ì†ì„± ê³„ìˆ˜ (beta)
        n_attributes = len(self.config.choice.choice_attributes)
        params.extend([0.0] * n_attributes)
        
        # - ì ì¬ë³€ìˆ˜ ê³„ìˆ˜ (lambda)
        params.append(1.0)
        
        # - ì‚¬íšŒì¸êµ¬í•™ì  ë³€ìˆ˜ ê³„ìˆ˜ (ì„ íƒëª¨ë¸ì— í¬í•¨ë˜ëŠ” ê²½ìš°)
        if self.config.structural.include_in_choice:
            params.extend([0.0] * n_sociodem)
        
        return np.array(params)
    

    
    def _get_parameter_bounds(self, measurement_model,
                              structural_model, choice_model) -> list:
        """
        Parameter bounds for L-BFGS-B
        
        Returns:
            bounds: [(lower, upper), ...] list
        """
        bounds = []
        
        # Measurement model parameters
        # - Factor loadings (zeta): [0.1, 10]
        n_indicators = len(self.config.measurement.indicators)
        bounds.extend([(0.1, 10.0)] * n_indicators)
        
        # - Thresholds (tau): [-10, 10]
        n_thresholds = self.config.measurement.n_categories - 1
        for _ in range(n_indicators):
            bounds.extend([(-10.0, 10.0)] * n_thresholds)
        
        # Structural model parameters (gamma): unbounded
        n_sociodem = len(self.config.structural.sociodemographics)
        bounds.extend([(None, None)] * n_sociodem)
        
        # Choice model parameters
        # - Intercept: unbounded
        bounds.append((None, None))
        
        # - Attribute coefficients (beta): unbounded
        n_attributes = len(self.config.choice.choice_attributes)
        bounds.extend([(None, None)] * n_attributes)
        
        # - Latent variable coefficient (lambda): unbounded
        bounds.append((None, None))
        
        # - Sociodemographic coefficients: unbounded
        if self.config.structural.include_in_choice:
            bounds.extend([(None, None)] * n_sociodem)
        
        return bounds
    def _unpack_parameters(self, params: np.ndarray,
                          measurement_model,
                          structural_model,
                          choice_model) -> Dict[str, Dict]:
        """íŒŒë¼ë¯¸í„° ë²¡í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        
        idx = 0
        param_dict = {
            'measurement': {},
            'structural': {},
            'choice': {}
        }
        
        # ì¸¡ì •ëª¨ë¸ íŒŒë¼ë¯¸í„°
        n_indicators = len(self.config.measurement.indicators)
        param_dict['measurement']['zeta'] = params[idx:idx+n_indicators]
        idx += n_indicators

        n_thresholds = self.config.measurement.n_categories - 1
        # tauë¥¼ 2D ë°°ì—´ë¡œ ì €ì¥ (n_indicators, n_thresholds)
        tau_list = []
        for i in range(n_indicators):
            tau_list.append(params[idx:idx+n_thresholds])
            idx += n_thresholds
        param_dict['measurement']['tau'] = np.array(tau_list)
        
        # êµ¬ì¡°ëª¨ë¸ íŒŒë¼ë¯¸í„°
        n_sociodem = len(self.config.structural.sociodemographics)
        param_dict['structural']['gamma'] = params[idx:idx+n_sociodem]
        idx += n_sociodem
        
        # ì„ íƒëª¨ë¸ íŒŒë¼ë¯¸í„°
        param_dict['choice']['intercept'] = params[idx]
        idx += 1
        
        n_attributes = len(self.config.choice.choice_attributes)
        param_dict['choice']['beta'] = params[idx:idx+n_attributes]
        idx += n_attributes
        
        param_dict['choice']['lambda'] = params[idx]
        idx += 1
        
        if self.config.structural.include_in_choice:
            param_dict['choice']['beta_sociodem'] = params[idx:idx+n_sociodem]
            idx += n_sociodem
        
        return param_dict

    def _pack_gradient(self, grad_dict: Dict, measurement_model,
                      structural_model, choice_model) -> np.ndarray:
        """
        ê·¸ë˜ë””ì–¸íŠ¸ ë”•ì…”ë„ˆë¦¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜ (íŒŒë¼ë¯¸í„° ìˆœì„œì™€ ë™ì¼)

        Args:
            grad_dict: ê·¸ë˜ë””ì–¸íŠ¸ ë”•ì…”ë„ˆë¦¬
            measurement_model: ì¸¡ì •ëª¨ë¸
            structural_model: êµ¬ì¡°ëª¨ë¸
            choice_model: ì„ íƒëª¨ë¸

        Returns:
            gradient_vector: ê·¸ë˜ë””ì–¸íŠ¸ ë²¡í„°
        """
        gradient_list = []

        # ì¸¡ì •ëª¨ë¸ ê·¸ë˜ë””ì–¸íŠ¸
        gradient_list.append(grad_dict['grad_zeta'])
        gradient_list.append(grad_dict['grad_tau'].flatten())

        # êµ¬ì¡°ëª¨ë¸ ê·¸ë˜ë””ì–¸íŠ¸
        gradient_list.append(grad_dict['grad_gamma'])

        # ì„ íƒëª¨ë¸ ê·¸ë˜ë””ì–¸íŠ¸
        gradient_list.append(np.array([grad_dict['grad_intercept']]))
        gradient_list.append(grad_dict['grad_beta'])
        gradient_list.append(np.array([grad_dict['grad_lambda']]))

        # ì‚¬íšŒì¸êµ¬í•™ì  ë³€ìˆ˜ê°€ ì„ íƒëª¨ë¸ì— í¬í•¨ë˜ëŠ” ê²½ìš°
        if self.config.structural.include_in_choice:
            # í˜„ì¬ëŠ” êµ¬í˜„ë˜ì§€ ì•ŠìŒ
            n_sociodem = len(self.config.structural.sociodemographics)
            gradient_list.append(np.zeros(n_sociodem))

        # ë²¡í„°ë¡œ ê²°í•©
        gradient_vector = np.concatenate(gradient_list)

        return gradient_vector

    def _process_results(self, optimization_result,
                        measurement_model,
                        structural_model,
                        choice_model) -> Dict:
        """ìµœì í™” ê²°ê³¼ ì²˜ë¦¬"""
        
        param_dict = self._unpack_parameters(
            optimization_result.x,
            measurement_model,
            structural_model,
            choice_model
        )
        
        results = {
            'success': optimization_result.success,
            'message': optimization_result.message,
            'log_likelihood': -optimization_result.fun,
            'n_iterations': optimization_result.nit,
            'parameters': param_dict,
            'raw_params': optimization_result.x,
            
            # ëª¨ë¸ ì í•©ë„
            'n_observations': len(self.data),
            'n_parameters': len(optimization_result.x),
        }
        
        # AIC, BIC ê³„ì‚°
        ll = results['log_likelihood']
        k = results['n_parameters']
        n = results['n_observations']
        
        results['aic'] = -2 * ll + 2 * k
        results['bic'] = -2 * ll + k * np.log(n)
        
        # í‘œì¤€ì˜¤ì°¨ ê³„ì‚° (Hessian ê¸°ë°˜)
        if self.config.estimation.calculate_se:
            try:
                hessian = optimization_result.hess_inv
                if hasattr(hessian, 'todense'):
                    hessian = hessian.todense()
                
                se = np.sqrt(np.diag(hessian))
                results['standard_errors'] = se
                
                # t-í†µê³„ëŸ‰
                results['t_statistics'] = optimization_result.x / se
                
                # p-ê°’
                from scipy.stats import t
                results['p_values'] = 2 * (1 - t.cdf(np.abs(results['t_statistics']), n - k))
                
            except Exception as e:
                self.logger.warning(f"í‘œì¤€ì˜¤ì°¨ ê³„ì‚° ì‹¤íŒ¨: {e}")
        
        return results


def estimate_iclv_simultaneous(data: pd.DataFrame, config,
                               measurement_model,
                               structural_model,
                               choice_model) -> Dict:
    """
    ICLV ëª¨ë¸ ë™ì‹œ ì¶”ì • í—¬í¼ í•¨ìˆ˜
    
    Args:
        data: í†µí•© ë°ì´í„°
        config: ICLVConfig
        measurement_model: ì¸¡ì •ëª¨ë¸
        structural_model: êµ¬ì¡°ëª¨ë¸
        choice_model: ì„ íƒëª¨ë¸
    
    Returns:
        ì¶”ì • ê²°ê³¼
    """
    estimator = SimultaneousEstimator(config)
    return estimator.estimate(data, measurement_model, structural_model, choice_model)

