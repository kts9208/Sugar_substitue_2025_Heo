"""
Simultaneous Estimation for ICLV Models

ICLV ëª¨ë¸ì˜ ë™ì‹œ ì¶”ì • ì—”ì§„ì…ë‹ˆë‹¤.
Apollo íŒ¨í‚¤ì§€ì˜ ë™ì‹œ ì¶”ì • ë°©ë²•ë¡ ì„ Pythonìœ¼ë¡œ êµ¬í˜„í•©ë‹ˆë‹¤.

ì°¸ì¡°:
- King (2022) - Apollo íŒ¨í‚¤ì§€ ì‚¬ìš©
- Train (2009) - Discrete Choice Methods with Simulation
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Tuple, Callable
from scipy import optimize
from scipy.stats import norm, qmc
from scipy.special import logsumexp
import logging
from concurrent.futures import ProcessPoolExecutor
import multiprocessing
import os

from .gradient_calculator import (
    MeasurementGradient,
    StructuralGradient,
    ChoiceGradient,
    JointGradient
)
from .parameter_scaler import ParameterScaler
from .parameter_context import ParameterContext
from .bhhh_calculator import BHHHCalculator
from .parameter_manager import ParameterManager
from .gpu_compute_state import GPUComputeState

logger = logging.getLogger(__name__)


# ============================================================================
# ë³‘ë ¬ì²˜ë¦¬ë¥¼ ìœ„í•œ ì „ì—­ í•¨ìˆ˜ (pickle ê°€ëŠ¥)
# ============================================================================

def _compute_individual_likelihood_parallel(args):
    """
    ê°œì¸ë³„ ìš°ë„ ê³„ì‚° (ë³‘ë ¬ì²˜ë¦¬ìš© ì „ì—­ í•¨ìˆ˜)

    Args:
        args: (ind_data_dict, ind_draws, param_dict, config_dict)
            - ind_data_dict: ê°œì¸ ë°ì´í„° (dict í˜•íƒœ)
            - ind_draws: Halton draws
            - param_dict: íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬
            - config_dict: ì„¤ì • ì •ë³´

    Returns:
        ê°œì¸ì˜ ë¡œê·¸ìš°ë„
    """
    # ë³‘ë ¬ í”„ë¡œì„¸ìŠ¤ì—ì„œ ë¶ˆí•„ìš”í•œ ë¡œê·¸ ì–µì œ
    import logging
    logging.getLogger('root').setLevel(logging.CRITICAL)

    from .measurement_equations import OrderedProbitMeasurement
    from .structural_equations import LatentVariableRegression
    from .choice_equations import BinaryProbitChoice
    from .iclv_config import MeasurementConfig, StructuralConfig, ChoiceConfig

    ind_data_dict, ind_draws, param_dict, config_dict = args

    # DataFrame ë³µì›
    ind_data = pd.DataFrame(ind_data_dict)

    # ëª¨ë¸ ì¬ìƒì„± (ê° í”„ë¡œì„¸ìŠ¤ì—ì„œ)
    measurement_config = MeasurementConfig(**config_dict['measurement'])
    structural_config = StructuralConfig(**config_dict['structural'])
    choice_config = ChoiceConfig(**config_dict['choice'])

    measurement_model = OrderedProbitMeasurement(measurement_config)
    structural_model = LatentVariableRegression(structural_config)
    choice_model = BinaryProbitChoice(choice_config)

    # ìš°ë„ ê³„ì‚°
    draw_lls = []

    for j, draw in enumerate(ind_draws):
        # êµ¬ì¡°ëª¨ë¸: LV = Î³*X + Î·
        lv = structural_model.predict(ind_data, param_dict['structural'], draw)

        # ì¸¡ì •ëª¨ë¸ ìš°ë„: P(Indicators|LV)
        ll_measurement = measurement_model.log_likelihood(
            ind_data, lv, param_dict['measurement']
        )

        # Panel Product: ê°œì¸ì˜ ì—¬ëŸ¬ ì„ íƒ ìƒí™©ì— ëŒ€í•œ í™•ë¥ ì„ ê³±í•¨
        choice_set_lls = []
        for idx in range(len(ind_data)):
            ll_choice_t = choice_model.log_likelihood(
                ind_data.iloc[idx:idx+1],
                lv,
                param_dict['choice']
            )
            choice_set_lls.append(ll_choice_t)

        ll_choice = sum(choice_set_lls)

        # êµ¬ì¡°ëª¨ë¸ ìš°ë„: P(LV|X)
        ll_structural = structural_model.log_likelihood(
            ind_data, lv, param_dict['structural'], draw
        )

        # ê²°í•© ë¡œê·¸ìš°ë„
        draw_ll = ll_measurement + ll_choice + ll_structural

        if not np.isfinite(draw_ll):
            draw_ll = -1e10

        draw_lls.append(draw_ll)

    # logsumexpë¥¼ ì‚¬ìš©í•˜ì—¬ í‰ê·  ê³„ì‚°
    person_ll = logsumexp(draw_lls) - np.log(len(draw_lls))

    return person_ll


class HaltonDrawGenerator:
    """
    Halton ì‹œí€€ìŠ¤ ìƒì„±ê¸°
    
    ì¤€ë‚œìˆ˜(Quasi-random) ì‹œí€€ìŠ¤ë¥¼ ìƒì„±í•˜ì—¬ ì‹œë®¬ë ˆì´ì…˜ ì •í™•ë„ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
    ì¼ë°˜ ë‚œìˆ˜ë³´ë‹¤ ê³µê°„ì„ ë” ê· ë“±í•˜ê²Œ ì»¤ë²„í•©ë‹ˆë‹¤.
    
    ì°¸ì¡°: Apollo íŒ¨í‚¤ì§€ì˜ Halton draws
    """
    
    def __init__(self, n_draws: int, n_individuals: int, n_dimensions: int = 1,
                 scramble: bool = True, seed: Optional[int] = None):
        """
        Args:
            n_draws: ê°œì¸ë‹¹ draw ìˆ˜
            n_individuals: ê°œì¸ ìˆ˜
            n_dimensions: ì°¨ì› ìˆ˜ (1ì°¨ LV ê°œìˆ˜ + 2ì°¨+ LV ê°œìˆ˜)
            scramble: ìŠ¤í¬ë¨ë¸” ì—¬ë¶€ (ê¶Œì¥)
            seed: ë‚œìˆ˜ ì‹œë“œ
        """
        self.n_draws = n_draws
        self.n_individuals = n_individuals
        self.n_dimensions = n_dimensions
        self.scramble = scramble
        self.seed = seed

        self.draws = None
        self._generate_draws()

    def _generate_draws(self):
        """Halton ì‹œí€€ìŠ¤ ìƒì„±"""
        logger.info(f"Halton draws ìƒì„±: {self.n_individuals} ê°œì¸ Ã— {self.n_draws} draws Ã— {self.n_dimensions} ì°¨ì›")

        # scipyì˜ Halton ì‹œí€€ìŠ¤ ìƒì„±ê¸° ì‚¬ìš©
        sampler = qmc.Halton(d=self.n_dimensions, scramble=self.scramble, seed=self.seed)

        # ê· ë“±ë¶„í¬ [0,1] ìƒ˜í”Œ ìƒì„±
        uniform_draws = sampler.random(n=self.n_individuals * self.n_draws)

        # í‘œì¤€ì •ê·œë¶„í¬ë¡œ ë³€í™˜ (ì—­ëˆ„ì ë¶„í¬í•¨ìˆ˜)
        normal_draws = norm.ppf(uniform_draws)

        # í˜•íƒœ ì¬êµ¬ì„±
        if self.n_dimensions == 1:
            # ë‹¨ì¼ ì°¨ì›: (n_individuals, n_draws)
            self.draws = normal_draws.reshape(self.n_individuals, self.n_draws)
        else:
            # ë‹¤ì°¨ì›: (n_individuals, n_draws, n_dimensions)
            self.draws = normal_draws.reshape(self.n_individuals, self.n_draws, self.n_dimensions)

        logger.info(f"Halton draws ìƒì„± ì™„ë£Œ: shape={self.draws.shape}")
    
    def get_draws(self) -> np.ndarray:
        """ìƒì„±ëœ draws ë°˜í™˜"""
        return self.draws
    
    def get_draw_for_individual(self, individual_idx: int) -> np.ndarray:
        """íŠ¹ì • ê°œì¸ì˜ draws ë°˜í™˜"""
        return self.draws[individual_idx, :]


class SimultaneousEstimator:
    """
    ICLV ëª¨ë¸ ë™ì‹œ ì¶”ì •ê¸°
    
    ì¸¡ì •ëª¨ë¸, êµ¬ì¡°ëª¨ë¸, ì„ íƒëª¨ë¸ì„ ë™ì‹œì— ì¶”ì •í•©ë‹ˆë‹¤.
    
    ê²°í•© ìš°ë„í•¨ìˆ˜:
    L = âˆáµ¢ âˆ« P(Choice|LV) Ã— P(Indicators|LV) Ã— P(LV|X) dLV
    
    ì‹œë®¬ë ˆì´ì…˜ ê¸°ë°˜ ì¶”ì •:
    L â‰ˆ âˆáµ¢ (1/R) Î£áµ£ P(Choice|LVáµ£) Ã— P(Indicators|LVáµ£) Ã— P(LVáµ£|X)
    """
    
    def __init__(self, config):
        """
        Args:
            config: ICLVConfig ê°ì²´
        """
        self.config = config
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")

        self.halton_generator = None
        self.data = None
        self.results = None

        # ë¡œê·¸ íŒŒì¼ í•¸ë“¤ëŸ¬ (ì¶”ì • ì‹œì‘ ì‹œ ì„¤ì •)
        self.log_file_handler = None
        self.iteration_logger = None

        # Gradient calculators (Apollo ë°©ì‹)
        self.measurement_grad = None
        self.structural_grad = None
        self.choice_grad = None
        self.joint_grad = None
        self.use_analytic_gradient = False  # ê¸°ë³¸ê°’: ìˆ˜ì¹˜ì  ê·¸ë˜ë””ì–¸íŠ¸

        # âœ… ParameterManager ì´ˆê¸°í™”
        self.param_manager = ParameterManager(config)
        self.param_names = None  # estimate() ì‹œì‘ ì‹œ ìƒì„±

    def _setup_iteration_logger(self, log_file_path: str):
        """
        ë°˜ë³µ ê³¼ì • ë¡œê¹…ì„ ìœ„í•œ íŒŒì¼ í•¸ë“¤ëŸ¬ ì„¤ì •

        Args:
            log_file_path: ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
        """
        # ë°˜ë³µ ê³¼ì • ì „ìš© ë¡œê±° ìƒì„±
        self.iteration_logger = logging.getLogger('iclv_iteration')
        self.iteration_logger.setLevel(logging.INFO)

        # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±° (ì¤‘ë³µ ë°©ì§€)
        self.iteration_logger.handlers.clear()

        # íŒŒì¼ í•¸ë“¤ëŸ¬ ì¶”ê°€
        self.log_file_handler = logging.FileHandler(log_file_path, mode='w', encoding='utf-8')
        self.log_file_handler.setLevel(logging.INFO)

        # í¬ë§· ì„¤ì •
        formatter = logging.Formatter('%(asctime)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
        self.log_file_handler.setFormatter(formatter)

        self.iteration_logger.addHandler(self.log_file_handler)

        # CSV ë¡œê·¸ íŒŒì¼ ì„¤ì • (íŒŒë¼ë¯¸í„° ë° ê·¸ë˜ë””ì–¸íŠ¸ ê°’ ì €ì¥ìš©)
        import csv
        csv_log_path = log_file_path.replace('.txt', '_params_grads.csv')
        self.csv_log_file = open(csv_log_path, 'w', newline='', encoding='utf-8')
        self.csv_writer = None  # ì²« ë²ˆì§¸ ê¸°ë¡ ì‹œ í—¤ë”ì™€ í•¨ê»˜ ì´ˆê¸°í™”
        self.csv_log_path = csv_log_path

    def _log_params_grads_to_csv(self, iteration, params, grads):
        """
        íŒŒë¼ë¯¸í„°ì™€ ê·¸ë˜ë””ì–¸íŠ¸ ê°’ì„ CSV íŒŒì¼ì— ê¸°ë¡

        Args:
            iteration: Major iteration ë²ˆí˜¸
            params: íŒŒë¼ë¯¸í„° ê°’ ë°°ì—´ (external scale)
            grads: ê·¸ë˜ë””ì–¸íŠ¸ ê°’ ë°°ì—´
        """
        import csv

        # ì²« ë²ˆì§¸ ê¸°ë¡ ì‹œ í—¤ë” ì‘ì„±
        if self.csv_writer is None:
            fieldnames = ['iteration']

            # íŒŒë¼ë¯¸í„° ì´ë¦„ ì¶”ê°€
            for idx in range(len(params)):
                param_name = self.param_names[idx] if hasattr(self, 'param_names') and idx < len(self.param_names) else f"param_{idx}"
                fieldnames.append(f'{param_name}_value')
                fieldnames.append(f'{param_name}_grad')

            self.csv_writer = csv.DictWriter(self.csv_log_file, fieldnames=fieldnames)
            self.csv_writer.writeheader()

        # ë°ì´í„° í–‰ ì‘ì„±
        row = {'iteration': iteration}
        for idx in range(len(params)):
            param_name = self.param_names[idx] if hasattr(self, 'param_names') and idx < len(self.param_names) else f"param_{idx}"
            row[f'{param_name}_value'] = params[idx]
            row[f'{param_name}_grad'] = grads[idx]

        self.csv_writer.writerow(row)
        self.csv_log_file.flush()  # ì¦‰ì‹œ ë””ìŠ¤í¬ì— ê¸°ë¡

        # ì½˜ì†” í•¸ë“¤ëŸ¬ ì œê±° (ì¤‘ë³µ ë°©ì§€ - íŒŒì¼ë§Œ ì‚¬ìš©)
        # console_handler = logging.StreamHandler()
        # console_handler.setLevel(logging.INFO)
        # console_handler.setFormatter(formatter)
        # self.iteration_logger.addHandler(console_handler)

        self.iteration_logger.info("="*70)
        self.iteration_logger.info("ICLV ëª¨ë¸ ì¶”ì • ì‹œì‘")
        self.iteration_logger.info("="*70)

    def _close_iteration_logger(self):
        """ë°˜ë³µ ê³¼ì • ë¡œê±° ì¢…ë£Œ"""
        if self.log_file_handler:
            self.iteration_logger.removeHandler(self.log_file_handler)
            self.log_file_handler.close()
            self.log_file_handler = None
    
    def estimate(self, data: pd.DataFrame,
                measurement_model,
                structural_model,
                choice_model,
                log_file: Optional[str] = None,
                initial_params: Optional[Dict] = None) -> Dict:
        """
        ICLV ëª¨ë¸ ë™ì‹œ ì¶”ì • (ì¸¡ì •ëª¨ë¸ ê³ ì •)

        âœ… ì¸¡ì •ëª¨ë¸ íŒŒë¼ë¯¸í„°ëŠ” í•­ìƒ ê³ ì • (CFA ê²°ê³¼ ì‚¬ìš©)
        âœ… êµ¬ì¡°ëª¨ë¸ + ì„ íƒëª¨ë¸ íŒŒë¼ë¯¸í„°ë§Œ ì¶”ì •

        Args:
            data: í†µí•© ë°ì´í„°
            measurement_model: ì¸¡ì •ëª¨ë¸ ê°ì²´ (CFA ê²°ê³¼ í¬í•¨)
            structural_model: êµ¬ì¡°ëª¨ë¸ ê°ì²´
            choice_model: ì„ íƒëª¨ë¸ ê°ì²´
            log_file: ë¡œê·¸ íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ìë™ ìƒì„±)
            initial_params: ì´ˆê¸° íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬ (ì„ íƒ, êµ¬ì¡°ëª¨ë¸ + ì„ íƒëª¨ë¸ë§Œ)

        Returns:
            ì¶”ì • ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        # ë¡œê·¸ íŒŒì¼ ì„¤ì •
        if log_file is None:
            from pathlib import Path
            from datetime import datetime

            results_dir = Path('results')
            results_dir.mkdir(exist_ok=True)

            # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            log_file = results_dir / f'iclv_estimation_log_{timestamp}.txt'

        self._setup_iteration_logger(str(log_file))

        self.iteration_logger.info("SimultaneousEstimator.estimate() ì‹œì‘")
        self.iteration_logger.info("ICLV ëª¨ë¸ ë™ì‹œ ì¶”ì • ì‹œì‘")

        # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ì˜ loggerë¥¼ iteration_loggerë¡œ ì—…ë°ì´íŠ¸
        if hasattr(self, 'memory_monitor') and self.memory_monitor is not None:
            self.memory_monitor.logger = self.iteration_logger

        self.data = data
        n_individuals = data[self.config.individual_id_column].nunique()

        self.iteration_logger.info(f"ë°ì´í„° shape: {data.shape}")
        self.iteration_logger.info(f"ê°œì¸ ìˆ˜: {n_individuals}")

        # Halton draws ìƒì„± (ì´ë¯¸ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ ê±´ë„ˆë›°ê¸°)
        if not hasattr(self, 'halton_generator') or self.halton_generator is None:
            # âœ… ë‹¤ì°¨ì› Halton draws: 1ì°¨ LV + 2ì°¨+ LV
            n_exo = len(structural_model.exogenous_lvs)
            higher_order_lvs = structural_model.get_higher_order_lvs()
            n_higher_order = len(higher_order_lvs)
            n_dimensions = n_exo + n_higher_order

            self.iteration_logger.info(
                f"\n{'='*70}\n"
                f"Halton draws ìƒì„± ì‹œì‘\n"
                f"{'='*70}\n"
                f"  n_draws: {self.config.estimation.n_draws}\n"
                f"  n_individuals: {n_individuals}\n"
                f"  n_dimensions: {n_dimensions}\n"
                f"    - 1ì°¨ LV ({n_exo}ê°œ): {structural_model.exogenous_lvs}\n"
                f"    - ê³ ì°¨ LV ({n_higher_order}ê°œ): {higher_order_lvs}\n"
                f"{'='*70}"
            )
            self.halton_generator = HaltonDrawGenerator(
                n_draws=self.config.estimation.n_draws,
                n_individuals=n_individuals,
                n_dimensions=n_dimensions,
                scramble=self.config.estimation.scramble_halton
            )

            # ğŸ” ë””ë²„ê¹…: ì²« ë²ˆì§¸ ê°œì¸ì˜ ì²« ë²ˆì§¸ draw ì¶œë ¥
            draws = self.halton_generator.get_draws()
            self.iteration_logger.info(
                f"\nHalton draws ìƒì„± ì™„ë£Œ\n"
                f"  Shape: {draws.shape}\n"
                f"  ì²« ë²ˆì§¸ ê°œì¸ì˜ ì²« ë²ˆì§¸ draw: {draws[0, 0] if draws.ndim > 1 else draws[0]}\n"
                f"{'='*70}\n"
            )
        else:
            self.iteration_logger.info("Halton draws ì´ë¯¸ ì„¤ì •ë¨ (ê±´ë„ˆë›°ê¸°)")

        # Gradient calculators ì´ˆê¸°í™” (Apollo ë°©ì‹)
        use_gradient = self.config.estimation.optimizer in ['BFGS', 'L-BFGS-B', 'BHHH']
        if use_gradient and hasattr(self.config.estimation, 'use_analytic_gradient'):
            self.use_analytic_gradient = self.config.estimation.use_analytic_gradient
        else:
            self.use_analytic_gradient = False

        if self.use_analytic_gradient:
            self.iteration_logger.info("Analytic gradient calculators ì´ˆê¸°í™” (Apollo ë°©ì‹)...")

            # ë‹¤ì¤‘ ì ì¬ë³€ìˆ˜ ì§€ì› í™•ì¸
            from .multi_latent_config import MultiLatentConfig
            is_multi_latent = isinstance(self.config, MultiLatentConfig)

            if is_multi_latent:
                # ë‹¤ì¤‘ ì ì¬ë³€ìˆ˜: MultiLatentMeasurementGradient ì‚¬ìš©
                from .multi_latent_gradient import MultiLatentMeasurementGradient
                self.measurement_grad = MultiLatentMeasurementGradient(
                    self.config.measurement_configs
                )
                self.iteration_logger.info(f"ë‹¤ì¤‘ ì ì¬ë³€ìˆ˜ ì¸¡ì •ëª¨ë¸ gradient ì´ˆê¸°í™”: {len(self.config.measurement_configs)}ê°œ LV")
            else:
                # ë‹¨ì¼ ì ì¬ë³€ìˆ˜
                self.measurement_grad = MeasurementGradient(
                    n_indicators=len(self.config.measurement.indicators),
                    n_categories=self.config.measurement.n_categories
                )
                self.iteration_logger.info("ë‹¨ì¼ ì ì¬ë³€ìˆ˜ ì¸¡ì •ëª¨ë¸ gradient ì´ˆê¸°í™”")

            # êµ¬ì¡°ëª¨ë¸ gradient
            if is_multi_latent:
                # ë‹¤ì¤‘ ì ì¬ë³€ìˆ˜: MultiLatentStructuralGradient ì‚¬ìš©
                from .multi_latent_gradient import MultiLatentStructuralGradient
                self.structural_grad = MultiLatentStructuralGradient(
                    n_exo=self.config.structural.n_exo,
                    n_cov=self.config.structural.n_cov,
                    error_variance=self.config.structural.error_variance
                )
                self.iteration_logger.info("ë‹¤ì¤‘ ì ì¬ë³€ìˆ˜ êµ¬ì¡°ëª¨ë¸ gradient ì´ˆê¸°í™”")
            else:
                # ë‹¨ì¼ ì ì¬ë³€ìˆ˜
                self.structural_grad = StructuralGradient(
                    n_sociodem=len(self.config.structural.sociodemographics),
                    error_variance=1.0
                )
                self.iteration_logger.info("ë‹¨ì¼ ì ì¬ë³€ìˆ˜ êµ¬ì¡°ëª¨ë¸ gradient ì´ˆê¸°í™”")

            # ì„ íƒëª¨ë¸ gradient (ë‹¤ì¤‘ ì ì¬ë³€ìˆ˜ë„ ë™ì¼)
            self.choice_grad = ChoiceGradient(
                n_attributes=len(self.config.choice.choice_attributes)
            )

            # JointGradient
            if is_multi_latent:
                # ë‹¤ì¤‘ ì ì¬ë³€ìˆ˜: MultiLatentJointGradient ì‚¬ìš©
                from .multi_latent_gradient import MultiLatentJointGradient

                # GPU ì‚¬ìš© ì—¬ë¶€ í™•ì¸
                use_gpu_gradient = False
                gpu_measurement_model = None

                if hasattr(self, 'use_gpu') and self.use_gpu:
                    if hasattr(self, 'gpu_measurement_model') and self.gpu_measurement_model is not None:
                        use_gpu_gradient = True
                        gpu_measurement_model = self.gpu_measurement_model
                        self.iteration_logger.info("GPU ë°°ì¹˜ ê·¸ë˜ë””ì–¸íŠ¸ í™œì„±í™”")

                # ì™„ì „ ë³‘ë ¬ ì²˜ë¦¬ ì˜µì…˜ í™•ì¸
                use_full_parallel = getattr(self, 'use_full_parallel', True)

                self.joint_grad = MultiLatentJointGradient(
                    self.measurement_grad,
                    self.structural_grad,
                    self.choice_grad,
                    use_gpu=use_gpu_gradient,
                    gpu_measurement_model=gpu_measurement_model,
                    use_full_parallel=use_full_parallel
                )
                # âœ… iteration_loggerì™€ config ì „ë‹¬
                self.joint_grad.iteration_logger = self.iteration_logger
                self.joint_grad.config = self.config

                self.iteration_logger.info("ë‹¤ì¤‘ ì ì¬ë³€ìˆ˜ JointGradient ì´ˆê¸°í™” ì™„ë£Œ")
                self.iteration_logger.info("âœ… ë™ì‹œì¶”ì •: ì¸¡ì •ëª¨ë¸ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ì œì™¸ (ê³ ì • íŒŒë¼ë¯¸í„°)")
            else:
                # ë‹¨ì¼ ì ì¬ë³€ìˆ˜
                self.joint_grad = JointGradient(
                    self.measurement_grad,
                    self.structural_grad,
                    self.choice_grad
                )
                self.iteration_logger.info("ë‹¨ì¼ ì ì¬ë³€ìˆ˜ JointGradient ì´ˆê¸°í™” ì™„ë£Œ")

        # âœ… ë™ì‹œì¶”ì • ì „ìš© ì´ˆê¸° íŒŒë¼ë¯¸í„° ì„¤ì • (ì¸¡ì •ëª¨ë¸ ì œì™¸)
        self.iteration_logger.info("ì´ˆê¸° íŒŒë¼ë¯¸í„° ì„¤ì • ì‹œì‘...")
        initial_params_opt = self._get_initial_parameters_simultaneous(
            measurement_model, structural_model, choice_model,
            user_initial_params=initial_params
        )
        self.iteration_logger.info(f"âœ… ì´ˆê¸° íŒŒë¼ë¯¸í„° ì„¤ì • ì™„ë£Œ: {len(initial_params_opt)}ê°œ (ì¸¡ì •ëª¨ë¸ ì œì™¸)")

        # ìµœì í™”í•  íŒŒë¼ë¯¸í„° ì´ë¦„ (ì¸¡ì •ëª¨ë¸ ì œì™¸)
        param_names_opt = self.param_manager.get_optimized_parameter_names(
            structural_model, choice_model
        )

        # âœ… self.param_namesë¥¼ ìµœì í™” íŒŒë¼ë¯¸í„°ë¡œ ì—…ë°ì´íŠ¸
        self.param_names = param_names_opt

        # ìµœì í™”ì— ì‚¬ìš©í•  íŒŒë¼ë¯¸í„°
        initial_params = initial_params_opt
        param_names = param_names_opt

        # íŒŒë¼ë¯¸í„° ìŠ¤ì¼€ì¼ë§ ì„¤ì • í™•ì¸
        use_parameter_scaling = getattr(self.config.estimation, 'use_parameter_scaling', True)

        if use_parameter_scaling:
            # Custom scales ìƒì„± (gradient ê· í˜• ìµœì í™”)
            custom_scales = self._get_custom_scales(param_names)

            # Apollo-style íŒŒë¼ë¯¸í„° ìŠ¤ì¼€ì¼ë§ ì´ˆê¸°í™”
            self.iteration_logger.info("=" * 80)
            self.iteration_logger.info("íŒŒë¼ë¯¸í„° ìŠ¤ì¼€ì¼ë§ ì´ˆê¸°í™” (Gradient-Balanced)")
            self.iteration_logger.info("=" * 80)
            self.param_scaler = ParameterScaler(
                initial_params=initial_params,
                param_names=param_names,
                custom_scales=custom_scales,
                logger=self.iteration_logger
            )

            # ì´ˆê¸° íŒŒë¼ë¯¸í„°ë¥¼ ìŠ¤ì¼€ì¼ë§ (External â†’ Internal)
            initial_params_scaled = self.param_scaler.scale_parameters(initial_params)

            # ìŠ¤ì¼€ì¼ë§ ë¹„êµ ë¡œê¹…
            self.param_scaler.log_parameter_comparison(initial_params, initial_params_scaled)
        else:
            # ìŠ¤ì¼€ì¼ë§ ë¹„í™œì„±í™”: í•­ë“± ìŠ¤ì¼€ì¼ëŸ¬ ì‚¬ìš©
            self.iteration_logger.info("=" * 80)
            self.iteration_logger.info("íŒŒë¼ë¯¸í„° ìŠ¤ì¼€ì¼ë§ ë¹„í™œì„±í™”")
            self.iteration_logger.info("=" * 80)
            self.param_scaler = ParameterScaler(
                initial_params=initial_params,
                param_names=param_names,
                custom_scales={name: 1.0 for name in param_names},  # ëª¨ë“  ìŠ¤ì¼€ì¼ì„ 1.0ìœ¼ë¡œ ì„¤ì •
                logger=self.iteration_logger
            )
            initial_params_scaled = initial_params  # ìŠ¤ì¼€ì¼ë§ ì—†ìŒ

        # âœ… ParameterContext ìƒì„± (íŒŒë¼ë¯¸í„° ë³€í™˜ ë¡œì§ ë‹¨ì¼í™”)
        self.iteration_logger.info("=" * 80)
        self.iteration_logger.info("ParameterContext ì´ˆê¸°í™” (ë™ì‹œì¶”ì • ì „ìš©)")
        self.iteration_logger.info("=" * 80)
        param_context = ParameterContext(
            param_manager=self.param_manager,
            param_scaler=self.param_scaler,
            measurement_model=measurement_model,
            logger=self.iteration_logger
        )
        self.iteration_logger.info("=" * 80)

        # ê²°í•© ìš°ë„í•¨ìˆ˜ ì •ì˜ (ë‹¨ê³„ë³„ ë¡œê¹… ì¶”ê°€)
        iteration_count = [0]  # Mutable counter
        best_ll = [-np.inf]  # Track best log-likelihood
        func_call_count = [0]  # í•¨ìˆ˜ í˜¸ì¶œ íšŸìˆ˜ (ìš°ë„ ê³„ì‚°)
        major_iter_count = [0]  # Major iteration ì¹´ìš´í„°
        line_search_call_count = [0]  # Line search ë‚´ í•¨ìˆ˜ í˜¸ì¶œ ì¹´ìš´í„°
        last_major_iter_func_value = [None]  # ë§ˆì§€ë§‰ major iterationì˜ í•¨ìˆ˜ê°’
        last_major_iter_ftol = [None]  # ë§ˆì§€ë§‰ major iterationì˜ ftol ê°’
        last_major_iter_gtol = [None]  # ë§ˆì§€ë§‰ major iterationì˜ gtol ê°’
        current_major_iter_start_call = [0]  # í˜„ì¬ major iteration ì‹œì‘ ì‹œ í•¨ìˆ˜ í˜¸ì¶œ ë²ˆí˜¸
        line_search_func_values = []  # Line search ì¤‘ í•¨ìˆ˜ê°’ ê¸°ë¡
        line_search_start_func_value = [None]  # Line search ì‹œì‘ ì‹œ í•¨ìˆ˜ê°’
        line_search_start_params = [None]  # Line search ì‹œì‘ ì‹œ íŒŒë¼ë¯¸í„°
        line_search_gradient = [None]  # Line search ì‹œì‘ ì‹œ gradient
        line_search_directional_derivative = [None]  # âˆ‡f(x)^TÂ·d (ì‹œì‘ ì‹œ)

        def negative_log_likelihood(params_scaled):
            """
            Negative log-likelihood function (ìŠ¤ì¼€ì¼ëœ íŒŒë¼ë¯¸í„° ì‚¬ìš©)

            Args:
                params_scaled: ìŠ¤ì¼€ì¼ëœ (internal) íŒŒë¼ë¯¸í„°
                              (ì¸¡ì •ëª¨ë¸ ê³ ì • ì‹œ êµ¬ì¡°ëª¨ë¸+ì„ íƒëª¨ë¸ë§Œ í¬í•¨)

            Returns:
                Negative log-likelihood
            """
            func_call_count[0] += 1

            # âœ… ParameterContextë¥¼ ì‚¬ìš©í•œ íŒŒë¼ë¯¸í„° ë³€í™˜
            # âœ… ë™ì‹œì¶”ì •: params_scaledëŠ” ì´ë¯¸ ìµœì í™” íŒŒë¼ë¯¸í„°ë§Œ í¬í•¨ (8ê°œ)
            params_opt = param_context.to_full_external(params_scaled)

            # Line search ì¤‘ì¸ì§€ íŒë‹¨
            # Major iteration ì‹œì‘ ì§í›„ ì²« í˜¸ì¶œì´ ì•„ë‹ˆë©´ line search ì¤‘
            calls_since_major_start = func_call_count[0] - current_major_iter_start_call[0]

            if calls_since_major_start == 1:
                # Major iteration ì‹œì‘ ì‹œ ì²« í•¨ìˆ˜ í˜¸ì¶œ
                context = f"Major Iteration #{major_iter_count[0] + 1} ì‹œì‘"
                line_search_call_count[0] = 0
                line_search_func_values.clear()
                line_search_start_params[0] = params_scaled.copy()  # âœ… ìµœì í™” íŒŒë¼ë¯¸í„°ë§Œ (8ê°œ)
            elif calls_since_major_start > 1:
                # Line search ì¤‘
                line_search_call_count[0] += 1
                context = f"Line Search í•¨ìˆ˜ í˜¸ì¶œ #iter{major_iter_count[0] + 1}-{line_search_call_count[0]}"
            else:
                # ì´ˆê¸° í˜¸ì¶œ
                context = "ì´ˆê¸° í•¨ìˆ˜ê°’ ê³„ì‚°"
                line_search_start_params[0] = params_scaled.copy()  # âœ… ì´ˆê¸° í˜¸ì¶œ ì‹œì—ë„ ì €ì¥

            # ë‹¨ê³„ ë¡œê·¸: ìš°ë„ ê³„ì‚° ì‹œì‘
            self.iteration_logger.info(f"[{context}] [ë‹¨ê³„ 1/2] ì „ì²´ ìš°ë„ ê³„ì‚°")

            ll = self._joint_log_likelihood(
                params_opt, measurement_model, structural_model, choice_model
            )

            # Track best value
            if ll > best_ll[0]:
                best_ll[0] = ll
                improvement = "[NEW BEST]"
            else:
                improvement = ""

            # í•¨ìˆ˜ê°’ ì¶œë ¥
            neg_ll = -ll  # scipyê°€ ìµœì†Œí™”í•˜ëŠ” ê°’
            log_msg = f"  LL = {ll:12.4f} (Best: {best_ll[0]:12.4f}) {improvement}"
            self.iteration_logger.info(log_msg)

            # Line search ì¤‘ì´ë©´ í•¨ìˆ˜ê°’ ë³€í™” ë¡œê¹…
            if calls_since_major_start == 1:
                line_search_start_func_value[0] = neg_ll
                # âœ… params_scaled ì €ì¥ (ìµœì í™” íŒŒë¼ë¯¸í„°ë§Œ, 8ê°œ)
                # (ì´ë¯¸ ìœ„ì—ì„œ line_search_start_params[0] = params_scaled.copy() ì‹¤í–‰ë¨)
            elif calls_since_major_start > 1:
                line_search_func_values.append(neg_ll)

                # íŒŒë¼ë¯¸í„° ë³€í™”ëŸ‰ê³¼ í•¨ìˆ˜ê°’ ë³€í™” ë¡œê¹…
                if line_search_start_params[0] is not None:
                    # âœ… ê°™ì€ íƒ€ì…ë¼ë¦¬ ë¹„êµ (params_scaled vs params_scaled)
                    param_diff = params_scaled - line_search_start_params[0]
                    param_change_norm = np.linalg.norm(param_diff)

                    f_start = line_search_start_func_value[0]
                    f_current = neg_ll
                    f_decrease = f_start - f_current

                    self.iteration_logger.info(
                        f"  íŒŒë¼ë¯¸í„° ë³€í™”ëŸ‰ (L2 norm): {param_change_norm:.6e}\n"
                        f"  í•¨ìˆ˜ê°’ ë³€í™”: {f_decrease:+.4f} ({'ê°ì†Œ' if f_decrease > 0 else 'ì¦ê°€'})"
                    )

                # Line searchê°€ maxlsì— ë„ë‹¬í–ˆëŠ”ì§€ ì²´í¬
                if line_search_call_count[0] >= 10:  # maxls = 10
                    self.iteration_logger.info(
                        f"\nâš ï¸  [Line Search ê²½ê³ ] maxls={10}ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.\n"
                        f"  ì‹œì‘ í•¨ìˆ˜ê°’: {line_search_start_func_value[0]:.4f}\n"
                        f"  í˜„ì¬ í•¨ìˆ˜ê°’: {neg_ll:.4f}\n"
                        f"  ë³€í™”ëŸ‰: {neg_ll - line_search_start_func_value[0]:.4f}\n"
                        f"  Line searchê°€ Wolfe ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” step sizeë¥¼ ì°¾ì§€ ëª»í–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                    )

            return neg_ll

        # Get parameter bounds (ì¸¡ì •ëª¨ë¸ ì œì™¸)
        self.iteration_logger.info("íŒŒë¼ë¯¸í„° bounds ê³„ì‚° ì‹œì‘...")
        bounds = self.param_manager.get_optimized_parameter_bounds(
            structural_model, choice_model
        )
        self.iteration_logger.info(f"íŒŒë¼ë¯¸í„° bounds ê³„ì‚° ì™„ë£Œ (ì´ {len(bounds)}ê°œ)")

        # ìµœì í™” ë°©ë²• ì„ íƒ
        use_gradient = self.config.estimation.optimizer in ['BFGS', 'L-BFGS-B', 'BHHH']

        # Gradient í•¨ìˆ˜ ì •ì˜ (Apollo ë°©ì‹)
        grad_call_count = [0]  # ê·¸ë˜ë””ì–¸íŠ¸ í˜¸ì¶œ íšŸìˆ˜

        def gradient_function(params_scaled):
            """
            Analytic gradient ê³„ì‚° (Apollo ë°©ì‹, ìŠ¤ì¼€ì¼ëœ íŒŒë¼ë¯¸í„° ì‚¬ìš©)

            Args:
                params_scaled: ìŠ¤ì¼€ì¼ëœ (internal) íŒŒë¼ë¯¸í„°
                              (ì¸¡ì •ëª¨ë¸ ê³ ì • ì‹œ êµ¬ì¡°ëª¨ë¸+ì„ íƒëª¨ë¸ë§Œ í¬í•¨)

            Returns:
                Gradient w.r.t. scaled parameters
            """
            if not self.use_analytic_gradient:
                return None  # ìˆ˜ì¹˜ì  ê·¸ë˜ë””ì–¸íŠ¸ ì‚¬ìš©

            grad_call_count[0] += 1

            # âœ… ParameterContextë¥¼ ì‚¬ìš©í•œ íŒŒë¼ë¯¸í„° ë³€í™˜
            # âœ… ë™ì‹œì¶”ì •: params_scaledëŠ” ì´ë¯¸ ìµœì í™” íŒŒë¼ë¯¸í„°ë§Œ í¬í•¨ (8ê°œ)
            params_opt = param_context.to_full_external(params_scaled)

            # Line search ì¤‘ì¸ì§€ íŒë‹¨ (gradient_functionì—ì„œë„ ê³„ì‚° í•„ìš”)
            calls_since_major_start = func_call_count[0] - current_major_iter_start_call[0]

            # ë‹¨ê³„ ë¡œê·¸: ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ì‹œì‘ (ëª¨ë“  í˜¸ì¶œì—ì„œ ì¶œë ¥)
            # Major iteration ë²ˆí˜¸ í¬í•¨
            context_str = f"iter{major_iter_count[0]}-{calls_since_major_start}" if major_iter_count[0] > 0 else "init"
            self.iteration_logger.info(f"[ë‹¨ê³„ 2/2] Analytic Gradient ê³„ì‚° #{context_str}")

            # ë©”ëª¨ë¦¬ ì²´í¬ (ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ì „) - ë¹„í™œì„±í™”
            # if hasattr(self, 'memory_monitor'):
            #     # 5íšŒë§ˆë‹¤ ë©”ëª¨ë¦¬ ìƒíƒœ ë¡œê¹…
            #     if grad_call_count[0] % 5 == 1:
            #         self.memory_monitor.log_memory_stats(f"Gradient ê³„ì‚° #{grad_call_count[0]}")
            #
            #     # í•­ìƒ ì„ê³„ê°’ ì²´í¬ ë° í•„ìš”ì‹œ ì •ë¦¬
            #     mem_info = self.memory_monitor.check_and_cleanup(f"Gradient ê³„ì‚° #{grad_call_count[0]}")

            # âœ… ë¦¬íŒ©í† ë§: ìˆœìˆ˜í•œ gradient ê³„ì‚°ì€ _compute_gradient ë©”ì„œë“œë¡œ ìœ„ì„
            # âœ… ë™ì‹œì¶”ì •: _compute_gradientëŠ” ì´ë¯¸ ìµœì í™” íŒŒë¼ë¯¸í„° ê·¸ë˜ë””ì–¸íŠ¸ë§Œ ë°˜í™˜ (ì¸¡ì •ëª¨ë¸ ì œì™¸)
            neg_grad_opt = self._compute_gradient(
                params_opt, measurement_model, structural_model, choice_model
            )

            # âœ… ParameterContextë¥¼ ì‚¬ìš©í•œ ê·¸ë˜ë””ì–¸íŠ¸ ìŠ¤ì¼€ì¼ë§
            neg_grad_scaled = param_context.scale_gradient(neg_grad_opt)

            # Line search ì¤‘ì¸ì§€ íŒë‹¨
            calls_since_major_start = func_call_count[0] - current_major_iter_start_call[0]

            # Gradient ë°©í–¥ ê²€ì¦ (ì²« ë²ˆì§¸ í˜¸ì¶œ ì‹œ)
            if grad_call_count[0] == 1:
                grad_norm_opt = np.linalg.norm(neg_grad_opt)
                grad_norm_scaled = np.linalg.norm(neg_grad_scaled)
                self.iteration_logger.info(
                    f"\n[Gradient ë°©í–¥ ê²€ì¦ - External (ì›ë³¸)]\n"
                    f"  Gradient norm: {grad_norm_opt:.6e}\n"
                    f"  Gradient max: {np.max(np.abs(neg_grad_opt)):.6e}\n"
                    f"  Gradient (ì²˜ìŒ 5ê°œ): {neg_grad_opt[:5]}\n"
                    f"  Gradient (ë§ˆì§€ë§‰ 5ê°œ): {neg_grad_opt[-5:]}\n"
                )
                self.iteration_logger.info(
                    f"\n[Gradient ë°©í–¥ ê²€ì¦ - Internal (ìŠ¤ì¼€ì¼ë¨)]\n"
                    f"  Gradient norm: {grad_norm_scaled:.6e}\n"
                    f"  Gradient max: {np.max(np.abs(neg_grad_scaled)):.6e}\n"
                    f"  Gradient (ì²˜ìŒ 5ê°œ): {neg_grad_scaled[:5]}\n"
                    f"  Gradient (ë§ˆì§€ë§‰ 5ê°œ): {neg_grad_scaled[-5:]}\n"
                    f"  ì£¼ì˜: scipyëŠ” ì´ gradientë¥¼ ì‚¬ìš©í•˜ì—¬ descent directionì„ ê³„ì‚°í•©ë‹ˆë‹¤.\n"
                    f"       d = -H^(-1) Â· gradientì´ë¯€ë¡œ, gradientê°€ ì–‘ìˆ˜ë©´ dëŠ” ìŒìˆ˜ ë°©í–¥ì…ë‹ˆë‹¤."
                )

                # ìŠ¤ì¼€ì¼ë§ ë¹„êµ ë¡œê¹…
                self.param_scaler.log_gradient_comparison(neg_grad_opt, neg_grad_scaled)

            # Line search ì‹œì‘ ì‹œ ë°©í–¥ ë¯¸ë¶„ ì €ì¥
            if calls_since_major_start == 1:
                # Major iteration ì‹œì‘ ì‹œ gradient ì €ì¥ (ìŠ¤ì¼€ì¼ëœ gradient ì‚¬ìš©)
                line_search_gradient[0] = neg_grad_scaled.copy()
                # ë‹¤ìŒ í•¨ìˆ˜ í˜¸ì¶œì—ì„œ íƒìƒ‰ ë°©í–¥ì„ ì•Œ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ë°©í–¥ ë¯¸ë¶„ì€ ë‚˜ì¤‘ì— ê³„ì‚°

                # âœ… Major iteration ì‹œì‘ ì‹œ íŒŒë¼ë¯¸í„° ì €ì¥ (íƒìƒ‰ ë°©í–¥ ê³„ì‚°ìš©)
                if not hasattr(gradient_function, 'major_iter_start_params'):
                    gradient_function.major_iter_start_params = {}
                gradient_function.major_iter_start_params[major_iter_count[0] + 1] = params_scaled.copy()

            # Line search ì¤‘ì´ë©´ Wolfe ì¡°ê±´ ê³„ì‚°
            elif calls_since_major_start > 1 and line_search_start_params[0] is not None:
                # íƒìƒ‰ ë°©í–¥ ê³„ì‚°: d = params_scaled - line_search_start_params
                search_direction = params_scaled - line_search_start_params[0]

                # âœ… ì²« line search í˜¸ì¶œ ì‹œ íƒìƒ‰ ë°©í–¥ ë¡œê¹…
                if line_search_call_count[0] == 1:
                    iter_num = major_iter_count[0] + 1

                    # íƒìƒ‰ ë°©í–¥ í†µê³„
                    d_norm = np.linalg.norm(search_direction)
                    d_max = np.max(np.abs(search_direction))

                    # Gradientì™€ íƒìƒ‰ ë°©í–¥ ë¹„êµ
                    grad_norm = np.linalg.norm(neg_grad_scaled)

                    # d = -H^(-1) Â· gradì´ë¯€ë¡œ, ë§Œì•½ H = Iì´ë©´ d â‰ˆ -grad
                    # ìƒê´€ê³„ìˆ˜ ê³„ì‚° (ë°©í–¥ ìœ ì‚¬ë„)
                    if grad_norm > 0 and d_norm > 0:
                        # ì •ê·œí™”ëœ ë²¡í„° ê°„ ë‚´ì  = ì½”ì‚¬ì¸ ìœ ì‚¬ë„
                        cosine_similarity = -np.dot(search_direction, neg_grad_scaled) / (d_norm * grad_norm)
                    else:
                        cosine_similarity = 0.0

                    # âœ… ì´ì „ iteration íŒŒë¼ë¯¸í„°ì™€ ë¹„êµ
                    if hasattr(gradient_function, 'major_iter_start_params') and (iter_num - 1) in gradient_function.major_iter_start_params:
                        prev_params = gradient_function.major_iter_start_params[iter_num - 1]
                        param_change = params_scaled - prev_params
                        param_change_norm = np.linalg.norm(param_change)

                        # ì‹¤ì œ íŒŒë¼ë¯¸í„° ë³€í™” = Î± Ã— d (ì´ì „ iterationì—ì„œ)
                        # í˜„ì¬ëŠ” ìƒˆ iterationì˜ ì²« line searchì´ë¯€ë¡œ, ì´ì „ iterationì˜ ìµœì¢… ê²°ê³¼
                        param_change_info = (
                            f"\n  ì´ì „ iteration ëŒ€ë¹„ íŒŒë¼ë¯¸í„° ë³€í™”:\n"
                            f"    - ë³€í™”ëŸ‰ norm: {param_change_norm:.6e}\n"
                            f"    - ë³€í™”ëŸ‰ max: {np.max(np.abs(param_change)):.6e}\n"
                            f"    - ë³€í™” ìƒìœ„ 5ê°œ ì¸ë±ìŠ¤: {np.argsort(np.abs(param_change))[-5:][::-1]}\n"
                            f"    - ë³€í™” ìƒìœ„ 5ê°œ ê°’: {param_change[np.argsort(np.abs(param_change))[-5:][::-1]]}\n"
                        )
                    else:
                        param_change_info = ""

                    self.iteration_logger.info(
                        f"\n[íƒìƒ‰ ë°©í–¥ ë¶„ì„ - Iteration #{iter_num}]\n"
                        f"  íƒìƒ‰ ë°©í–¥ d norm: {d_norm:.6e}\n"
                        f"  íƒìƒ‰ ë°©í–¥ d max: {d_max:.6e}\n"
                        f"  Gradient norm: {grad_norm:.6e}\n"
                        f"  dì™€ -gradì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„: {cosine_similarity:.6f}\n"
                        f"    (1.0 = ì™„ì „ ë™ì¼ ë°©í–¥ [H=I], 0.0 = ì§êµ, -1.0 = ë°˜ëŒ€ ë°©í–¥)\n"
                        f"  d ìƒìœ„ 5ê°œ: {search_direction[:5]}\n"
                        f"  -grad ìƒìœ„ 5ê°œ: {-neg_grad_scaled[:5]}\n"
                        f"  â†’ Hessianì´ ë°©í–¥ì„ {'ê±°ì˜ ì¡°ì • ì•ˆ í•¨' if cosine_similarity > 0.99 else 'ì¡°ì •í•¨'}\n"
                        f"{param_change_info}"
                    )

                    # âœ… ì‹¤ì œ ê³„ì‚°ì— ì‚¬ìš©ëœ íŒŒë¼ë¯¸í„° ê°’ ë¡œê¹… ë¹„í™œì„±í™” (ìš”ì²­ì‚¬í•­ 4)
                    # params_external = self.param_scaler.unscale_parameters(params_scaled)
                    # top_10_indices = np.argsort(np.abs(params_external))[-10:][::-1]
                    # self.iteration_logger.info(
                    #     f"\n[ì‹¤ì œ ê³„ì‚°ì— ì‚¬ìš©ëœ íŒŒë¼ë¯¸í„° ê°’ - Iteration #{iter_num}]\n"
                    #     f"  (External scale, ìƒìœ„ 10ê°œ)\n"
                    # )
                    # for idx in top_10_indices:
                    #     param_name = self.param_names[idx] if hasattr(self, 'param_names') and idx < len(self.param_names) else f"param_{idx}"
                    #     self.iteration_logger.info(
                    #         f"    [{idx:2d}] {param_name:40s}: {params_external[idx]:+.6e} (internal: {params_scaled[idx]:+.6e})"
                    #     )
                    pass

                # í˜„ì¬ ìœ„ì¹˜ì—ì„œ ë°©í–¥ ë¯¸ë¶„: âˆ‡f(x + Î±Â·d)^TÂ·d (ìŠ¤ì¼€ì¼ëœ gradient ì‚¬ìš©)
                directional_derivative_new = np.dot(neg_grad_scaled, search_direction)

                # Line search ì‹œì‘ ì‹œ ë°©í–¥ ë¯¸ë¶„ ê³„ì‚° (ì²« line search í˜¸ì¶œ ì‹œ)
                if line_search_directional_derivative[0] is None and line_search_gradient[0] is not None:
                    # ì‹œì‘ ìœ„ì¹˜ì—ì„œ ë°©í–¥ ë¯¸ë¶„: âˆ‡f(x)^TÂ·d
                    line_search_directional_derivative[0] = np.dot(line_search_gradient[0], search_direction)

                # Wolfe ì¡°ê±´ ì²´í¬
                if line_search_directional_derivative[0] is not None:
                    dd_start = line_search_directional_derivative[0]
                    dd_new = directional_derivative_new

                    # Armijo ì¡°ê±´: f(x+Î±d) â‰¤ f(x) + câ‚Â·Î±Â·âˆ‡f(x)áµ€d
                    # ì—¬ê¸°ì„œ Î± = ||search_direction|| / ||d||ì¸ë°, dë¥¼ ëª¨ë¥´ë¯€ë¡œ
                    # ëŒ€ì‹  ì´ì „ í•¨ìˆ˜ í˜¸ì¶œì˜ í•¨ìˆ˜ê°’ì„ ì‚¬ìš©
                    c1 = 1e-3  # ì¡°ì •ëœ ê°’ (ê¸°ë³¸ê°’: 1e-4)

                    # ì´ì „ line search í˜¸ì¶œì˜ í•¨ìˆ˜ê°’ ê°€ì ¸ì˜¤ê¸°
                    if len(line_search_func_values) > 0:
                        f_start = line_search_start_func_value[0]
                        f_current = line_search_func_values[-1]  # ê°€ì¥ ìµœê·¼ í•¨ìˆ˜ê°’

                        # Armijo ì¡°ê±´ ê·¼ì‚¬ ì²´í¬
                        # f(x+Î±d) - f(x) â‰¤ câ‚Â·Î±Â·âˆ‡f(x)áµ€d
                        # Î±Â·âˆ‡f(x)áµ€dë¥¼ ì •í™•íˆ ëª¨ë¥´ë¯€ë¡œ, ë‹¨ìˆœíˆ í•¨ìˆ˜ê°’ ê°ì†Œ ì²´í¬
                        armijo_satisfied = (f_current <= f_start)
                    else:
                        armijo_satisfied = None

                    # Curvature ì¡°ê±´: |âˆ‡f(x + Î±Â·d)^TÂ·d| â‰¤ c2Â·|âˆ‡f(x)^TÂ·d|
                    c2 = 0.5  # ì¡°ì •ëœ ê°’ (ê¸°ë³¸ê°’: 0.9)
                    curvature_lhs = abs(dd_new)
                    curvature_rhs = c2 * abs(dd_start)
                    curvature_satisfied = curvature_lhs <= curvature_rhs

                    # Strong Wolfe ì¡°ê±´ = Armijo + Curvature
                    strong_wolfe_satisfied = (armijo_satisfied and curvature_satisfied) if armijo_satisfied is not None else curvature_satisfied

                    # Wolfe ì¡°ê±´ ì²´í¬ ë¡œê¹… ë¹„í™œì„±í™” (ìš”ì²­ì‚¬í•­ 1)
                    # armijo_msg = ""
                    # if armijo_satisfied is not None:
                    #     armijo_msg = f"  Armijo ì¡°ê±´ (c1={c1}): {'âœ“ ë§Œì¡±' if armijo_satisfied else 'âŒ ë¶ˆë§Œì¡±'}\n"
                    # self.iteration_logger.info(
                    #     f"\n[Wolfe ì¡°ê±´ ì²´í¬]\n"
                    #     f"{armijo_msg}"
                    #     f"  Curvature ì¡°ê±´ (c2={c2}): {'âœ“ ë§Œì¡±' if curvature_satisfied else 'âŒ ë¶ˆë§Œì¡±'}\n"
                    #     f"  â†’ Strong Wolfe: {'âœ“ ë§Œì¡±' if strong_wolfe_satisfied else 'âŒ ë¶ˆë§Œì¡±'}\n"
                    #     f"  â†’ Gradientê°€ {'ì¶©ë¶„íˆ í‰í‰í•´ì§' if curvature_satisfied else 'ì•„ì§ ê°€íŒŒë¦„'}"
                    # )

            # ìŠ¤ì¼€ì¼ëœ gradient ë°˜í™˜ (optimizerëŠ” internal parametersì— ëŒ€í•´ ì‘ë™)
            return neg_grad_scaled

        print("=" * 70, flush=True)
        if use_gradient:
            print(f"ìµœì í™” ì‹œì‘: {self.config.estimation.optimizer} (gradient-based)", flush=True)
            if self.use_analytic_gradient:
                print("Analytic gradient ì‚¬ìš© (Apollo ë°©ì‹ + Parameter Scaling)", flush=True)
            else:
                print("ìˆ˜ì¹˜ì  ê·¸ë˜ë””ì–¸íŠ¸ ì‚¬ìš© (2-point finite difference)", flush=True)
        else:
            print("ìµœì í™” ì‹œì‘: Nelder-Mead (gradient-free)", flush=True)
        print(f"ì´ˆê¸° íŒŒë¼ë¯¸í„° ê°œìˆ˜: {len(initial_params_scaled)}", flush=True)
        self.iteration_logger.info(f"ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜: {self.config.estimation.max_iterations}")
        self.iteration_logger.info("=" * 70)

        # ë³‘ë ¬ì²˜ë¦¬ ì„¤ì • ë¡œê¹…
        use_parallel = getattr(self.config.estimation, 'use_parallel', False)
        if use_parallel:
            n_cores = getattr(self.config.estimation, 'n_cores', None)
            if n_cores is None:
                n_cores = max(1, multiprocessing.cpu_count() - 1)
            self.iteration_logger.info(f"ë³‘ë ¬ì²˜ë¦¬ í™œì„±í™”: {n_cores} ì½”ì–´ ì‚¬ìš©")
        else:
            self.iteration_logger.info("ìˆœì°¨ì²˜ë¦¬ ì‚¬ìš©")

        # ì¡°ê¸° ì¢…ë£Œë¥¼ ìœ„í•œ Wrapper í´ë˜ìŠ¤ (BFGS ì •ìƒ ì¢…ë£Œ í™œìš©)
        class EarlyStoppingWrapper:
            """
            ëª©ì  í•¨ìˆ˜ì™€ gradient í•¨ìˆ˜ë¥¼ ê°ì‹¸ì„œ ì¡°ê¸° ì¢…ë£Œ êµ¬í˜„
            StopIteration ì˜ˆì™¸ ëŒ€ì‹  ë§¤ìš° í° ê°’ì„ ë°˜í™˜í•˜ì—¬ BFGSê°€ ì •ìƒ ì¢…ë£Œí•˜ë„ë¡ ìœ ë„
            â†’ BFGSê°€ ì •ìƒ ì¢…ë£Œí•˜ë©´ result.hess_inv ìë™ ì œê³µ (ì¶”ê°€ ê³„ì‚° 0íšŒ!)
            """

            def __init__(self, func, grad_func, patience=5, tol=1e-6, logger=None, iteration_logger=None, param_scaler=None, param_names=None, parent_estimator=None):
                """
                Args:
                    func: ëª©ì  í•¨ìˆ˜ (negative log-likelihood)
                    grad_func: Gradient í•¨ìˆ˜
                    patience: í•¨ìˆ˜ í˜¸ì¶œ ê¸°ì¤€ ê°œì„  ì—†ëŠ” íšŸìˆ˜ (ê¸°ë³¸ê°’: 5)
                    tol: LL ë³€í™” í—ˆìš© ì˜¤ì°¨ (ì ˆëŒ€ê°’)
                    logger: ë©”ì¸ ë¡œê±°
                    iteration_logger: ë°˜ë³µ ë¡œê±°
                    param_scaler: íŒŒë¼ë¯¸í„° ìŠ¤ì¼€ì¼ëŸ¬ (ì™¸ë¶€ í´ë˜ìŠ¤ì—ì„œ ì „ë‹¬)
                    param_names: íŒŒë¼ë¯¸í„° ì´ë¦„ ë¦¬ìŠ¤íŠ¸ (ì™¸ë¶€ í´ë˜ìŠ¤ì—ì„œ ì „ë‹¬)
                    parent_estimator: ë¶€ëª¨ estimator ì¸ìŠ¤í„´ìŠ¤ (ì™¸ë¶€ í´ë˜ìŠ¤ì—ì„œ ì „ë‹¬)
                """
                self.func = func
                self.grad_func = grad_func
                self.patience = patience
                self.tol = tol
                self.logger = logger
                self.iteration_logger = iteration_logger
                self.param_scaler = param_scaler  # âœ… ì™¸ë¶€ì—ì„œ ì „ë‹¬ë°›ì€ param_scaler ì €ì¥
                self.param_names = param_names    # âœ… ì™¸ë¶€ì—ì„œ ì „ë‹¬ë°›ì€ param_names ì €ì¥
                self.parent_estimator = parent_estimator  # âœ… ë¶€ëª¨ estimator ì €ì¥

                self.best_ll = np.inf
                self.best_x = None  # ìµœì  íŒŒë¼ë¯¸í„° ì €ì¥
                self.no_improvement_count = 0
                self.func_call_count = 0
                self.grad_call_count = 0
                self.early_stopped = False
                self.bfgs_iteration_count = 0  # BFGS iteration ì¹´ìš´í„°

            def objective(self, x):
                """
                ëª©ì  í•¨ìˆ˜ wrapper - ì¡°ê¸° ì¢…ë£Œ ì‹œ ë§¤ìš° í° ê°’ ë°˜í™˜
                """
                # ì´ë¯¸ ì¡°ê¸° ì¢…ë£Œëœ ê²½ìš°: ë§¤ìš° í° ê°’ ë°˜í™˜í•˜ì—¬ BFGSê°€ ì¢…ë£Œí•˜ë„ë¡ ìœ ë„
                if self.early_stopped:
                    return 1e10

                self.func_call_count += 1
                current_ll = self.func(x)

                # LL ê°œì„  ì²´í¬
                if current_ll < self.best_ll - self.tol:
                    # ëª…í™•í•œ ê°œì„ 
                    self.best_ll = current_ll
                    self.best_x = x.copy()  # ìµœì  íŒŒë¼ë¯¸í„° ì €ì¥
                    self.no_improvement_count = 0
                else:
                    # ê°œì„  ì—†ìŒ
                    self.no_improvement_count += 1

                # ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´ ì²´í¬
                if self.no_improvement_count >= self.patience:
                    self.early_stopped = True
                    msg = f"ì¡°ê¸° ì¢…ë£Œ: {self.patience}íšŒ ì—°ì† í•¨ìˆ˜ í˜¸ì¶œì—ì„œ LL ê°œì„  ì—†ìŒ (Best LL={self.best_ll:.4f})"
                    if self.iteration_logger:
                        self.iteration_logger.info(msg)
                    # StopIteration ëŒ€ì‹  ë§¤ìš° í° ê°’ ë°˜í™˜
                    return 1e10

                return current_ll

            def gradient(self, x):
                """
                Gradient í•¨ìˆ˜ wrapper - ì¡°ê¸° ì¢…ë£Œ ì‹œ 0 ë²¡í„° ë°˜í™˜
                """
                # ì´ë¯¸ ì¡°ê¸° ì¢…ë£Œëœ ê²½ìš°: 0 ë²¡í„° ë°˜í™˜í•˜ì—¬ BFGSê°€ ì¢…ë£Œí•˜ë„ë¡ ìœ ë„
                if self.early_stopped:
                    return np.zeros_like(x)

                self.grad_call_count += 1
                return self.grad_func(x)

            def callback(self, xk):
                """
                BFGS callback - ë§¤ Major iterationë§ˆë‹¤ í˜¸ì¶œë¨
                ì¡°ê¸° ì¢…ë£Œ ì‹œ ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ë³µì›
                ftol AND gtol ì¡°ê±´ì„ ëª¨ë‘ ì²´í¬í•˜ì—¬ ì¡°ê¸° ì¢…ë£Œ
                """
                self.bfgs_iteration_count += 1
                major_iter_count[0] = self.bfgs_iteration_count

                # âœ… Hessian ì¶”ì ì„ ìœ„í•œ ë³€ìˆ˜ ì €ì¥
                if not hasattr(self, 'prev_xk'):
                    self.prev_xk = None
                if not hasattr(self, 'prev_grad'):
                    self.prev_grad = None

                # âœ… ftol AND gtol ì¡°ê±´ ì²´í¬ë¥¼ ìœ„í•œ ë³€ìˆ˜
                if not hasattr(self, 'ftol_threshold'):
                    self.ftol_threshold = 1e-6  # ftol ê¸°ì¤€
                if not hasattr(self, 'gtol_threshold'):
                    self.gtol_threshold = 1e-5  # gtol ê¸°ì¤€

                # Major iteration ì™„ë£Œ ë¡œê¹…
                if self.iteration_logger:
                    # í˜„ì¬ í•¨ìˆ˜ê°’ ê³„ì‚°
                    current_f = self.func(xk)
                    current_ll = -current_f

                    # Line search í†µê³„
                    line_search_calls = line_search_call_count[0]

                    # Line search ì„±ê³µ ì—¬ë¶€ íŒë‹¨
                    if line_search_start_func_value[0] is not None:
                        f_start = line_search_start_func_value[0]
                        f_final = current_f
                        f_decrease = f_start - f_final

                        if f_decrease > 0:
                            ls_status = f"[OK] ì„±ê³µ (í•¨ìˆ˜ê°’ ê°ì†Œ: {f_decrease:.4f})"
                        elif f_decrease == 0:
                            ls_status = f"[WARN] ì •ì²´ (í•¨ìˆ˜ê°’ ë³€í™” ì—†ìŒ)"
                        else:
                            ls_status = f"[FAIL] ì‹¤íŒ¨ (í•¨ìˆ˜ê°’ ì¦ê°€: {-f_decrease:.4f})"
                    else:
                        ls_status = "N/A (ì²« iteration)"

                    # ftol ê³„ì‚° (ì´ì „ major iterationê³¼ ë¹„êµ)
                    if last_major_iter_func_value[0] is not None:
                        f_prev = last_major_iter_func_value[0]
                        f_curr = current_f
                        rel_change = abs(f_prev - f_curr) / max(abs(f_prev), abs(f_curr), 1.0)

                        # ì´ì „ ftol ëŒ€ë¹„ ë³€í™”ëŸ‰ ê³„ì‚°
                        if last_major_iter_ftol[0] is not None:
                            ftol_change = rel_change - last_major_iter_ftol[0]
                            ftol_change_pct = (ftol_change / last_major_iter_ftol[0]) * 100 if last_major_iter_ftol[0] != 0 else 0
                            ftol_status = f"ftol = {rel_change:.6e} (ê¸°ì¤€: 1e-3, ë³€í™”: {ftol_change:+.2e} [{ftol_change_pct:+.1f}%])"
                        else:
                            ftol_status = f"ftol = {rel_change:.6e} (ê¸°ì¤€: 1e-3)"

                        if rel_change <= 1e-3:
                            ftol_status += " [OK] ìˆ˜ë ´ ì¡°ê±´ ë§Œì¡±"

                        last_major_iter_ftol[0] = rel_change
                    else:
                        ftol_status = "ftol = N/A (ì²« iteration)"

                    # Gradient norm ê³„ì‚°
                    if self.grad_func:
                        grad = self.grad_func(xk)
                        grad_norm = np.linalg.norm(grad, ord=np.inf)

                        # âœ… ê³ ì •ë˜ì§€ ì•Šì€ íŒŒë¼ë¯¸í„°ì˜ ê·¸ë˜ë””ì–¸íŠ¸ë§Œ ê³„ì‚° (L-BFGS-Bì˜ projected gradientì™€ ìœ ì‚¬)
                        non_zero_grad = grad[np.abs(grad) > 1e-10]
                        if len(non_zero_grad) > 0:
                            grad_norm_active = np.linalg.norm(non_zero_grad, ord=np.inf)
                            n_active = len(non_zero_grad)
                        else:
                            grad_norm_active = 0.0
                            n_active = 0

                        # ì´ì „ gtol ëŒ€ë¹„ ë³€í™”ëŸ‰ ê³„ì‚°
                        if last_major_iter_gtol[0] is not None:
                            gtol_change = grad_norm - last_major_iter_gtol[0]
                            gtol_change_pct = (gtol_change / last_major_iter_gtol[0]) * 100 if last_major_iter_gtol[0] != 0 else 0
                            gtol_status = f"gtol = {grad_norm:.6e} (ì „ì²´), {grad_norm_active:.6e} (í™œì„± {n_active}ê°œ) (ê¸°ì¤€: 1e-3, ë³€í™”: {gtol_change:+.2e} [{gtol_change_pct:+.1f}%])"
                        else:
                            gtol_status = f"gtol = {grad_norm:.6e} (ì „ì²´), {grad_norm_active:.6e} (í™œì„± {n_active}ê°œ) (ê¸°ì¤€: 1e-3)"

                        if grad_norm_active <= 1e-3:
                            gtol_status += " [OK] í™œì„± íŒŒë¼ë¯¸í„° ìˆ˜ë ´"

                        last_major_iter_gtol[0] = grad_norm

                        # âœ… ì „ì²´ íŒŒë¼ë¯¸í„° ê°’ê³¼ ê·¸ë˜ë””ì–¸íŠ¸ ê°’ ë¡œê¹… (ìš”ì²­ì‚¬í•­ 3)
                        # ìƒìœ„ 10ê°œ ëŒ€ì‹  ì „ì²´ íŒŒë¼ë¯¸í„° ì¶œë ¥
                        params_external = self.param_scaler.unscale_parameters(xk)

                        gradient_details = "\n  ì „ì²´ íŒŒë¼ë¯¸í„° ê°’ ë° ê·¸ë˜ë””ì–¸íŠ¸:\n"
                        for idx in range(len(params_external)):
                            param_name = self.param_names[idx] if hasattr(self, 'param_names') and idx < len(self.param_names) else f"param_{idx}"
                            gradient_details += f"    [{idx:2d}] {param_name:50s}: param={params_external[idx]:+12.6e}, grad={grad[idx]:+12.6e}\n"

                        # CSV íŒŒì¼ì— ê¸°ë¡ (ìš”ì²­ì‚¬í•­ 5)
                        if self.parent_estimator is not None and hasattr(self.parent_estimator, '_log_params_grads_to_csv'):
                            self.parent_estimator._log_params_grads_to_csv(major_iter_count[0], params_external, grad)
                    else:
                        gtol_status = "gtol = N/A"
                        gradient_details = ""

                    # âœ… Hessian ì—…ë°ì´íŠ¸ ì •ë³´ ë¡œê¹…
                    hessian_update_info = ""
                    if self.prev_xk is not None and self.prev_grad is not None:
                        # s_k = x_k - x_{k-1}
                        s_k = xk - self.prev_xk
                        # y_k = grad_k - grad_{k-1}
                        current_grad = self.grad_func(xk)
                        y_k = current_grad - self.prev_grad

                        # s_k, y_k í†µê³„
                        s_norm = np.linalg.norm(s_k)
                        y_norm = np.linalg.norm(y_k)
                        s_y_dot = np.dot(s_k, y_k)

                        # BFGS ì—…ë°ì´íŠ¸ ì¡°ê±´ ì²´í¬
                        if s_y_dot > 0:
                            rho = 1.0 / s_y_dot
                            hessian_update_info = (
                                f"\n  Hessian ì—…ë°ì´íŠ¸ ì •ë³´:\n"
                                f"    - s_k (íŒŒë¼ë¯¸í„° ë³€í™”) norm: {s_norm:.6e}\n"
                                f"    - y_k (gradient ë³€í™”) norm: {y_norm:.6e}\n"
                                f"    - s_k^T Â· y_k: {s_y_dot:.6e} (ì–‘ìˆ˜ OK)\n"
                                f"    - Ï = 1/(s_k^T Â· y_k): {rho:.6e}\n"
                                f"    - s_k ìƒìœ„ 5ê°œ: {s_k[:5]}\n"
                                f"    - y_k ìƒìœ„ 5ê°œ: {y_k[:5]}\n"
                            )
                        else:
                            hessian_update_info = (
                                f"\n  âš ï¸  Hessian ì—…ë°ì´íŠ¸ ê²½ê³ :\n"
                                f"    - s_k^T Â· y_k: {s_y_dot:.6e} (ìŒìˆ˜ ë˜ëŠ” 0 âŒ)\n"
                                f"    - BFGS ì—…ë°ì´íŠ¸ê°€ ê±´ë„ˆë›°ì–´ì§ˆ ìˆ˜ ìˆìŒ!\n"
                            )
                    else:
                        hessian_update_info = "\n  Hessian ì—…ë°ì´íŠ¸: ì²« iteration (ì´ˆê¸° H = I)\n"

                    # í˜„ì¬ ìƒíƒœ ì €ì¥ (ë‹¤ìŒ iterationì„ ìœ„í•´)
                    self.prev_xk = xk.copy()
                    self.prev_grad = self.grad_func(xk).copy()

                    self.iteration_logger.info(
                        f"\n{'='*80}\n"
                        f"[Major Iteration #{self.bfgs_iteration_count} ì™„ë£Œ]\n"
                        f"  ìµœì¢… LL: {current_ll:.4f}\n"
                        f"  Line Search: {line_search_calls}íšŒ í•¨ìˆ˜ í˜¸ì¶œ - {ls_status}\n"
                        f"  í•¨ìˆ˜ í˜¸ì¶œ: {self.func_call_count}íšŒ (ëˆ„ì )\n"
                        f"  ê·¸ë˜ë””ì–¸íŠ¸ í˜¸ì¶œ: {self.grad_call_count}íšŒ (ëˆ„ì )\n"
                        f"  ìˆ˜ë ´ ì¡°ê±´:\n"
                        f"    - {ftol_status}\n"
                        f"    - {gtol_status}\n"
                        f"{gradient_details}"
                        f"{hessian_update_info}"
                        f"  Hessian ê·¼ì‚¬: BFGS ê³µì‹ìœ¼ë¡œ ì—…ë°ì´íŠ¸ ì™„ë£Œ\n"
                        f"{'='*80}"
                    )

                    # âœ… ftol AND gtol ì¡°ê±´ ì²´í¬ (ë‘˜ ë‹¤ ë§Œì¡±í•´ì•¼ ì¡°ê¸° ì¢…ë£Œ)
                    ftol_satisfied = False
                    gtol_satisfied = False

                    if last_major_iter_func_value[0] is not None:
                        f_prev = last_major_iter_func_value[0]
                        f_curr = current_f
                        rel_change = abs(f_prev - f_curr) / max(abs(f_prev), abs(f_curr), 1.0)
                        ftol_satisfied = (rel_change <= self.ftol_threshold)

                    if self.grad_func:
                        grad = self.grad_func(xk)
                        grad_norm_active = np.linalg.norm(grad[np.abs(grad) > 1e-10], ord=np.inf) if np.any(np.abs(grad) > 1e-10) else 0.0
                        gtol_satisfied = (grad_norm_active <= self.gtol_threshold)

                    # ftol AND gtol ëª¨ë‘ ë§Œì¡±í•˜ë©´ ì¡°ê¸° ì¢…ë£Œ
                    if ftol_satisfied and gtol_satisfied:
                        self.early_stopped = True
                        self.best_x = xk.copy()
                        msg = (
                            f"\n{'='*80}\n"
                            f"âœ… ìˆ˜ë ´ ì™„ë£Œ: ftol AND gtol ì¡°ê±´ ëª¨ë‘ ë§Œì¡±\n"
                            f"  - ftol: {rel_change:.6e} <= {self.ftol_threshold:.6e} âœ“\n"
                            f"  - gtol: {grad_norm_active:.6e} <= {self.gtol_threshold:.6e} âœ“\n"
                            f"  - Major iteration: {self.bfgs_iteration_count}\n"
                            f"  - ìµœì¢… LL: {current_ll:.4f}\n"
                            f"{'='*80}"
                        )
                        if self.iteration_logger:
                            self.iteration_logger.info(msg)
                        # StopIteration ëŒ€ì‹  early_stopped í”Œë˜ê·¸ ì„¤ì •
                        # ë‹¤ìŒ objective/gradient í˜¸ì¶œ ì‹œ í° ê°’/0 ë²¡í„° ë°˜í™˜í•˜ì—¬ ì¢…ë£Œ ìœ ë„

                    # ë‹¤ìŒ major iterationì„ ìœ„í•œ ì¤€ë¹„
                    last_major_iter_func_value[0] = current_f
                    current_major_iter_start_call[0] = func_call_count[0]
                    line_search_call_count[0] = 0  # Line search ì¹´ìš´í„° ë¦¬ì…‹
                    line_search_func_values.clear()
                    line_search_directional_derivative[0] = None  # ë°©í–¥ ë¯¸ë¶„ ë¦¬ì…‹

                if self.early_stopped and self.best_x is not None:
                    # ì¡°ê¸° ì¢…ë£Œ í›„ì—ëŠ” ìµœì  íŒŒë¼ë¯¸í„°ë¥¼ ìœ ì§€
                    xk[:] = self.best_x

        if use_gradient:
            self.iteration_logger.info(f"ìµœì í™” ì‹œì‘: {self.config.estimation.optimizer} (gradient-based)")
            if self.use_analytic_gradient:
                self.iteration_logger.info("Analytic gradient ì‚¬ìš© (Apollo ë°©ì‹)")
            else:
                self.iteration_logger.info("ìˆ˜ì¹˜ì  ê·¸ë˜ë””ì–¸íŠ¸ ì‚¬ìš© (2-point finite difference)")

            # ì¡°ê¸° ì¢…ë£Œ ì„¤ì • í™•ì¸
            use_early_stopping = getattr(self.config.estimation, 'early_stopping', False)
            early_stopping_patience = getattr(self.config.estimation, 'early_stopping_patience', 5)
            early_stopping_tol = getattr(self.config.estimation, 'early_stopping_tol', 1e-6)

            # ì¡°ê¸° ì¢…ë£Œ Wrapper ìƒì„±
            early_stopping_wrapper = EarlyStoppingWrapper(
                func=negative_log_likelihood,
                grad_func=gradient_function if self.use_analytic_gradient else None,
                patience=early_stopping_patience if use_early_stopping else 999999,  # ë¹„í™œì„±í™” ì‹œ ë§¤ìš° í° ê°’
                tol=early_stopping_tol,
                logger=self.logger,
                iteration_logger=self.iteration_logger,
                param_scaler=self.param_scaler,  # âœ… param_scaler ì „ë‹¬
                param_names=self.param_names,    # âœ… param_names ì „ë‹¬
                parent_estimator=self            # âœ… parent_estimator ì „ë‹¬
            )

            # ì´ˆê¸° í•¨ìˆ˜ í˜¸ì¶œ ì‹œì‘ ìœ„ì¹˜ ì„¤ì •
            current_major_iter_start_call[0] = func_call_count[0]

            if use_early_stopping:
                self.iteration_logger.info(f"ì¡°ê¸° ì¢…ë£Œ í™œì„±í™”: {early_stopping_patience}íšŒ ì—°ì† í•¨ìˆ˜ í˜¸ì¶œì—ì„œ LL ê°œì„  ì—†ìœ¼ë©´ ì¢…ë£Œ (tol={early_stopping_tol})")
            else:
                self.iteration_logger.info("ì¡°ê¸° ì¢…ë£Œ ë¹„í™œì„±í™” (ì •ìƒ ì¢…ë£Œë§Œ ì‚¬ìš©)")

            # BFGS ë˜ëŠ” L-BFGS-B (ì •ìƒ ì¢…ë£Œë¡œ ì²˜ë¦¬)
            # ìˆ˜ì¹˜ì  ê·¸ë˜ë””ì–¸íŠ¸ í•¨ìˆ˜ (epsilon ì œì–´)
            if not self.use_analytic_gradient:
                from scipy.optimize import approx_fprime

                # ê·¸ë˜ë””ì–¸íŠ¸ í˜¸ì¶œ ì¹´ìš´í„°
                grad_call_count = [0]

                def numerical_gradient(x):
                    grad_call_count[0] += 1
                    grad = approx_fprime(x, early_stopping_wrapper.objective, epsilon=1e-4)

                    # ì²˜ìŒ 5ë²ˆë§Œ ë¡œê¹…
                    if grad_call_count[0] <= 5:
                        self.iteration_logger.info(f"[ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° #{grad_call_count[0]}]")
                        self.iteration_logger.info(f"  íŒŒë¼ë¯¸í„° (ì²˜ìŒ 10ê°œ): {x[:10]}")
                        self.iteration_logger.info(f"  ê·¸ë˜ë””ì–¸íŠ¸ (ì²˜ìŒ 10ê°œ): {grad[:10]}")
                        self.iteration_logger.info(f"  ê·¸ë˜ë””ì–¸íŠ¸ norm: {np.linalg.norm(grad):.6f}")
                        self.iteration_logger.info(f"  ê·¸ë˜ë””ì–¸íŠ¸ max: {np.max(np.abs(grad)):.6f}")

                    return grad

                jac_function = numerical_gradient
            else:
                jac_function = early_stopping_wrapper.gradient

            # Optimizerë³„ ì˜µì…˜ ì„¤ì •
            if self.config.estimation.optimizer == 'BHHH':
                # BHHH: Newton-CG with custom Hessian (OPG)
                optimizer_options = {
                    'maxiter': 200,  # Major iteration ìµœëŒ€ íšŸìˆ˜
                    'xtol': 1e-5,    # íŒŒë¼ë¯¸í„° ë³€í™” í—ˆìš© ì˜¤ì°¨
                    'disp': True
                }

                # BHHH Hessian í•¨ìˆ˜ ìƒì„±
                self.iteration_logger.info("BHHH ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ì´ˆê¸°í™”...")
                self.iteration_logger.info("  - ë°©ë²•: Newton-CG with OPG (Outer Product of Gradients)")
                self.iteration_logger.info("  - Hessian ê³„ì‚°: ê° iterationë§ˆë‹¤ ê°œì¸ë³„ gradientë¡œ OPG ê³„ì‚°")

                bhhh_hess_func = self._create_bhhh_hessian_function(
                    measurement_model,
                    structural_model,
                    choice_model,
                    negative_log_likelihood,
                    gradient_function
                )

                self.iteration_logger.info(f"BHHH ì˜µì…˜: xtol={optimizer_options['xtol']}")

                result = optimize.minimize(
                    early_stopping_wrapper.objective,
                    initial_params_scaled,
                    method='Newton-CG',  # Newton-CGëŠ” custom hess ì§€ì›
                    jac=jac_function,
                    hess=bhhh_hess_func,  # â† BHHH Hessian ì œê³µ!
                    callback=early_stopping_wrapper.callback,
                    options=optimizer_options
                )

            elif self.config.estimation.optimizer == 'BFGS':
                optimizer_options = {
                    'maxiter': 200,  # Major iteration ìµœëŒ€ íšŸìˆ˜
                    'ftol': 1e-3,    # í•¨ìˆ˜ê°’ ìƒëŒ€ì  ë³€í™” 0.1% ì´í•˜ë©´ ì¢…ë£Œ
                    'gtol': 1e-3,    # ê·¸ë˜ë””ì–¸íŠ¸ norm í—ˆìš© ì˜¤ì°¨
                    'c1': 1e-4,      # Armijo ì¡°ê±´ íŒŒë¼ë¯¸í„° (scipy ê¸°ë³¸ê°’)
                    'c2': 0.9,       # Curvature ì¡°ê±´ íŒŒë¼ë¯¸í„° (scipy ê¸°ë³¸ê°’)
                    'disp': True
                }
                self.iteration_logger.info(f"BFGS ì˜µì…˜: c1={optimizer_options['c1']}, c2={optimizer_options['c2']} (scipy ê¸°ë³¸ê°’)")

                result = optimize.minimize(
                    early_stopping_wrapper.objective,  # Wrapperì˜ objective ì‚¬ìš©
                    initial_params_scaled,  # ìŠ¤ì¼€ì¼ëœ ì´ˆê¸° íŒŒë¼ë¯¸í„° ì‚¬ìš©
                    method='BFGS',
                    jac=jac_function,
                    callback=early_stopping_wrapper.callback,  # Callback ì¶”ê°€
                    options=optimizer_options
                )

            elif self.config.estimation.optimizer == 'L-BFGS-B':
                optimizer_options = {
                    'maxiter': 200,  # Major iteration ìµœëŒ€ íšŸìˆ˜
                    'maxls': 20,     # Line search ìµœëŒ€ íšŸìˆ˜ (ê¸°ë³¸ê°’: 20)
                    'disp': True
                    # ftol, gtolì„ ì„¤ì •í•˜ì§€ ì•ŠìŒ â†’ scipy ê¸°ë³¸ê°’ ì‚¬ìš©
                    # ê¸°ë³¸ê°’: ftol=2.220446049250313e-09, pgtol=1e-05
                }
                self.iteration_logger.info(
                    f"L-BFGS-B ì˜µì…˜:\n"
                    f"  - maxiter: {optimizer_options['maxiter']}\n"
                    f"  - ftol: ê¸°ë³¸ê°’ (2.22e-09 * factr, factr=1e7)\n"
                    f"  - pgtol: ê¸°ë³¸ê°’ (1e-05)\n"
                    f"  - maxls: {optimizer_options['maxls']} (line search ìµœëŒ€ íšŸìˆ˜)\n"
                    f"\n"
                    f"  âœ… ì»¤ìŠ¤í…€ ìˆ˜ë ´ ì¡°ê±´ (callbackì—ì„œ ftol AND gtol ëª¨ë‘ ì²´í¬):\n"
                    f"    1. ftol ì¡°ê±´: (f^k - f^{{k+1}})/max{{|f^k|,|f^{{k+1}}|,1}} <= 1e-6\n"
                    f"    2. gtol ì¡°ê±´: max{{|proj g_i|}} <= 1e-5\n"
                    f"    â†’ ë‘ ì¡°ê±´ì„ ëª¨ë‘ ë§Œì¡±í•´ì•¼ ì¡°ê¸° ì¢…ë£Œ\n"
                    f"\n"
                    f"  ğŸ’¡ scipyì˜ ê¸°ë³¸ ìˆ˜ë ´ ì¡°ê±´ê³¼ ë³‘í–‰í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤."
                )

                result = optimize.minimize(
                    early_stopping_wrapper.objective,  # Wrapperì˜ objective ì‚¬ìš©
                    initial_params_scaled,  # ìŠ¤ì¼€ì¼ëœ ì´ˆê¸° íŒŒë¼ë¯¸í„° ì‚¬ìš©
                    method='L-BFGS-B',
                    jac=jac_function,
                    bounds=bounds,
                    callback=early_stopping_wrapper.callback,  # Callback ì¶”ê°€
                    options=optimizer_options
                )

            else:
                optimizer_options = {
                    'maxiter': 200,
                    'disp': True
                }

                result = optimize.minimize(
                    early_stopping_wrapper.objective,  # Wrapperì˜ objective ì‚¬ìš©
                    initial_params_scaled,  # ìŠ¤ì¼€ì¼ëœ ì´ˆê¸° íŒŒë¼ë¯¸í„° ì‚¬ìš©
                    method=self.config.estimation.optimizer,
                    jac=jac_function,
                    callback=early_stopping_wrapper.callback,  # Callback ì¶”ê°€
                    options=optimizer_options
                )

            # ìµœì í™” ê²°ê³¼ ë¡œê¹…
            self.iteration_logger.info(f"ìµœì í™” ì¢…ë£Œ: {result.message}")
            self.iteration_logger.info(f"  ì„±ê³µ ì—¬ë¶€: {result.success}")
            self.iteration_logger.info(f"  Major iterations: {major_iter_count[0]}")
            self.iteration_logger.info(f"  í•¨ìˆ˜ í˜¸ì¶œ: {result.nfev}íšŒ")

            # Line search ì‹¤íŒ¨ ê²½ê³ 
            if not result.success and 'ABNORMAL_TERMINATION_IN_LNSRCH' in result.message:
                self.iteration_logger.warning(
                    "\nâš ï¸  Line Search ì‹¤íŒ¨ë¡œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n"
                    "  ê°€ëŠ¥í•œ ì›ì¸:\n"
                    "    1. Gradient ê³„ì‚° ì˜¤ë¥˜\n"
                    "    2. í•¨ìˆ˜ê°€ ë„ˆë¬´ í‰í‰í•¨ (flat region)\n"
                    "    3. ìˆ˜ì¹˜ì  ë¶ˆì•ˆì •ì„±\n"
                    "  ê¶Œì¥ ì¡°ì¹˜:\n"
                    "    - maxls ê°’ì„ ì¦ê°€ (í˜„ì¬: 10)\n"
                    "    - ftol, gtol ê°’ì„ ì™„í™”\n"
                    "    - ì´ˆê¸°ê°’ ë³€ê²½"
                )
                self.iteration_logger.warning(
                    "\nâš ï¸  Line Search ì‹¤íŒ¨ë¡œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n"
                    "  ê°€ëŠ¥í•œ ì›ì¸:\n"
                    "    1. Gradient ê³„ì‚° ì˜¤ë¥˜\n"
                    "    2. í•¨ìˆ˜ê°€ ë„ˆë¬´ í‰í‰í•¨ (flat region)\n"
                    "    3. ìˆ˜ì¹˜ì  ë¶ˆì•ˆì •ì„±\n"
                    "  ê¶Œì¥ ì¡°ì¹˜:\n"
                    "    - maxls ê°’ì„ ì¦ê°€ (í˜„ì¬: 10)\n"
                    "    - ftol, gtol ê°’ì„ ì™„í™”\n"
                    "    - ì´ˆê¸°ê°’ ë³€ê²½"
                )

            # ì¡°ê¸° ì¢…ë£Œëœ ê²½ìš° ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ë³µì›
            if early_stopping_wrapper.early_stopped:
                from scipy.optimize import OptimizeResult

                # Wrapperì— ì €ì¥ëœ ìµœì  íŒŒë¼ë¯¸í„°ë¡œ result ê°ì²´ ì¬ìƒì„±
                result = OptimizeResult(
                    x=early_stopping_wrapper.best_x,
                    success=True,
                    message=f"Early stopping: {early_stopping_wrapper.patience}íšŒ ì—°ì† ê°œì„  ì—†ìŒ",
                    fun=early_stopping_wrapper.best_ll,
                    nit=early_stopping_wrapper.func_call_count,
                    nfev=early_stopping_wrapper.func_call_count,
                    njev=early_stopping_wrapper.grad_call_count,
                    hess_inv=None  # ë‚˜ì¤‘ì— ì„¤ì •
                )

            # Hessian ì—­í–‰ë ¬ ì²˜ë¦¬
            if self.config.estimation.calculate_se:
                # BFGSì˜ hess_invê°€ ìˆìœ¼ë©´ ì‚¬ìš© (ì¶”ê°€ ê³„ì‚° 0íšŒ!)
                if hasattr(result, 'hess_inv') and result.hess_inv is not None:
                    self.iteration_logger.info("Hessian ì—­í–‰ë ¬: BFGSì—ì„œ ìë™ ì œê³µ (ì¶”ê°€ ê³„ì‚° 0íšŒ)")
                    self.iteration_logger.info("Hessian ì—­í–‰ë ¬: BFGSì—ì„œ ìë™ ì œê³µ (ì¶”ê°€ ê³„ì‚° 0íšŒ)")

                    # âœ… Hessian ì—­í–‰ë ¬ í†µê³„ ë¡œê¹…
                    hess_inv = result.hess_inv
                    if hasattr(hess_inv, 'todense'):
                        hess_inv_array = hess_inv.todense()
                    else:
                        hess_inv_array = hess_inv

                    # âœ… Hessian ì—­í–‰ë ¬ì„ resultì— ì €ì¥ (ë‚˜ì¤‘ì— CSVë¡œ ì €ì¥)
                    self.hessian_inv_matrix = np.array(hess_inv_array)

                    # ëŒ€ê° ì›ì†Œ (ê° íŒŒë¼ë¯¸í„°ì˜ ë¶„ì‚° ê·¼ì‚¬)
                    diag_elements = np.diag(hess_inv_array)

                    # ë¹„ëŒ€ê° ì›ì†Œ (íŒŒë¼ë¯¸í„° ê°„ ê³µë¶„ì‚°)
                    off_diag_mask = ~np.eye(hess_inv_array.shape[0], dtype=bool)
                    off_diag_elements = hess_inv_array[off_diag_mask]

                    self.iteration_logger.info(
                        f"\n{'='*80}\n"
                        f"ìµœì¢… Hessian ì—­í–‰ë ¬ (H^(-1)) í†µê³„\n"
                        f"{'='*80}\n"
                        f"  Shape: {hess_inv_array.shape}\n"
                        f"  ëŒ€ê° ì›ì†Œ (ë¶„ì‚° ê·¼ì‚¬):\n"
                        f"    - ë²”ìœ„: [{np.min(diag_elements):.6e}, {np.max(diag_elements):.6e}]\n"
                        f"    - í‰ê· : {np.mean(diag_elements):.6e}\n"
                        f"    - ì¤‘ì•™ê°’: {np.median(diag_elements):.6e}\n"
                        f"    - ìŒìˆ˜ ê°œìˆ˜: {np.sum(diag_elements < 0)}/{len(diag_elements)}\n"
                        f"  ë¹„ëŒ€ê° ì›ì†Œ (ê³µë¶„ì‚°):\n"
                        f"    - ë²”ìœ„: [{np.min(off_diag_elements):.6e}, {np.max(off_diag_elements):.6e}]\n"
                        f"    - í‰ê· : {np.mean(off_diag_elements):.6e}\n"
                        f"    - ì ˆëŒ€ê°’ í‰ê· : {np.mean(np.abs(off_diag_elements)):.6e}\n"
                        f"\n  ìƒìœ„ 10ê°œ ëŒ€ê° ì›ì†Œ (íŒŒë¼ë¯¸í„° ì¸ë±ìŠ¤):\n"
                    )

                    # ìƒìœ„ 10ê°œ ëŒ€ê° ì›ì†Œ
                    top_10_indices = np.argsort(np.abs(diag_elements))[-10:][::-1]
                    for idx in top_10_indices:
                        # âœ… ì¸¡ì •ëª¨ë¸ íŒŒë¼ë¯¸í„° ê³ ì • ì‹œ param_namesëŠ” ìµœì í™”ëœ íŒŒë¼ë¯¸í„°ë§Œ í¬í•¨
                        param_name = param_names[idx] if idx < len(param_names) else f"param_{idx}"
                        self.iteration_logger.info(
                            f"    [{idx:2d}] {param_name:40s}: {diag_elements[idx]:+.6e}"
                        )

                    self.iteration_logger.info(f"{'='*80}\n")

                    # âœ… Hessian ì—­í–‰ë ¬ì€ ë³„ë„ íŒŒì¼ë¡œ ì €ì¥ (ë¡œê·¸ì—ëŠ” ì¶œë ¥í•˜ì§€ ì•ŠìŒ)
                    # (HESSIAN_ROW ë¡œê·¸ ì‚­ì œ - ë¡œê·¸ íŒŒì¼ í¬ê¸° ì ˆì•½)

                else:
                    # BFGS hess_invê°€ ì—†ìœ¼ë©´ BHHH ë°©ë²•ìœ¼ë¡œ ê³„ì‚° (L-BFGS-Bì˜ ê²½ìš°)
                    self.iteration_logger.warning("Hessian ì—­í–‰ë ¬ ì—†ìŒ (L-BFGS-BëŠ” hess_inv ì œê³µ ì•ˆ í•¨)")
                    self.iteration_logger.warning("Hessian ì—­í–‰ë ¬ ì—†ìŒ (L-BFGS-BëŠ” hess_inv ì œê³µ ì•ˆ í•¨)")
                    self.iteration_logger.info("BHHH ë°©ë²•ìœ¼ë¡œ Hessian ê³„ì‚° ì‹œì‘...")
                    self.iteration_logger.info("BHHH ë°©ë²•ìœ¼ë¡œ Hessian ê³„ì‚° ì‹œì‘...")

                    try:
                        # BHHH ë°©ë²•ìœ¼ë¡œ Hessian ê³„ì‚°
                        hess_inv_bhhh = self._compute_bhhh_hessian_inverse(
                            result.x,
                            measurement_model,
                            structural_model,
                            choice_model
                        )

                        if hess_inv_bhhh is not None:
                            self.hessian_inv_matrix = hess_inv_bhhh
                            self.iteration_logger.info("BHHH Hessian ê³„ì‚° ì„±ê³µ")
                            self.iteration_logger.info("BHHH Hessian ê³„ì‚° ì„±ê³µ")

                            # BHHH Hessian í†µê³„ ë¡œê¹… (BFGSì™€ ë™ì¼í•œ í˜•ì‹)
                            diag_elements = np.diag(hess_inv_bhhh)
                            off_diag_mask = ~np.eye(hess_inv_bhhh.shape[0], dtype=bool)
                            off_diag_elements = hess_inv_bhhh[off_diag_mask]

                            self.iteration_logger.info(
                                f"\n{'='*80}\n"
                                f"ìµœì¢… Hessian ì—­í–‰ë ¬ (H^(-1)) - BHHH ë°©ë²•\n"
                                f"{'='*80}\n"
                                f"  Shape: {hess_inv_bhhh.shape}\n"
                                f"  ëŒ€ê° ì›ì†Œ (ë¶„ì‚° ê·¼ì‚¬):\n"
                                f"    - ë²”ìœ„: [{np.min(diag_elements):.6e}, {np.max(diag_elements):.6e}]\n"
                                f"    - í‰ê· : {np.mean(diag_elements):.6e}\n"
                                f"    - ì¤‘ì•™ê°’: {np.median(diag_elements):.6e}\n"
                                f"    - ìŒìˆ˜ ê°œìˆ˜: {np.sum(diag_elements < 0)}/{len(diag_elements)}\n"
                                f"  ë¹„ëŒ€ê° ì›ì†Œ (ê³µë¶„ì‚°):\n"
                                f"    - ë²”ìœ„: [{np.min(off_diag_elements):.6e}, {np.max(off_diag_elements):.6e}]\n"
                                f"    - í‰ê· : {np.mean(off_diag_elements):.6e}\n"
                                f"    - ì ˆëŒ€ê°’ í‰ê· : {np.mean(np.abs(off_diag_elements)):.6e}\n"
                                f"\n  ìƒìœ„ 10ê°œ ëŒ€ê° ì›ì†Œ (íŒŒë¼ë¯¸í„° ì¸ë±ìŠ¤):\n"
                            )

                            # ìƒìœ„ 10ê°œ ëŒ€ê° ì›ì†Œ
                            top_10_indices = np.argsort(np.abs(diag_elements))[-10:][::-1]
                            for idx in top_10_indices:
                                param_name = self.param_names[idx] if hasattr(self, 'param_names') and idx < len(self.param_names) else f"param_{idx}"
                                self.iteration_logger.info(
                                    f"    [{idx:2d}] {param_name:40s}: {diag_elements[idx]:+.6e}"
                                )

                            self.iteration_logger.info(f"{'='*80}\n")
                        else:
                            self.iteration_logger.warning("BHHH Hessian ê³„ì‚° ì‹¤íŒ¨")
                            self.iteration_logger.warning("BHHH Hessian ê³„ì‚° ì‹¤íŒ¨")
                            self.hessian_inv_matrix = None

                    except Exception as e:
                        self.iteration_logger.error(f"BHHH Hessian ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
                        self.iteration_logger.error(f"BHHH Hessian ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
                        import traceback
                        self.iteration_logger.debug(traceback.format_exc())
                        self.hessian_inv_matrix = None
            else:
                self.hessian_inv_matrix = None

            # ìµœì¢… ë¡œê·¸
            if early_stopping_wrapper.early_stopped:
                self.iteration_logger.info(f"ì¡°ê¸° ì¢…ë£Œ ì™„ë£Œ: í•¨ìˆ˜ í˜¸ì¶œ {early_stopping_wrapper.func_call_count}íšŒ, LL={-early_stopping_wrapper.best_ll:.4f}")
                self.iteration_logger.info(f"ì¡°ê¸° ì¢…ë£Œ ì™„ë£Œ: í•¨ìˆ˜ í˜¸ì¶œ {early_stopping_wrapper.func_call_count}íšŒ, LL={-early_stopping_wrapper.best_ll:.4f}")
            else:
                self.iteration_logger.info(f"ì •ìƒ ì¢…ë£Œ: í•¨ìˆ˜ í˜¸ì¶œ {early_stopping_wrapper.func_call_count}íšŒ")
                self.iteration_logger.info(f"ì •ìƒ ì¢…ë£Œ: í•¨ìˆ˜ í˜¸ì¶œ {early_stopping_wrapper.func_call_count}íšŒ")
        else:
            self.iteration_logger.info(f"ìµœì í™” ì‹œì‘: Nelder-Mead (gradient-free)")
            self.iteration_logger.info(f"ìµœì í™” ì‹œì‘: Nelder-Mead (gradient-free)")

            result = optimize.minimize(
                negative_log_likelihood,
                initial_params_scaled,  # ìŠ¤ì¼€ì¼ëœ ì´ˆê¸° íŒŒë¼ë¯¸í„° ì‚¬ìš©
                method='Nelder-Mead',
                options={
                    'maxiter': self.config.estimation.max_iterations,
                    'xatol': 1e-4,
                    'fatol': 1e-4,
                    'disp': True
                }
            )

        if result.success:
            self.iteration_logger.info("ìµœì í™” ì„±ê³µ")
            self.iteration_logger.info("ìµœì í™” ì„±ê³µ")
        else:
            self.iteration_logger.warning(f"ìµœì í™” ì‹¤íŒ¨: {result.message}")
            self.iteration_logger.warning(f"ìµœì í™” ì‹¤íŒ¨: {result.message}")

        self.iteration_logger.info("=" * 70)
        self.iteration_logger.info(f"ìµœì¢… ë¡œê·¸ìš°ë„: {-result.fun:.4f}")
        self.iteration_logger.info(f"ë°˜ë³µ íšŸìˆ˜: {iteration_count[0]}")
        self.iteration_logger.info("=" * 70)

        # ìµœì  íŒŒë¼ë¯¸í„° ì–¸ìŠ¤ì¼€ì¼ë§ (Internal â†’ External)
        # self.iteration_logger.info("")  # âœ… ë¹ˆ ë¡œê·¸ ë¹„í™œì„±í™”
        self.iteration_logger.info("=" * 80)
        self.iteration_logger.info("ìµœì  íŒŒë¼ë¯¸í„° ë³€í™˜ (Scaled â†’ Full External)")
        self.iteration_logger.info("=" * 80)

        optimal_params_scaled = result.x

        # âœ… ParameterContextë¥¼ ì‚¬ìš©í•œ íŒŒë¼ë¯¸í„° ë³€í™˜ (í•œ ì¤„ë¡œ ê°„ì†Œí™”)
        optimal_params_full = param_context.to_full_external(optimal_params_scaled)
        optimal_params_opt = param_context.to_optimized_external(optimal_params_scaled)

        self.iteration_logger.info(
            f"âœ… íŒŒë¼ë¯¸í„° ë³€í™˜ ì™„ë£Œ:\n"
            f"  - ìµœì í™” íŒŒë¼ë¯¸í„°: {len(optimal_params_opt)}ê°œ\n"
            f"  - ì „ì²´ íŒŒë¼ë¯¸í„°: {len(optimal_params_full)}ê°œ"
        )

        # ìŠ¤ì¼€ì¼ë§ ë¹„êµ ë¡œê¹… (ìµœì í™”ëœ íŒŒë¼ë¯¸í„°ë§Œ)
        self.param_scaler.log_parameter_comparison(optimal_params_opt, optimal_params_scaled)

        # result.xë¥¼ ì „ì²´ external parametersë¡œ êµì²´
        result.x = optimal_params_full

        # ê²°ê³¼ ì²˜ë¦¬ (ë¡œê±° ì¢…ë£Œ ì „ì— ìˆ˜í–‰)
        self.results = self._process_results(
            result, measurement_model, structural_model, choice_model
        )

        # ë¡œê±° ì¢…ë£Œ (ê²°ê³¼ ì²˜ë¦¬ í›„)
        self._close_iteration_logger()

        return self.results
    
    def _compute_individual_likelihood(self, ind_id, ind_data, ind_draws,
                                       param_dict, measurement_model,
                                       structural_model, choice_model) -> float:
        """
        ê°œì¸ë³„ ìš°ë„ ê³„ì‚° (ë³‘ë ¬í™” ê°€ëŠ¥)

        Args:
            ind_id: ê°œì¸ ID
            ind_data: ê°œì¸ ë°ì´í„°
            ind_draws: ê°œì¸ì˜ Halton draws (n_draws, n_dimensions)
            param_dict: íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬
            measurement_model: ì¸¡ì •ëª¨ë¸
            structural_model: êµ¬ì¡°ëª¨ë¸
            choice_model: ì„ íƒëª¨ë¸

        Returns:
            ê°œì¸ì˜ ë¡œê·¸ìš°ë„
        """
        draw_lls = []

        # âœ… ì°¨ì› ì •ë³´ ì¶”ì¶œ
        n_exo = len(structural_model.exogenous_lvs)
        higher_order_lvs = structural_model.get_higher_order_lvs()
        n_higher_order = len(higher_order_lvs)

        # ğŸ” ë””ë²„ê¹…: ì²« ë²ˆì§¸ ê°œì¸ì˜ ì²« ë²ˆì§¸ drawë§Œ ë¡œê¹…
        log_debug = (ind_id == ind_data[self.config.individual_id_column].iloc[0])

        for j, draw in enumerate(ind_draws):
            # âœ… draws ë¶„ë¦¬: 1ì°¨ LV + 2ì°¨+ LV
            if ind_draws.ndim == 1:
                # 1ì°¨ì› (í•˜ìœ„ í˜¸í™˜)
                exo_draws = np.array([draw])
                higher_order_draws = {}

                if log_debug and j == 0:
                    self.iteration_logger.info(
                        f"[ê°œì¸ {ind_id}, Draw #0] 1ì°¨ì› draws (í•˜ìœ„ í˜¸í™˜ ëª¨ë“œ)\n"
                        f"  exo_draws: {exo_draws}\n"
                        f"  higher_order_draws: {higher_order_draws}"
                    )
            else:
                # ë‹¤ì°¨ì›
                exo_draws = draw[:n_exo]  # ì²˜ìŒ n_exoê°œ: 1ì°¨ LV
                higher_order_draws_array = draw[n_exo:]  # ë‚˜ë¨¸ì§€: 2ì°¨+ LV

                # ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
                higher_order_draws = {}
                for i, lv_name in enumerate(higher_order_lvs):
                    higher_order_draws[lv_name] = higher_order_draws_array[i]

                if log_debug and j == 0:
                    self.iteration_logger.info(
                        f"[ê°œì¸ {ind_id}, Draw #0] ë‹¤ì°¨ì› draws ë¶„ë¦¬\n"
                        f"  ind_draws.shape: {ind_draws.shape}\n"
                        f"  draw.shape: {draw.shape}\n"
                        f"  n_exo: {n_exo}, n_higher_order: {n_higher_order}\n"
                        f"  exo_draws ({len(exo_draws)}ê°œ): {exo_draws}\n"
                        f"  higher_order_draws ({len(higher_order_draws)}ê°œ): {higher_order_draws}"
                    )

            # âœ… êµ¬ì¡°ëª¨ë¸: LV = Î³*X + Î· (ì˜¬ë°”ë¥¸ ì¸ì ì „ë‹¬)
            # ğŸ” ë””ë²„ê¹…: ì²« ë²ˆì§¸ drawì— ë¡œê±° ì „ë‹¬
            if log_debug and j == 0:
                structural_model._iteration_logger = self.iteration_logger

            lv = structural_model.predict(
                data=ind_data,
                exo_draws=exo_draws,
                params=param_dict['structural'],
                higher_order_draws=higher_order_draws
            )

            if log_debug and j == 0:
                self.iteration_logger.info(
                    f"[ê°œì¸ {ind_id}, Draw #0] ì˜ˆì¸¡ëœ ì ì¬ë³€ìˆ˜\n"
                    f"  lv: {lv}"
                )

                # ğŸ” ì²« ë²ˆì§¸ draw ì´í›„ ë””ë²„ê¹… í”Œë˜ê·¸ ë¹„í™œì„±í™”
                if hasattr(structural_model, '_debug_predict'):
                    structural_model._debug_predict = False
                if hasattr(structural_model, '_debug_ll'):
                    structural_model._debug_ll = False

            # ì¸¡ì •ëª¨ë¸ ìš°ë„: P(Indicators|LV)
            ll_measurement = measurement_model.log_likelihood(
                ind_data, lv, param_dict['measurement']
            )

            # ğŸ” ë””ë²„ê¹…: ì²« ë²ˆì§¸ drawì—ì„œ ì¸¡ì •ëª¨ë¸ íŒŒë¼ë¯¸í„° í™•ì¸
            if log_debug and j == 0:
                first_lv = list(param_dict['measurement'].keys())[0]
                first_params = param_dict['measurement'][first_lv]
                self.iteration_logger.info(
                    f"[ê°œì¸ {ind_id}, Draw #0] ì¸¡ì •ëª¨ë¸ íŒŒë¼ë¯¸í„° (ì²« ë²ˆì§¸ LV: {first_lv})\n"
                    f"  zeta (ì²˜ìŒ 3ê°œ): {first_params['zeta'][:3] if len(first_params['zeta']) >= 3 else first_params['zeta']}\n"
                    f"  sigma_sq (ì²˜ìŒ 3ê°œ): {first_params['sigma_sq'][:3] if len(first_params['sigma_sq']) >= 3 else first_params['sigma_sq']}"
                )

            # Panel Product: ê°œì¸ì˜ ì—¬ëŸ¬ ì„ íƒ ìƒí™©ì— ëŒ€í•œ í™•ë¥ ì„ ê³±í•¨
            choice_set_lls = []
            for idx in range(len(ind_data)):
                ll_choice_t = choice_model.log_likelihood(
                    ind_data.iloc[idx:idx+1],  # ê° ì„ íƒ ìƒí™©
                    lv,
                    param_dict['choice']
                )
                choice_set_lls.append(ll_choice_t)

            # Panel product: log(P1 * P2 * ... * PT) = log(P1) + log(P2) + ... + log(PT)
            ll_choice = sum(choice_set_lls)

            # âœ… êµ¬ì¡°ëª¨ë¸ ìš°ë„: P(LV|X) - ì •ê·œë¶„í¬ ê°€ì • (ì˜¬ë°”ë¥¸ ì¸ì ì „ë‹¬)
            # ğŸ” ë””ë²„ê¹…: ì²« ë²ˆì§¸ drawì— ë¡œê±° ì „ë‹¬
            if log_debug and j == 0:
                structural_model._iteration_logger = self.iteration_logger

            ll_structural = structural_model.log_likelihood(
                data=ind_data,
                latent_vars=lv,
                exo_draws=exo_draws,
                params=param_dict['structural'],
                higher_order_draws=higher_order_draws
            )

            # ê²°í•© ë¡œê·¸ìš°ë„
            draw_ll = ll_measurement + ll_choice + ll_structural

            if log_debug and j == 0:
                self.iteration_logger.info(
                    f"[ê°œì¸ {ind_id}, Draw #0] ìš°ë„ ì„±ë¶„\n"
                    f"  ll_measurement: {ll_measurement:.4f}\n"
                    f"  ll_choice: {ll_choice:.4f}\n"
                    f"  ll_structural: {ll_structural:.4f}\n"
                    f"  draw_ll (í•©ê³„): {draw_ll:.4f}\n"
                    f"\n"
                    f"  âš ï¸ ìš°ë„ ì„±ë¶„ ë¹„ìœ¨:\n"
                    f"    ì¸¡ì •ëª¨ë¸: {ll_measurement:.1f} ({100*ll_measurement/draw_ll:.1f}%)\n"
                    f"    ì„ íƒëª¨ë¸: {ll_choice:.1f} ({100*ll_choice/draw_ll:.1f}%)\n"
                    f"    êµ¬ì¡°ëª¨ë¸: {ll_structural:.1f} ({100*ll_structural/draw_ll:.1f}%)"
                )

            # ğŸ”´ ìˆ˜ì •: -infë¥¼ ë§¤ìš° ì‘ì€ ê°’ìœ¼ë¡œ ëŒ€ì²´ (ì—°ì†ì„± í™•ë³´ for gradient)
            if not np.isfinite(draw_ll):
                draw_ll = -1e10  # -inf ëŒ€ì‹  ë§¤ìš° ì‘ì€ ê°’

            draw_lls.append(draw_ll)

        # ğŸ”´ ìˆ˜ì •: logsumexpë¥¼ ì‚¬ìš©í•˜ì—¬ í‰ê·  ê³„ì‚°
        # log[(1/R) Î£áµ£ exp(ll_r)] = logsumexp(ll_r) - log(R)
        person_ll = logsumexp(draw_lls) - np.log(len(draw_lls))

        return person_ll

    def _joint_log_likelihood(self, params: np.ndarray,
                             measurement_model,
                             structural_model,
                             choice_model) -> float:
        """
        ê²°í•© ë¡œê·¸ìš°ë„ ê³„ì‚°

        ì‹œë®¬ë ˆì´ì…˜ ê¸°ë°˜:
        log L â‰ˆ Î£áµ¢ log[(1/R) Î£áµ£ P(Choice|LVáµ£) Ã— P(Indicators|LVáµ£) Ã— P(LVáµ£|X)]
        """
        # íŒŒë¼ë¯¸í„° ë¶„í•´
        param_dict = self._unpack_parameters(
            params, measurement_model, structural_model, choice_model
        )

        # ğŸ” ë””ë²„ê¹…: ì²« ë²ˆì§¸ í˜¸ì¶œ ì‹œ íŒŒë¼ë¯¸í„° ë¡œê¹…
        if not hasattr(self, '_first_ll_logged'):
            self._first_ll_logged = True
            self.iteration_logger.info(
                f"\n{'='*70}\n"
                f"ì²« ë²ˆì§¸ ë¡œê·¸ìš°ë„ ê³„ì‚°\n"
                f"{'='*70}\n"
                f"  êµ¬ì¡°ëª¨ë¸ íŒŒë¼ë¯¸í„°: {param_dict['structural']}\n"
                f"  ì„ íƒëª¨ë¸ íŒŒë¼ë¯¸í„° (ì¼ë¶€): {list(param_dict['choice'].keys())[:5]}...\n"
                f"{'='*70}\n"
            )

        # ğŸ” êµ¬ì¡°ëª¨ë¸ ë””ë²„ê¹… í”Œë˜ê·¸ í™œì„±í™” (ë§¤ iterationë§ˆë‹¤, ì²« ë²ˆì§¸ ê°œì¸ì˜ ì²« ë²ˆì§¸ drawë§Œ)
        structural_model._debug_predict = True
        structural_model._debug_ll = True

        # ë©”ëª¨ë¦¬ ì²´í¬ (Halton draws ê°€ì ¸ì˜¤ê¸° ì „)
        if hasattr(self, 'memory_monitor') and hasattr(self, '_likelihood_call_count'):
            self.memory_monitor.log_memory_stats(f"Halton draws ê°€ì ¸ì˜¤ê¸° ì „ (ìš°ë„ #{self._likelihood_call_count})")

        draws = self.halton_generator.get_draws()

        # ë©”ëª¨ë¦¬ ì²´í¬ (Halton draws ê°€ì ¸ì˜¨ í›„)
        if hasattr(self, 'memory_monitor') and hasattr(self, '_likelihood_call_count'):
            self.memory_monitor.log_memory_stats(f"Halton draws ê°€ì ¸ì˜¨ í›„ (ìš°ë„ #{self._likelihood_call_count})")

        individual_ids = self.data[self.config.individual_id_column].unique()

        # ë³‘ë ¬ì²˜ë¦¬ ì—¬ë¶€ í™•ì¸
        use_parallel = getattr(self.config.estimation, 'use_parallel', False)

        if use_parallel:
            # ë³‘ë ¬ì²˜ë¦¬ ì‚¬ìš© (ì „ì—­ í•¨ìˆ˜ ì‚¬ìš©)
            n_cores = getattr(self.config.estimation, 'n_cores', None)
            if n_cores is None:
                n_cores = max(1, multiprocessing.cpu_count() - 1)

            # ì„¤ì • ì •ë³´ë¥¼ dictë¡œ ë³€í™˜ (pickle ê°€ëŠ¥)
            config_dict = {
                'measurement': {
                    'latent_variable': self.config.measurement.latent_variable,
                    'indicators': self.config.measurement.indicators,
                    'n_categories': self.config.measurement.n_categories
                },
                'structural': {
                    'sociodemographics': self.config.structural.sociodemographics,
                    'error_variance': self.config.structural.error_variance
                },
                'choice': {
                    'choice_attributes': self.config.choice.choice_attributes
                }
            }

            # ê°œì¸ë³„ ë°ì´í„° ì¤€ë¹„ (dict í˜•íƒœë¡œ ë³€í™˜)
            args_list = []
            for i, ind_id in enumerate(individual_ids):
                ind_data = self.data[self.data[self.config.individual_id_column] == ind_id]
                ind_data_dict = ind_data.to_dict('list')  # pickle ê°€ëŠ¥í•œ dictë¡œ ë³€í™˜
                ind_draws = draws[i, :]
                args_list.append((ind_data_dict, ind_draws, param_dict, config_dict))

            # ë³‘ë ¬ ê³„ì‚°
            with ProcessPoolExecutor(max_workers=n_cores) as executor:
                person_lls = list(executor.map(_compute_individual_likelihood_parallel, args_list))

            total_ll = sum(person_lls)
        else:
            # ìˆœì°¨ì²˜ë¦¬
            total_ll = 0.0
            for i, ind_id in enumerate(individual_ids):
                ind_data = self.data[self.data[self.config.individual_id_column] == ind_id]
                ind_draws = draws[i, :]

                person_ll = self._compute_individual_likelihood(
                    ind_id, ind_data, ind_draws, param_dict,
                    measurement_model, structural_model, choice_model
                )
                total_ll += person_ll

        return total_ll

    def _get_parameter_bounds(self, measurement_model,
                              structural_model, choice_model,
                              exclude_measurement: bool = False) -> list:
        """
        Parameter bounds for L-BFGS-B

        âœ… ParameterManagerì— ìœ„ì„ (ë‹¨ì¼ ì§„ì‹¤ ê³µê¸‰ì›)
        âœ… Optimizer ì¢…ë¥˜ì™€ ë¬´ê´€í•˜ê²Œ ë™ì¼í•œ ë¡œì§ ì‚¬ìš©
        âœ… íŒŒë¼ë¯¸í„° êµ¬ì¡° ë³€ê²½ ì‹œ ParameterManagerë§Œ ìˆ˜ì •
        âœ… exclude_measurement=Trueì´ë©´ ì¸¡ì •ëª¨ë¸ íŒŒë¼ë¯¸í„° ì œì™¸

        Args:
            measurement_model: ì¸¡ì •ëª¨ë¸ ê°ì²´
            structural_model: êµ¬ì¡°ëª¨ë¸ ê°ì²´
            choice_model: ì„ íƒëª¨ë¸ ê°ì²´
            exclude_measurement: Trueì´ë©´ ì¸¡ì •ëª¨ë¸ íŒŒë¼ë¯¸í„° ì œì™¸

        Returns:
            bounds: [(lower, upper), ...] list
        """
        # âœ… ParameterManagerì— ì™„ì „íˆ ìœ„ì„
        return self.param_manager.get_parameter_bounds(
            measurement_model, structural_model, choice_model,
            exclude_measurement=exclude_measurement
        )

    # âŒ ì œê±°ë¨: _get_parameter_names
    # âœ… ParameterManager.get_parameter_names()ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ë³€ê²½
    # (ì¤‘ë³µ ë¡œì§ ì œê±°, ë‹¨ì¼ ì§„ì‹¤ ê³µê¸‰ì›)

    def _get_custom_scales(self, param_names: List[str]) -> Dict[str, float]:
        """
        Custom scale ê°’ ìƒì„± (gradient ê· í˜• ìµœì í™”)

        ëª©í‘œ: ëª¨ë“  internal gradientë¥¼ 50~1,000 ë²”ìœ„ë¡œ

        Args:
            param_names: íŒŒë¼ë¯¸í„° ì´ë¦„ ë¦¬ìŠ¤íŠ¸

        Returns:
            custom_scales: íŒŒë¼ë¯¸í„° ì´ë¦„ â†’ scale ê°’ ë§¤í•‘
        """
        custom_scales = {}

        for name in param_names:
            # zeta (factor loading) ìŠ¤ì¼€ì¼
            if name.startswith('zeta_'):
                if 'health_concern' in name:
                    custom_scales[name] = 0.024
                elif 'perceived_benefit' in name:
                    custom_scales[name] = 0.050
                elif 'perceived_price' in name:
                    custom_scales[name] = 0.120
                elif 'nutrition_knowledge' in name:
                    custom_scales[name] = 0.022
                elif 'purchase_intention' in name:
                    custom_scales[name] = 0.083
                else:
                    custom_scales[name] = 0.05  # ê¸°ë³¸ê°’

            # sigma_sq (error variance) ìŠ¤ì¼€ì¼
            elif name.startswith('sigma_sq_'):
                if 'health_concern' in name:
                    custom_scales[name] = 0.034
                elif 'perceived_benefit' in name:
                    custom_scales[name] = 0.036
                elif 'perceived_price' in name:
                    custom_scales[name] = 0.023
                elif 'nutrition_knowledge' in name:
                    custom_scales[name] = 0.046
                elif 'purchase_intention' in name:
                    custom_scales[name] = 0.026
                else:
                    custom_scales[name] = 0.03  # ê¸°ë³¸ê°’

            # beta (choice model coefficients) ìŠ¤ì¼€ì¼
            elif name.startswith('beta_'):
                if name == 'beta_intercept':
                    custom_scales[name] = 0.290
                elif name == 'beta_sugar_free':
                    custom_scales[name] = 0.230
                elif name == 'beta_health_label':
                    custom_scales[name] = 0.220
                elif name == 'beta_price':
                    custom_scales[name] = 0.056
                else:
                    custom_scales[name] = 0.2  # ê¸°ë³¸ê°’

            # lambda (latent variable coefficients) ìŠ¤ì¼€ì¼
            elif name.startswith('lambda_'):
                # âœ… ëª¨ë“  LV ì£¼íš¨ê³¼ ì§€ì›
                if name == 'lambda_main':
                    custom_scales[name] = 0.890
                elif name == 'lambda_mod_perceived_price':
                    custom_scales[name] = 0.470
                elif name == 'lambda_mod_nutrition_knowledge':
                    custom_scales[name] = 1.200
                # ê°œë³„ LV lambda ìŠ¤ì¼€ì¼
                elif name == 'lambda_health_concern':
                    custom_scales[name] = 0.8
                elif name == 'lambda_perceived_benefit':
                    custom_scales[name] = 0.8
                elif name == 'lambda_perceived_price':
                    custom_scales[name] = 0.8
                elif name == 'lambda_nutrition_knowledge':
                    custom_scales[name] = 0.8
                elif name == 'lambda_purchase_intention':
                    custom_scales[name] = 0.8
                else:
                    custom_scales[name] = 0.5  # ê¸°ë³¸ê°’

            # gamma (structural model coefficients) ìŠ¤ì¼€ì¼
            elif name.startswith('gamma_'):
                custom_scales[name] = 0.5  # ê¸°ë³¸ê°’

            # tau (thresholds) ìŠ¤ì¼€ì¼
            elif name.startswith('tau_'):
                custom_scales[name] = 1.0  # ìŠ¤ì¼€ì¼ë§ ì•ˆí•¨

            # ê¸°íƒ€
            else:
                custom_scales[name] = 1.0  # ìŠ¤ì¼€ì¼ë§ ì•ˆí•¨

        return custom_scales

    def _get_initial_parameters_simultaneous(self, measurement_model, structural_model,
                                            choice_model, user_initial_params: Dict = None) -> np.ndarray:
        """
        ë™ì‹œì¶”ì • ì „ìš© ì´ˆê¸°ê°’ ì„¤ì •

        âœ… ì¸¡ì •ëª¨ë¸ íŒŒë¼ë¯¸í„°ëŠ” ì™„ì „íˆ ì œì™¸
        âœ… êµ¬ì¡°ëª¨ë¸ + ì„ íƒëª¨ë¸ë§Œ ì²˜ë¦¬ (8ê°œ íŒŒë¼ë¯¸í„°)
        âœ… ê°„ì†Œí™”ëœ ë¡œì§ìœ¼ë¡œ íŒŒë¼ë¯¸í„° ì´ë¦„-ê°’ ë§¤ì¹­ ë³´ì¥

        Args:
            measurement_model: ì¸¡ì •ëª¨ë¸ (íŒŒë¼ë¯¸í„° ì´ë¦„ ìƒì„±ì—ëŠ” ì‚¬ìš© ì•ˆ í•¨)
            structural_model: êµ¬ì¡°ëª¨ë¸
            choice_model: ì„ íƒëª¨ë¸
            user_initial_params: ì‚¬ìš©ì ì •ì˜ ì´ˆê¸°ê°’ ë”•ì…”ë„ˆë¦¬
                {'measurement': {...}, 'structural': {...}, 'choice': {...}}

        Returns:
            ì´ˆê¸° íŒŒë¼ë¯¸í„° ë°°ì—´ (8ê°œ, ì¸¡ì •ëª¨ë¸ ì œì™¸)
        """
        # âœ… ìµœì í™” íŒŒë¼ë¯¸í„° ì´ë¦„ë§Œ ìƒì„± (êµ¬ì¡°ëª¨ë¸ + ì„ íƒëª¨ë¸)
        param_names_opt = self.param_manager.get_optimized_parameter_names(
            structural_model, choice_model
        )

        self.iteration_logger.info(f"âœ… ë™ì‹œì¶”ì • ì´ˆê¸°ê°’ ì„¤ì •: {len(param_names_opt)}ê°œ íŒŒë¼ë¯¸í„° (ì¸¡ì •ëª¨ë¸ ì œì™¸)")
        self.iteration_logger.info(f"   íŒŒë¼ë¯¸í„° ì´ë¦„: {param_names_opt}")

        # âœ… ì‚¬ìš©ì ì •ì˜ ì´ˆê¸°ê°’ í•„ìˆ˜ ê²€ì¦
        if user_initial_params is None:
            raise ValueError(
                "ë™ì‹œì¶”ì •ì€ ì´ˆê¸°ê°’ì´ í•„ìˆ˜ì…ë‹ˆë‹¤!\n"
                "initial_params ë”•ì…”ë„ˆë¦¬ë¥¼ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.\n"
                "ì˜ˆ: initial_params = {'structural': {...}, 'choice': {...}}\n"
                "ì¸¡ì •ëª¨ë¸ íŒŒë¼ë¯¸í„°ëŠ” measurement_model ê°ì²´ì— ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤."
            )

        if not isinstance(user_initial_params, dict):
            raise TypeError(
                f"initial_paramsëŠ” ë”•ì…”ë„ˆë¦¬ì—¬ì•¼ í•©ë‹ˆë‹¤. í˜„ì¬ íƒ€ì…: {type(user_initial_params)}"
            )

        self.iteration_logger.info("ì‚¬ìš©ì ì •ì˜ ì´ˆê¸°ê°’ ì‚¬ìš©")
        self.iteration_logger.info(f"   ì œê³µëœ í‚¤: {list(user_initial_params.keys())}")

        # êµ¬ì¡°ëª¨ë¸ + ì„ íƒëª¨ë¸ë§Œ ì¶”ì¶œ
        opt_dict = {
            'structural': user_initial_params.get('structural', {}),
            'choice': user_initial_params.get('choice', {})
        }

        self.iteration_logger.info(f"   êµ¬ì¡°ëª¨ë¸ íŒŒë¼ë¯¸í„°: {list(opt_dict['structural'].keys())}")
        self.iteration_logger.info(f"   ì„ íƒëª¨ë¸ íŒŒë¼ë¯¸í„°: {list(opt_dict['choice'].keys())}")

        # âœ… ë™ì‹œì¶”ì • ì „ìš© ë³€í™˜ í•¨ìˆ˜ ì‚¬ìš©
        initial_values = self.param_manager.dict_to_array_optimized(
            opt_dict, param_names_opt, structural_model, choice_model
        )

        self.iteration_logger.info(f"âœ… ì´ˆê¸°ê°’ ë³€í™˜ ì™„ë£Œ: {len(initial_values)}ê°œ")

        # âœ… ì´ˆê¸°ê°’ ê²€ì¦: ì´ë¦„-ê°’ ë§¤ì¹­ í™•ì¸
        self.iteration_logger.info("=" * 80)
        self.iteration_logger.info("ì´ˆê¸°ê°’ ê²€ì¦: íŒŒë¼ë¯¸í„° ì´ë¦„-ê°’ ë§¤ì¹­ í™•ì¸")
        self.iteration_logger.info("=" * 80)

        mismatch_found = False
        for i, (name, value) in enumerate(zip(param_names_opt, initial_values)):
            # ë”•ì…”ë„ˆë¦¬ì—ì„œ ì§ì ‘ ê°’ ì¶”ì¶œ
            if name.startswith('gamma_') and '_to_' in name:
                expected_value = opt_dict['structural'].get(name, None)
                source = 'structural'
            else:
                expected_value = opt_dict['choice'].get(name, None)
                source = 'choice'

            # ë§¤ì¹­ í™•ì¸
            if expected_value is not None:
                if abs(value - expected_value) > 1e-6:
                    self.iteration_logger.error(
                        f"[{i:2d}] {name:50s} = {value:10.6f} (MISMATCH! Expected: {expected_value:10.6f}, Source: {source})"
                    )
                    mismatch_found = True
                else:
                    self.iteration_logger.info(f"[{i:2d}] {name:50s} = {value:10.6f} âœ“")
            else:
                self.iteration_logger.warning(f"[{i:2d}] {name:50s} = {value:10.6f} (NOT FOUND in {source}, using default)")

        if mismatch_found:
            self.iteration_logger.error("=" * 80)
            self.iteration_logger.error("íŒŒë¼ë¯¸í„° ì´ë¦„-ê°’ ë§¤ì¹­ ì˜¤ë¥˜ ë°œê²¬!")
            self.iteration_logger.error("=" * 80)
            raise ValueError(
                "íŒŒë¼ë¯¸í„° ì´ë¦„-ê°’ ë§¤ì¹­ ì˜¤ë¥˜!\n"
                "ìœ„ ë¡œê·¸ì—ì„œ MISMATCH í‘œì‹œëœ íŒŒë¼ë¯¸í„°ë¥¼ í™•ì¸í•˜ì„¸ìš”."
            )

        self.iteration_logger.info("=" * 80)
        self.iteration_logger.info("âœ… ëª¨ë“  íŒŒë¼ë¯¸í„° ì´ë¦„-ê°’ ë§¤ì¹­ ê²€ì¦ ì™„ë£Œ")
        self.iteration_logger.info("=" * 80)

        return initial_values

    def _get_initial_parameters(self, measurement_model,
                                structural_model, choice_model) -> np.ndarray:
        """
        ì´ˆê¸° íŒŒë¼ë¯¸í„° ìƒì„± (ParameterManager ì‚¬ìš©)

        ì‚¬ìš©ì ì •ì˜ ì´ˆê¸°ê°’(self.user_initial_params)ì´ ìˆìœ¼ë©´ ì‚¬ìš©í•˜ê³ ,
        ì—†ìœ¼ë©´ ìë™ ìƒì„±í•©ë‹ˆë‹¤.

        Returns:
            ì´ˆê¸° íŒŒë¼ë¯¸í„° ë²¡í„°
        """
        # âœ… ì „ì²´ íŒŒë¼ë¯¸í„° ì´ë¦„ ë¦¬ìŠ¤íŠ¸ ìƒì„± (ì´ˆê¸°ê°’ ìƒì„±ìš©)
        # ğŸ” ë””ë²„ê¹…: choice_model ì„¤ì • í™•ì¸
        self.iteration_logger.info(f"[DEBUG _get_initial_parameters] choice_model.all_lvs_as_main = {getattr(choice_model, 'all_lvs_as_main', None)}")
        self.iteration_logger.info(f"[DEBUG _get_initial_parameters] choice_model.main_lvs = {getattr(choice_model, 'main_lvs', None)}")

        param_names_full = self.param_manager.get_parameter_names(
            measurement_model, structural_model, choice_model
        )

        self.iteration_logger.info(f"ì „ì²´ íŒŒë¼ë¯¸í„° ì´ë¦„ ë¦¬ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ: {len(param_names_full)}ê°œ")
        # ğŸ” ë””ë²„ê¹…: ì„ íƒëª¨ë¸ íŒŒë¼ë¯¸í„° ì´ë¦„ í™•ì¸
        choice_param_names = [name for name in param_names_full if name.startswith(('beta_', 'lambda_', 'gamma_')) and '_to_' not in name]
        self.iteration_logger.info(f"[DEBUG _get_initial_parameters] ì„ íƒëª¨ë¸ íŒŒë¼ë¯¸í„° ì´ë¦„: {choice_param_names}")

        # âœ… ì‚¬ìš©ì ì •ì˜ ì´ˆê¸°ê°’ í™•ì¸
        if hasattr(self, 'user_initial_params') and self.user_initial_params is not None:
            user_params = self.user_initial_params

            # ë”•ì…”ë„ˆë¦¬ í˜•íƒœì¸ ê²½ìš° (ìˆœì°¨ì¶”ì • ê²°ê³¼)
            if isinstance(user_params, dict):
                self.iteration_logger.info("ì‚¬ìš©ì ì •ì˜ ì´ˆê¸°ê°’ (ë”•ì…”ë„ˆë¦¬) ì‚¬ìš©")

                # âœ… ì„ íƒëª¨ë¸ íŒŒë¼ë¯¸í„° í™•ì¸
                user_choice_params = user_params.get('choice', {})

                # ì¸¡ì •ëª¨ë¸ + êµ¬ì¡°ëª¨ë¸ íŒŒë¼ë¯¸í„°ëŠ” ì‚¬ìš©ì ê°’ ì‚¬ìš©
                partial_dict = {
                    'measurement': user_params.get('measurement', {}),
                    'structural': user_params.get('structural', {}),
                    'choice': {}  # ì¼ë‹¨ ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¡œ ì´ˆê¸°í™”
                }

                # âœ… ì„ íƒëª¨ë¸ ì´ˆê¸°ê°’: ì‚¬ìš©ì ì œê³µ ê°’ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ìë™ ìƒì„±
                if user_choice_params and len(user_choice_params) > 0:
                    # ì‚¬ìš©ìê°€ ì„ íƒëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ì œê³µí•œ ê²½ìš°
                    self.iteration_logger.info(f"ì‚¬ìš©ì ì •ì˜ ì„ íƒëª¨ë¸ ì´ˆê¸°ê°’ ì‚¬ìš© ({len(user_choice_params)}ê°œ íŒŒë¼ë¯¸í„°)")
                    partial_dict['choice'] = user_choice_params
                else:
                    # ì„ íƒëª¨ë¸ ì´ˆê¸°ê°’ ìë™ ìƒì„±
                    self.iteration_logger.info("ì„ íƒëª¨ë¸ ì´ˆê¸°ê°’ ìë™ ìƒì„±")
                    # MultinomialLogitChoiceëŠ” data ì¸ì í•„ìš”
                    if hasattr(choice_model, 'get_initial_params'):
                        import inspect
                        sig = inspect.signature(choice_model.get_initial_params)
                        if 'data' in sig.parameters:
                            choice_initial = choice_model.get_initial_params(data=self.data)
                        else:
                            choice_initial = choice_model.get_initial_params()
                    else:
                        choice_initial = {}

                    partial_dict['choice'] = choice_initial

                # ë”•ì…”ë„ˆë¦¬ â†’ ë°°ì—´ ë³€í™˜
                initial_values = self.param_manager.dict_to_array(
                    partial_dict, param_names_full, measurement_model
                )

                self.iteration_logger.info(f"ì‚¬ìš©ì ì •ì˜ ì´ˆê¸°ê°’ ë³€í™˜ ì™„ë£Œ: {len(initial_values)}ê°œ")

            # ë°°ì—´ í˜•íƒœì¸ ê²½ìš° (ì´ì „ ë™ì‹œì¶”ì • ê²°ê³¼)
            elif isinstance(user_params, np.ndarray):
                self.iteration_logger.info(f"ì‚¬ìš©ì ì •ì˜ ì´ˆê¸°ê°’ (ë°°ì—´) ì‚¬ìš©: {len(user_params)}ê°œ")
                initial_values = user_params
            else:
                self.iteration_logger.warning(f"ì‚¬ìš©ì ì •ì˜ ì´ˆê¸°ê°’ í˜•ì‹ ì˜¤ë¥˜: {type(user_params)}")
                self.iteration_logger.warning("ìë™ ì´ˆê¸°í™”ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                initial_values = self.param_manager.get_initial_values(
                    param_names_full, measurement_model, structural_model, choice_model
                )
        else:
            # âœ… ìë™ ì´ˆê¸°ê°’ ìƒì„± (ì´ë¦„ ê¸°ë°˜)
            initial_values = self.param_manager.get_initial_values(
                param_names_full, measurement_model, structural_model, choice_model
            )

            self.iteration_logger.info(f"ìë™ ì´ˆê¸° íŒŒë¼ë¯¸í„° ìƒì„± ì™„ë£Œ: {len(initial_values)}ê°œ")

        return initial_values



    # âŒ ì¤‘ë³µ í•¨ìˆ˜ ì œê±°ë¨ - ìœ„ì˜ _get_parameter_bounds() ì‚¬ìš©
    def _unpack_parameters(self, params: np.ndarray,
                          measurement_model,
                          structural_model,
                          choice_model) -> Dict[str, Dict]:
        """
        íŒŒë¼ë¯¸í„° ë²¡í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (ë™ì‹œì¶”ì • ì „ìš©)

        âœ… paramsëŠ” ìµœì í™” íŒŒë¼ë¯¸í„°ë§Œ í¬í•¨ (8ê°œ, ì¸¡ì •ëª¨ë¸ ì œì™¸)
        âœ… ì¸¡ì •ëª¨ë¸ íŒŒë¼ë¯¸í„°ëŠ” CFA ê²°ê³¼ì—ì„œ ìë™ìœ¼ë¡œ ì¶”ê°€ë¨

        Args:
            params: ìµœì í™” íŒŒë¼ë¯¸í„° ë°°ì—´ (8ê°œ, ì¸¡ì •ëª¨ë¸ ì œì™¸)
            measurement_model: ì¸¡ì •ëª¨ë¸ ê°ì²´
            structural_model: êµ¬ì¡°ëª¨ë¸ ê°ì²´
            choice_model: ì„ íƒëª¨ë¸ ê°ì²´

        Returns:
            íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬ (ì¸¡ì •ëª¨ë¸ í¬í•¨)
        """
        # âœ… ë™ì‹œì¶”ì • ì „ìš© ë³€í™˜ í•¨ìˆ˜ ì‚¬ìš©
        param_dict = self.param_manager.array_to_dict_optimized(
            params, self.param_names,
            measurement_model, structural_model, choice_model
        )

        return param_dict

    def _compute_gradient(self, params: np.ndarray,
                         measurement_model,
                         structural_model,
                         choice_model) -> np.ndarray:
        """
        ìˆœìˆ˜í•œ analytic gradient ê³„ì‚° (ìƒíƒœ ì˜ì¡´ì„± ì œê±°)

        ì´ ë©”ì„œë“œëŠ” ë‹¨ìœ„í…ŒìŠ¤íŠ¸ ë° gradient ê²€ì¦ì„ ìœ„í•´ ì¶”ì¶œë˜ì—ˆìŠµë‹ˆë‹¤.
        estimate() ë‚´ë¶€ì˜ gradient_function()ê³¼ ë™ì¼í•œ ë¡œì§ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

        Args:
            params: íŒŒë¼ë¯¸í„° ë²¡í„° (unscaled, external)
            measurement_model: ì¸¡ì •ëª¨ë¸
            structural_model: êµ¬ì¡°ëª¨ë¸
            choice_model: ì„ íƒëª¨ë¸

        Returns:
            gradient ë²¡í„° (negative gradient for minimization)
        """
        # íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        param_dict = self._unpack_parameters(
            params, measurement_model, structural_model, choice_model
        )

        # ğŸ” ë””ë²„ê¹…: param_dict['choice'] í‚¤ í™•ì¸
        if 'choice' in param_dict:
            self.iteration_logger.info(f"[DEBUG _compute_gradient] param_dict['choice'] í‚¤: {list(param_dict['choice'].keys())}")

        # ë³‘ë ¬ì²˜ë¦¬ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        use_parallel = getattr(self.config.estimation, 'use_parallel', False)
        n_cores = getattr(self.config.estimation, 'n_cores', None)

        # ë‹¤ì¤‘ ì ì¬ë³€ìˆ˜ ì—¬ë¶€ í™•ì¸
        from .multi_latent_config import MultiLatentConfig
        is_multi_latent = isinstance(self.config, MultiLatentConfig)

        if is_multi_latent:
            # âœ… GPU ìƒíƒœ ê°ì²´ ìƒì„±
            gpu_state = GPUComputeState.from_joint_gradient(
                self.joint_grad,
                getattr(self, 'gpu_measurement_model', None)
            )

            # âœ… í†µí•© ë¡œê¹…
            self._log_gpu_status(gpu_state)

            # ê°œì¸ ë°ì´í„° ì¤€ë¹„
            individual_ids = self.data[self.config.individual_id_column].unique()
            self.iteration_logger.info(f"ì²˜ë¦¬í•  ê°œì¸ ìˆ˜: {len(individual_ids)}")

            all_ind_data = []
            all_ind_draws = []

            for ind_id in individual_ids:
                ind_data = self.data[self.data[self.config.individual_id_column] == ind_id]
                ind_idx = np.where(individual_ids == ind_id)[0][0]
                ind_draws = self.halton_generator.get_draws()[ind_idx]

                all_ind_data.append(ind_data)
                all_ind_draws.append(ind_draws)

            # NumPy ë°°ì—´ë¡œ ë³€í™˜
            all_ind_draws = np.array(all_ind_draws)  # (N, n_draws, n_dims)

            # ğŸ¯ ë‹¨ì¼ ì§„ì…ì ìœ¼ë¡œ gradient ê³„ì‚°
            all_grad_dicts = self.joint_grad.compute_gradients(
                all_ind_data=all_ind_data,
                all_ind_draws=all_ind_draws,
                params_dict=param_dict,
                measurement_model=measurement_model,
                structural_model=structural_model,
                choice_model=choice_model,
                iteration_logger=self.iteration_logger,
                log_level='MINIMAL'
            )

            # ëª¨ë“  ê°œì¸ì˜ gradient í•©ì‚°
            total_grad_dict = None
            for ind_grad in all_grad_dicts:
                if total_grad_dict is None:
                    import copy
                    total_grad_dict = copy.deepcopy(ind_grad)
                else:
                    # ì¬ê·€ì ìœ¼ë¡œ í•©ì‚°
                    def add_gradients(total, ind):
                        for key in total:
                            if isinstance(total[key], dict):
                                add_gradients(total[key], ind[key])
                            elif isinstance(total[key], np.ndarray):
                                total[key] += ind[key]
                            else:
                                total[key] += ind[key]

                    add_gradients(total_grad_dict, ind_grad)

            grad_dict = total_grad_dict
        else:
            # ë‹¨ì¼ ì ì¬ë³€ìˆ˜: compute_gradient ì‚¬ìš©
            grad_dict = self.joint_grad.compute_gradient(
                data=self.data,
                params_dict=param_dict,
                draws=self.halton_generator.get_draws(),
                individual_id_column=self.config.individual_id_column,
                measurement_model=measurement_model,
                structural_model=structural_model,
                choice_model=choice_model,
                indicators=self.config.measurement.indicators,
                sociodemographics=self.config.structural.sociodemographics,
                choice_attributes=self.config.choice.choice_attributes,
                use_parallel=use_parallel,
                n_cores=n_cores
            )

        # ê·¸ë˜ë””ì–¸íŠ¸ ë²¡í„°ë¡œ ë³€í™˜ (íŒŒë¼ë¯¸í„° ìˆœì„œì™€ ë™ì¼)
        grad_vector = self._pack_gradient(grad_dict, measurement_model, structural_model, choice_model)

        # Negative gradient (minimize -LL)
        return -grad_vector

    def _pack_gradient(self, grad_dict: Dict, measurement_model,
                      structural_model, choice_model) -> np.ndarray:
        """
        ê·¸ë˜ë””ì–¸íŠ¸ ë”•ì…”ë„ˆë¦¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜ (ParameterManager ì‚¬ìš©)

        Args:
            grad_dict: ê·¸ë˜ë””ì–¸íŠ¸ ë”•ì…”ë„ˆë¦¬ (GPUì—ì„œ ì§ì ‘ ë°˜í™˜, grad_ ì ‘ë‘ì‚¬ ì—†ìŒ)
            measurement_model: ì¸¡ì •ëª¨ë¸
            structural_model: êµ¬ì¡°ëª¨ë¸
            choice_model: ì„ íƒëª¨ë¸

        Returns:
            gradient_vector: ê·¸ë˜ë””ì–¸íŠ¸ ë²¡í„° (ìµœì í™” íŒŒë¼ë¯¸í„°ë§Œ, ì¸¡ì •ëª¨ë¸ ì œì™¸)
        """
        # âœ… ë™ì‹œì¶”ì •: ì¸¡ì •ëª¨ë¸ ê·¸ë˜ë””ì–¸íŠ¸ ì œê±° (ê³ ì • íŒŒë¼ë¯¸í„°)
        grad_dict_opt = {
            'structural': grad_dict.get('structural', {}),
            'choice': grad_dict.get('choice', {})
        }

        # âœ… Gradient ë”•ì…”ë„ˆë¦¬ ê²€ì¦ (ìµœì í™” íŒŒë¼ë¯¸í„°ë§Œ)
        self._validate_gradient_dict(grad_dict_opt, self.param_names, measurement_model)

        # âœ… ParameterManagerë¥¼ ì‚¬ìš©í•˜ì—¬ ë°°ì—´ë¡œ ë³€í™˜ (ìµœì í™” íŒŒë¼ë¯¸í„°ë§Œ)
        gradient_vector = self.param_manager.dict_to_array(
            grad_dict_opt, self.param_names, measurement_model
        )

        return gradient_vector



    def _validate_gradient_dict(self, grad_dict: Dict, param_names: list, measurement_model) -> None:
        """
        Gradient ë”•ì…”ë„ˆë¦¬ê°€ ëª¨ë“  í•„ìš”í•œ íŒŒë¼ë¯¸í„°ë¥¼ í¬í•¨í•˜ëŠ”ì§€ ê²€ì¦

        Args:
            grad_dict: Gradient ë”•ì…”ë„ˆë¦¬ (parameter ìŠ¤íƒ€ì¼ë¡œ ë³€í™˜ëœ í›„)
            param_names: í•„ìš”í•œ íŒŒë¼ë¯¸í„° ì´ë¦„ ë¦¬ìŠ¤íŠ¸

        Raises:
            ValueError: í•„ìš”í•œ íŒŒë¼ë¯¸í„°ê°€ ëˆ„ë½ëœ ê²½ìš°
        """
        missing_params = []

        for name in param_names:
            # ì¸¡ì •ëª¨ë¸ íŒŒë¼ë¯¸í„°
            if name.startswith('zeta_'):
                # âœ… indicator ì´ë¦„ íŒŒì‹± (ì˜ˆ: zeta_health_concern_q7)
                parts = name.split('_')
                lv_name = '_'.join(parts[1:-1])  # 'health_concern'

                # measurement[lv_name]['zeta'] ë°°ì—´ì´ ìˆëŠ”ì§€ í™•ì¸
                found = False
                if 'measurement' in grad_dict and lv_name in grad_dict['measurement']:
                    if isinstance(grad_dict['measurement'][lv_name], dict):
                        if 'zeta' in grad_dict['measurement'][lv_name]:
                            found = True
                if not found:
                    missing_params.append(name)

            elif name.startswith('sigma_sq_'):
                # âœ… indicator ì´ë¦„ íŒŒì‹± (ì˜ˆ: sigma_sq_health_concern_q7)
                parts = name.split('_')
                lv_name = '_'.join(parts[2:-1])  # 'sigma_sq' ì œì™¸

                # measurement[lv_name]['sigma_sq'] ë°°ì—´ì´ ìˆëŠ”ì§€ í™•ì¸
                found = False
                if 'measurement' in grad_dict and lv_name in grad_dict['measurement']:
                    if isinstance(grad_dict['measurement'][lv_name], dict):
                        if 'sigma_sq' in grad_dict['measurement'][lv_name]:
                            found = True
                if not found:
                    missing_params.append(name)

            elif name.startswith('tau_'):
                # âœ… indicator ì´ë¦„ íŒŒì‹± (ì˜ˆ: tau_health_concern_q7_1)
                parts = name.split('_')
                lv_name = '_'.join(parts[1:-2])  # 'tau' ì œì™¸, indicatorì™€ tau_idx ì œì™¸

                # measurement[lv_name]['tau'] ë°°ì—´ì´ ìˆëŠ”ì§€ í™•ì¸
                found = False
                if 'measurement' in grad_dict and lv_name in grad_dict['measurement']:
                    if isinstance(grad_dict['measurement'][lv_name], dict):
                        if 'tau' in grad_dict['measurement'][lv_name]:
                            found = True
                if not found:
                    missing_params.append(name)

            # êµ¬ì¡°ëª¨ë¸ íŒŒë¼ë¯¸í„°
            elif name.startswith('gamma_') and '_to_' in name:
                if 'structural' not in grad_dict or name not in grad_dict['structural']:
                    missing_params.append(name)

            # ì„ íƒëª¨ë¸ íŒŒë¼ë¯¸í„°
            elif name == 'beta_intercept':
                # beta_intercept â†’ 'intercept'
                if 'choice' not in grad_dict or 'intercept' not in grad_dict['choice']:
                    missing_params.append(name)

            elif name.startswith('beta_'):
                # beta_sugar_free, beta_health_label, beta_price â†’ 'beta' ë°°ì—´
                if 'choice' not in grad_dict or 'beta' not in grad_dict['choice']:
                    missing_params.append(name)

            elif name.startswith('lambda_'):
                if 'choice' not in grad_dict or name not in grad_dict['choice']:
                    missing_params.append(name)

            elif name.startswith('gamma_') and not '_to_' in name:
                # LV-Attribute ìƒí˜¸ì‘ìš© íŒŒë¼ë¯¸í„°
                if 'choice' not in grad_dict or name not in grad_dict['choice']:
                    missing_params.append(name)

        if missing_params:
            # ìƒì„¸í•œ ì—ëŸ¬ ë©”ì‹œì§€ ìƒì„±
            error_msg = [
                "=" * 80,
                "Gradient ë”•ì…”ë„ˆë¦¬ ê²€ì¦ ì‹¤íŒ¨",
                "=" * 80,
                f"ëˆ„ë½ëœ íŒŒë¼ë¯¸í„° ({len(missing_params)}ê°œ):",
            ]
            for param in missing_params[:10]:  # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ
                error_msg.append(f"  - {param}")
            if len(missing_params) > 10:
                error_msg.append(f"  ... ì™¸ {len(missing_params) - 10}ê°œ")

            error_msg.append("")
            error_msg.append("ì‚¬ìš© ê°€ëŠ¥í•œ Gradient í‚¤:")
            error_msg.append(f"  measurement: {list(grad_dict.get('measurement', {}).keys())[:5]}...")
            error_msg.append(f"  structural: {list(grad_dict.get('structural', {}).keys())}")
            error_msg.append(f"  choice: {list(grad_dict.get('choice', {}).keys())}")
            error_msg.append("=" * 80)

            raise ValueError("\n".join(error_msg))

    def _log_gpu_status(self, gpu_state: GPUComputeState, prefix: str = ""):
        """
        GPU ìƒíƒœë¥¼ ì¼ê´€ë˜ê²Œ ë¡œê¹…

        Args:
            gpu_state: GPU ê³„ì‚° ìƒíƒœ ê°ì²´
            prefix: ë¡œê·¸ ë©”ì‹œì§€ ì ‘ë‘ì‚¬
        """
        separator = "=" * 80
        mode_msg = f"{prefix}Gradient ê³„ì‚° ëª¨ë“œ: {gpu_state.get_mode_name()}"

        # ì½˜ì†”ê³¼ íŒŒì¼ ëª¨ë‘ì— ê¸°ë¡
        self.iteration_logger.info(separator)
        self.iteration_logger.info(mode_msg)
        self.iteration_logger.info(separator)

        self.iteration_logger.info(separator)
        self.iteration_logger.info(mode_msg)
        self.iteration_logger.info(separator)

        # ìƒì„¸ ì •ë³´ëŠ” íŒŒì¼ì—ë§Œ ê¸°ë¡
        status = gpu_state.get_status_dict()
        self.iteration_logger.info(f"{prefix}  enabled: {status['enabled']}")
        self.iteration_logger.info(f"{prefix}  measurement_model: {status['measurement_model_available']}")
        self.iteration_logger.info(f"{prefix}  full_parallel: {status['full_parallel']}")
        self.iteration_logger.info(f"{prefix}  is_ready: {status['is_ready']}")
        self.iteration_logger.info(separator)

    def _process_results(self, optimization_result,
                        measurement_model,
                        structural_model,
                        choice_model) -> Dict:
        """ìµœì í™” ê²°ê³¼ ì²˜ë¦¬"""
        
        param_dict = self._unpack_parameters(
            optimization_result.x,
            measurement_model,
            structural_model,
            choice_model
        )
        
        results = {
            'success': optimization_result.success,
            'message': optimization_result.message,
            'log_likelihood': -optimization_result.fun,
            'n_iterations': optimization_result.nit,
            'parameters': param_dict,
            'raw_params': optimization_result.x,
            
            # ëª¨ë¸ ì í•©ë„
            'n_observations': len(self.data),
            'n_parameters': len(optimization_result.x),
        }
        
        # AIC, BIC ê³„ì‚°
        ll = results['log_likelihood']
        k = results['n_parameters']
        n = results['n_observations']
        
        results['aic'] = -2 * ll + 2 * k
        results['bic'] = -2 * ll + k * np.log(n)
        
        # í‘œì¤€ì˜¤ì°¨ ê³„ì‚° (Hessian ê¸°ë°˜)
        if self.config.estimation.calculate_se:
            try:
                # BFGSëŠ” hess_invë¥¼ ë°˜í™˜ (ì—­ Hessian)
                # í‘œì¤€ì˜¤ì°¨ = sqrt(diag(H^-1))
                if hasattr(optimization_result, 'hess_inv'):
                    hess_inv = optimization_result.hess_inv
                    if hasattr(hess_inv, 'todense'):
                        hess_inv = hess_inv.todense()

                    # âœ… Hessian ì—­í–‰ë ¬ì„ ê²°ê³¼ì— ì €ì¥
                    results['hessian_inv'] = np.array(hess_inv)

                    # ëŒ€ê° ì›ì†Œ ì¶”ì¶œ (ë¶„ì‚°)
                    variances = np.diag(hess_inv)

                    # ìŒìˆ˜ ë¶„ì‚° ì²˜ë¦¬ (ìˆ˜ì¹˜ ì˜¤ë¥˜)
                    variances = np.maximum(variances, 1e-10)

                    se = np.sqrt(variances)
                    results['standard_errors'] = se

                    # t-í†µê³„ëŸ‰
                    results['t_statistics'] = optimization_result.x / se

                    # p-ê°’ (ì–‘ì¸¡ ê²€ì •, ëŒ€í‘œë³¸ì´ë¯€ë¡œ ì •ê·œë¶„í¬ ì‚¬ìš©)
                    from scipy.stats import norm
                    results['p_values'] = 2 * (1 - norm.cdf(np.abs(results['t_statistics'])))

                    # íŒŒë¼ë¯¸í„°ë³„ë¡œ êµ¬ì¡°í™”
                    self.iteration_logger.info("íŒŒë¼ë¯¸í„°ë³„ í†µê³„ëŸ‰ êµ¬ì¡°í™” ì¤‘...")
                    results['parameter_statistics'] = self._structure_statistics(
                        optimization_result.x, se,
                        results['t_statistics'], results['p_values'],
                        measurement_model, structural_model, choice_model
                    )
                    self.iteration_logger.info("íŒŒë¼ë¯¸í„°ë³„ í†µê³„ëŸ‰ êµ¬ì¡°í™” ì™„ë£Œ")

                else:
                    self.iteration_logger.warning("Hessian ì •ë³´ê°€ ì—†ì–´ í‘œì¤€ì˜¤ì°¨ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    results['hessian_inv'] = None

            except Exception as e:
                self.iteration_logger.warning(f"í‘œì¤€ì˜¤ì°¨ ê³„ì‚° ì‹¤íŒ¨: {e}")
                import traceback
                self.iteration_logger.debug(traceback.format_exc())
                results['hessian_inv'] = None
        else:
            results['hessian_inv'] = None

        # CSV ë¡œê·¸ íŒŒì¼ ë‹«ê¸°
        if hasattr(self, 'csv_log_file') and self.csv_log_file:
            self.csv_log_file.close()
            self.iteration_logger.info(f"CSV ë¡œê·¸ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {self.csv_log_path}")

        return results

    def _structure_statistics(self, estimates, std_errors, t_stats, p_values,
                              measurement_model, structural_model, choice_model):
        """
        íŒŒë¼ë¯¸í„°ë³„ í†µê³„ëŸ‰ì„ êµ¬ì¡°í™”ëœ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜

        Args:
            estimates: ì¶”ì •ê°’ ë²¡í„°
            std_errors: í‘œì¤€ì˜¤ì°¨ ë²¡í„°
            t_stats: t-í†µê³„ëŸ‰ ë²¡í„°
            p_values: p-value ë²¡í„°
            measurement_model: ì¸¡ì •ëª¨ë¸
            structural_model: êµ¬ì¡°ëª¨ë¸
            choice_model: ì„ íƒëª¨ë¸

        Returns:
            êµ¬ì¡°í™”ëœ í†µê³„ëŸ‰ ë”•ì…”ë„ˆë¦¬
            {
                'measurement': {'zeta': {...}, 'tau': {...}},
                'structural': {'gamma': {...}},
                'choice': {'intercept': {...}, 'beta': {...}, 'lambda': {...}}
            }
        """
        # íŒŒë¼ë¯¸í„° ì–¸íŒ©
        param_dict = self._unpack_parameters(
            estimates, measurement_model, structural_model, choice_model
        )

        # ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ í‘œì¤€ì˜¤ì°¨, t-í†µê³„ëŸ‰, p-value ì–¸íŒ©
        se_dict = self._unpack_parameters(
            std_errors, measurement_model, structural_model, choice_model
        )
        t_dict = self._unpack_parameters(
            t_stats, measurement_model, structural_model, choice_model
        )
        p_dict = self._unpack_parameters(
            p_values, measurement_model, structural_model, choice_model
        )

        # êµ¬ì¡°í™”ëœ ê²°ê³¼ ìƒì„±
        structured = {
            'measurement': {},
            'structural': {},
            'choice': {}
        }

        # ì¸¡ì •ëª¨ë¸
        if 'measurement' in param_dict:
            for key in param_dict['measurement']:
                structured['measurement'][key] = {
                    'estimate': param_dict['measurement'][key],
                    'std_error': se_dict['measurement'][key],
                    't_statistic': t_dict['measurement'][key],
                    'p_value': p_dict['measurement'][key]
                }

        # êµ¬ì¡°ëª¨ë¸
        if 'structural' in param_dict:
            for key in param_dict['structural']:
                structured['structural'][key] = {
                    'estimate': param_dict['structural'][key],
                    'std_error': se_dict['structural'][key],
                    't_statistic': t_dict['structural'][key],
                    'p_value': p_dict['structural'][key]
                }

        # ì„ íƒëª¨ë¸
        if 'choice' in param_dict:
            for key in param_dict['choice']:
                structured['choice'][key] = {
                    'estimate': param_dict['choice'][key],
                    'std_error': se_dict['choice'][key],
                    't_statistic': t_dict['choice'][key],
                    'p_value': p_dict['choice'][key]
                }

        return structured

    def _create_bhhh_hessian_function(
        self,
        measurement_model,
        structural_model,
        choice_model,
        negative_log_likelihood_func,
        gradient_func
    ):
        """
        BHHH Hessian ê³„ì‚° í•¨ìˆ˜ ìƒì„± (scipy.optimize.minimizeì˜ hess íŒŒë¼ë¯¸í„°ìš©)

        BHHH ë°©ë²•:
        - Hessianì„ ì§ì ‘ ê³„ì‚°í•˜ì§€ ì•Šê³  OPG (Outer Product of Gradients)ë¡œ ëŒ€ì²´
        - OPG = Î£_i (grad_i Ã— grad_i^T)
        - ê° iterationë§ˆë‹¤ ëª¨ë“  ê°œì¸ì˜ gradientë¥¼ ê³„ì‚°í•˜ì—¬ OPG ìƒì„±

        Args:
            negative_log_likelihood_func: negative log-likelihood í•¨ìˆ˜
            gradient_func: gradient í•¨ìˆ˜

        Returns:
            callable: hess(x) -> np.ndarray (n_params, n_params)
        """
        from src.analysis.hybrid_choice_model.iclv_models.bhhh_calculator import BHHHCalculator

        bhhh_calc = BHHHCalculator(logger=self.iteration_logger)
        hess_call_count = [0]  # Hessian í˜¸ì¶œ íšŸìˆ˜ ì¶”ì 

        # âœ… Major iteration ì¶”ì ì„ ìœ„í•œ ë³€ìˆ˜
        prev_x = [None]  # ì´ì „ íŒŒë¼ë¯¸í„°
        prev_ll = [None]  # ì´ì „ LL
        major_iter_count = [0]  # Major iteration ì¹´ìš´í„°

        def bhhh_hessian(x):
            """
            í˜„ì¬ íŒŒë¼ë¯¸í„°ì—ì„œ BHHH Hessian ê³„ì‚°

            Args:
                x: í˜„ì¬ íŒŒë¼ë¯¸í„° ë²¡í„° (scaled)

            Returns:
                BHHH Hessian (n_params, n_params)
            """
            import time
            hess_start_time = time.time()

            hess_call_count[0] += 1

            # âœ… Major iteration íŒë‹¨: íŒŒë¼ë¯¸í„°ê°€ ë³€ê²½ë˜ì—ˆìœ¼ë©´ ìƒˆë¡œìš´ major iteration
            is_new_major_iter = False
            if prev_x[0] is None or not np.allclose(x, prev_x[0], rtol=1e-10):
                major_iter_count[0] += 1
                is_new_major_iter = True

                # âœ… Major Iteration ì‹œì‘ ë¡œê¹…
                self.iteration_logger.info(
                    f"\n{'='*80}\n"
                    f"[Major Iteration #{major_iter_count[0]} ì‹œì‘]\n"
                    f"{'='*80}"
                )

            self.iteration_logger.info(
                f"\n{'='*80}\n"
                f"BHHH Hessian ê³„ì‚° #{hess_call_count[0]}\n"
                f"{'='*80}"
            )

            # íŒŒë¼ë¯¸í„° ì–¸ìŠ¤ì¼€ì¼ë§
            if self.param_scaler is not None:
                x_unscaled = self.param_scaler.unscale_parameters(x)
            else:
                x_unscaled = x

            # íŒŒë¼ë¯¸í„° ì–¸íŒ©
            param_dict = self._unpack_parameters(
                x_unscaled, measurement_model, structural_model, choice_model
            )

            # ê°œì¸ë³„ gradient ê³„ì‚°
            self.iteration_logger.info("ê°œì¸ë³„ gradient ê³„ì‚° ì‹œì‘...")
            individual_ids = self.data[self.config.individual_id_column].unique()
            n_individuals = len(individual_ids)

            # âœ… GPU batch í™œìš© ì—¬ë¶€ í™•ì¸
            use_gpu = hasattr(self.joint_grad, 'use_gpu') and self.joint_grad.use_gpu

            # âœ… ì™„ì „ GPU Batch: ëª¨ë“  ê°œì¸ì„ ë™ì‹œì— ì²˜ë¦¬
            if use_gpu and hasattr(self.joint_grad, 'compute_all_individuals_gradients_batch'):
                import time

                self.iteration_logger.info(
                    f"  âœ… ì™„ì „ GPU Batch ëª¨ë“œ: {n_individuals}ëª… ë™ì‹œ ì²˜ë¦¬"
                )

                # ëª¨ë“  ê°œì¸ì˜ ë°ì´í„°ì™€ draws ì¤€ë¹„
                prep_start = time.time()
                all_ind_data = []
                all_ind_draws = []

                for ind_id in individual_ids:
                    ind_data = self.data[self.data[self.config.individual_id_column] == ind_id]
                    ind_idx = np.where(individual_ids == ind_id)[0][0]
                    ind_draws = self.halton_generator.get_draws()[ind_idx]

                    all_ind_data.append(ind_data)
                    all_ind_draws.append(ind_draws)

                # NumPy ë°°ì—´ë¡œ ë³€í™˜
                all_ind_draws = np.array(all_ind_draws)  # (N, n_draws, n_dims)
                prep_time = time.time() - prep_start

                self.iteration_logger.info(
                    f"  ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ ({prep_time:.3f}ì´ˆ): "
                    f"all_ind_draws shape = {all_ind_draws.shape}"
                )

                # âœ… ì™„ì „ GPU Batchë¡œ ëª¨ë“  ê°œì¸ì˜ gradient ë™ì‹œ ê³„ì‚°
                # ğŸš€ 326ëª… Ã— 100 draws Ã— 80 params = 2,608,000ê°œ ë™ì‹œ ê³„ì‚°
                gpu_start = time.time()

                # ì™„ì „ GPU Batch ì‚¬ìš© (hasattrë¡œ í™•ì¸)
                if hasattr(self.joint_grad, 'compute_all_individuals_gradients_full_batch'):
                    all_grad_dicts = self.joint_grad.compute_all_individuals_gradients_full_batch(
                        all_ind_data=all_ind_data,
                        all_ind_draws=all_ind_draws,
                        params_dict=param_dict,
                        measurement_model=measurement_model,
                        structural_model=structural_model,
                        choice_model=choice_model,
                        iteration_logger=self.iteration_logger,
                        log_level='MODERATE' if hess_call_count[0] <= 2 else 'MINIMAL'
                    )
                else:
                    # í´ë°±: ì¼ë°˜ batch
                    all_grad_dicts = self.joint_grad.compute_all_individuals_gradients_batch(
                        all_ind_data=all_ind_data,
                        all_ind_draws=all_ind_draws,
                        params_dict=param_dict,
                        measurement_model=measurement_model,
                        structural_model=structural_model,
                        choice_model=choice_model,
                        iteration_logger=self.iteration_logger,
                        log_level='MODERATE' if hess_call_count[0] <= 2 else 'MINIMAL'
                    )

                gpu_time = time.time() - gpu_start

                self.iteration_logger.info(
                    f"  GPU Batch gradient ê³„ì‚° ì™„ë£Œ ({gpu_time:.3f}ì´ˆ)"
                )

                # Gradient ë²¡í„°ë¡œ ë³€í™˜ ë° ìŠ¤ì¼€ì¼ë§
                self.iteration_logger.info(f"  ê°œì¸ë³„ gradient ë²¡í„° ë³€í™˜ ì‹œì‘ ({len(all_grad_dicts)}ëª…)...")
                individual_gradients = []
                for i, ind_grad_dict in enumerate(all_grad_dicts):
                    grad_vector = self._pack_gradient(
                        ind_grad_dict,
                        measurement_model,
                        structural_model,
                        choice_model
                    )

                    if self.param_scaler is not None:
                        grad_vector = self.param_scaler.scale_gradient(grad_vector)

                    individual_gradients.append(grad_vector)

                    # ì²˜ìŒ 3ëª…ë§Œ ìƒì„¸ ë¡œê¹…
                    if i < 3:
                        self.iteration_logger.info(
                            f"  ê°œì¸ {i} (ID={individual_ids[i]}): gradient norm = {np.linalg.norm(grad_vector):.6e}"
                        )

                self.iteration_logger.info(
                    f"âœ… ì™„ì „ GPU Batch gradient ê³„ì‚° ì™„ë£Œ: {n_individuals}ëª…"
                )

                # Gradient í†µê³„ ë¡œê¹…
                grad_norms = [np.linalg.norm(g) for g in individual_gradients]
                self.iteration_logger.info(
                    f"  Gradient norm í†µê³„: min={min(grad_norms):.6e}, "
                    f"max={max(grad_norms):.6e}, mean={np.mean(grad_norms):.6e}"
                )

            else:
                # ê¸°ì¡´ ë°©ì‹: ê°œì¸ë³„ ìˆœì°¨ ì²˜ë¦¬ (ê° ê°œì¸ ë‚´ë¶€ëŠ” GPU batch)
                if use_gpu:
                    self.iteration_logger.info("  GPU batch ëª¨ë“œë¡œ ê°œì¸ë³„ gradient ê³„ì‚° (ìˆœì°¨)")
                else:
                    self.iteration_logger.info("  CPU ëª¨ë“œë¡œ ê°œì¸ë³„ gradient ê³„ì‚°")

                individual_gradients = []
                for i, ind_id in enumerate(individual_ids):
                    # ê°œì¸ ë°ì´í„° ë° draws ê°€ì ¸ì˜¤ê¸°
                    ind_data = self.data[self.data[self.config.individual_id_column] == ind_id]
                    ind_idx = np.where(individual_ids == ind_id)[0][0]
                    ind_draws = self.halton_generator.get_draws()[ind_idx]

                    # ê°œì¸ë³„ gradient ê³„ì‚°
                    ind_grad_dict = self.joint_grad.compute_individual_gradient(
                        ind_data=ind_data,
                        ind_draws=ind_draws,
                        params_dict=param_dict,
                        measurement_model=measurement_model,
                        structural_model=structural_model,
                        choice_model=choice_model,
                        ind_id=ind_id
                    )

                    # Gradient ë²¡í„°ë¡œ ë³€í™˜
                    grad_vector = self._pack_gradient(
                        ind_grad_dict,
                        measurement_model,
                        structural_model,
                        choice_model
                    )

                    # ìŠ¤ì¼€ì¼ë§ ì ìš©
                    if self.param_scaler is not None:
                        grad_vector = self.param_scaler.scale_gradient(grad_vector)

                    individual_gradients.append(grad_vector)

                    # ì²˜ìŒ 3ëª…ë§Œ ìƒì„¸ ë¡œê¹…
                    if i < 3:
                        self.iteration_logger.info(
                            f"  ê°œì¸ {i} (ID={ind_id}): gradient norm = {np.linalg.norm(grad_vector):.6e}"
                        )

                self.iteration_logger.info(
                    f"ê°œì¸ë³„ gradient ê³„ì‚° ì™„ë£Œ: {n_individuals}ëª…"
                )

            # BHHH Hessian ê³„ì‚° (OPG)
            import time
            self.iteration_logger.info("OPG í–‰ë ¬ ê³„ì‚° ì¤‘...")
            opg_start = time.time()
            hessian_bhhh = bhhh_calc.compute_bhhh_hessian(
                individual_gradients,
                for_minimization=True  # scipyëŠ” ìµœì†Œí™” ë¬¸ì œ
            )
            opg_time = time.time() - opg_start
            self.iteration_logger.info(f"OPG ê³„ì‚° ì™„ë£Œ ({opg_time:.3f}ì´ˆ)")

            hess_total_time = time.time() - hess_start_time

            self.iteration_logger.info(
                f"\n{'='*80}\n"
                f"BHHH Hessian ê³„ì‚° ì™„ë£Œ (ì´ {hess_total_time:.3f}ì´ˆ)\n"
                f"{'='*80}\n"
                f"  Shape: {hessian_bhhh.shape}\n"
                f"  ì‹œê°„ ë¶„ì„:\n"
                f"    - ë°ì´í„° ì¤€ë¹„: {prep_time if 'prep_time' in locals() else 0:.3f}ì´ˆ\n"
                f"    - GPU Batch gradient: {gpu_time if 'gpu_time' in locals() else 0:.3f}ì´ˆ\n"
                f"    - OPG ê³„ì‚°: {opg_time:.3f}ì´ˆ\n"
                f"  ì„±ëŠ¥:\n"
                f"    - ê°œì¸ë‹¹ ì‹œê°„: {hess_total_time / n_individuals * 1000:.2f}ms\n"
                f"    - ì²˜ë¦¬ëŸ‰: {n_individuals / hess_total_time:.1f} ê°œì¸/ì´ˆ\n"
                f"{'='*80}"
            )

            # âœ… Major Iteration ì™„ë£Œ ë¡œê¹… ë° CSV ì €ì¥
            if is_new_major_iter:
                # í˜„ì¬ LL ê³„ì‚°
                x_unscaled = self.param_scaler.unscale_parameters(x) if self.param_scaler is not None else x
                current_ll = -negative_log_likelihood_func(x)  # objectiveëŠ” -LLì´ë¯€ë¡œ ë¶€í˜¸ ë°˜ì „

                # íŒŒë¼ë¯¸í„° ë³€í™”ëŸ‰ ê³„ì‚°
                if prev_x[0] is not None:
                    param_change = np.linalg.norm(x - prev_x[0])
                    ll_change = current_ll - prev_ll[0] if prev_ll[0] is not None else 0.0
                else:
                    param_change = 0.0
                    ll_change = 0.0

                # Gradient ê³„ì‚° (ì „ì²´ gradient)
                grad = gradient_func(x)
                grad_norm = np.linalg.norm(grad)

                # íŒŒë¼ë¯¸í„° ë° gradient ìƒì„¸ ë¡œê¹…
                params_external = self.param_scaler.unscale_parameters(x) if self.param_scaler is not None else x

                gradient_details = "\n  ì „ì²´ íŒŒë¼ë¯¸í„° ê°’ ë° ê·¸ë˜ë””ì–¸íŠ¸:\n"
                for idx in range(len(params_external)):
                    param_name = self.param_names[idx] if hasattr(self, 'param_names') and idx < len(self.param_names) else f"param_{idx}"
                    gradient_details += f"    [{idx:2d}] {param_name:50s}: param={params_external[idx]:+12.6e}, grad={grad[idx]:+12.6e}\n"

                # CSV íŒŒì¼ì— ê¸°ë¡
                if hasattr(self, '_log_params_grads_to_csv'):
                    self._log_params_grads_to_csv(major_iter_count[0], params_external, grad)

                # Major Iteration ì™„ë£Œ ë¡œê¹…
                self.iteration_logger.info(
                    f"\n{'='*80}\n"
                    f"[Major Iteration #{major_iter_count[0]} ì™„ë£Œ]\n"
                    f"  ìµœì¢… LL: {current_ll:.4f}\n"
                    f"  íŒŒë¼ë¯¸í„° ë³€í™”ëŸ‰ (L2 norm): {param_change:.6e}\n"
                    f"  LL ë³€í™”: {ll_change:+.4f}\n"
                    f"  Gradient norm: {grad_norm:.6e}\n"
                    f"{gradient_details}"
                    f"  Hessian ê·¼ì‚¬: BHHH (OPG) ë°©ë²•ìœ¼ë¡œ ê³„ì‚° ì™„ë£Œ\n"
                    f"{'='*80}"
                )

                # í˜„ì¬ ìƒíƒœ ì €ì¥
                prev_x[0] = x.copy()
                prev_ll[0] = current_ll

            return hessian_bhhh

        return bhhh_hessian

    def _compute_bhhh_hessian_inverse(
        self,
        optimal_params: np.ndarray,
        measurement_model,
        structural_model,
        choice_model,
        max_individuals: int = 100,
        use_all_individuals: bool = False
    ) -> Optional[np.ndarray]:
        """
        BHHH ë°©ë²•ìœ¼ë¡œ Hessian ì—­í–‰ë ¬ ê³„ì‚°

        Args:
            optimal_params: ìµœì  íŒŒë¼ë¯¸í„° ë²¡í„°
            measurement_model: ì¸¡ì •ëª¨ë¸
            structural_model: êµ¬ì¡°ëª¨ë¸
            choice_model: ì„ íƒëª¨ë¸
            max_individuals: ìµœëŒ€ ê°œì¸ ìˆ˜ (ìƒ˜í”Œë§)
            use_all_individuals: Trueë©´ ëª¨ë“  ê°œì¸ ì‚¬ìš©

        Returns:
            Hessian ì—­í–‰ë ¬ (n_params, n_params) ë˜ëŠ” None (ì‹¤íŒ¨ ì‹œ)
        """
        try:
            # BHHH ê³„ì‚°ê¸° ì´ˆê¸°í™”
            bhhh_calc = BHHHCalculator(logger=self.iteration_logger)

            # íŒŒë¼ë¯¸í„° ì–¸íŒ©
            param_dict = self._unpack_parameters(
                optimal_params, measurement_model, structural_model, choice_model
            )

            # ê°œì¸ë³„ gradient ê³„ì‚°
            self.iteration_logger.info("ê°œì¸ë³„ gradient ê³„ì‚° ì‹œì‘...")
            individual_gradients = []

            # ê°œì¸ ID ëª©ë¡
            individual_ids = self.data[self.config.individual_id_column].unique()
            n_total_individuals = len(individual_ids)

            # ìƒ˜í”Œë§ ì—¬ë¶€ ê²°ì •
            if use_all_individuals:
                n_individuals = n_total_individuals
                sampled_ids = individual_ids
            else:
                n_individuals = min(max_individuals, n_total_individuals)
                # ê· ë“± ìƒ˜í”Œë§
                step = max(1, n_total_individuals // n_individuals)
                sampled_ids = individual_ids[::step][:n_individuals]

            self.iteration_logger.info(
                f"BHHH ê³„ì‚°: {n_individuals}ëª… ì‚¬ìš© "
                f"(ì „ì²´ {n_total_individuals}ëª… ì¤‘)"
            )

            # ë‹¤ì¤‘ ì ì¬ë³€ìˆ˜ ì—¬ë¶€ í™•ì¸
            from .multi_latent_config import MultiLatentConfig
            is_multi_latent = isinstance(self.config, MultiLatentConfig)

            if is_multi_latent:
                # ë‹¤ì¤‘ ì ì¬ë³€ìˆ˜: compute_individual_gradient ì‚¬ìš©
                from .multi_latent_gradient import MultiLatentJointGradient

                for i, ind_id in enumerate(sampled_ids):
                    if i % 10 == 0:
                        self.iteration_logger.info(f"  ì§„í–‰: {i}/{n_individuals}")

                    # ê°œì¸ ë°ì´í„°
                    ind_data = self.data[
                        self.data[self.config.individual_id_column] == ind_id
                    ]

                    # ê°œì¸ draws
                    ind_idx = np.where(individual_ids == ind_id)[0][0]
                    ind_draws = self.halton_generator.get_draws()[ind_idx]

                    # ê°œì¸ë³„ gradient ê³„ì‚°
                    ind_grad_dict = self.joint_grad.compute_individual_gradient(
                        ind_data=ind_data,
                        ind_draws=ind_draws,
                        params_dict=param_dict,
                        measurement_model=measurement_model,
                        structural_model=structural_model,
                        choice_model=choice_model,
                        ind_id=ind_id
                    )

                    # Gradientë¥¼ ë²¡í„°ë¡œ ë³€í™˜
                    grad_vector = self._pack_gradient(
                        ind_grad_dict,
                        measurement_model,
                        structural_model,
                        choice_model
                    )

                    # ì²˜ìŒ 3ëª…ì˜ gradient ìƒì„¸ ë¡œê¹…
                    if i < 3:
                        self.iteration_logger.info(
                            f"\nê°œì¸ {i} (ID={ind_id}) Gradient ë²¡í„°:\n"
                            f"  Shape: {grad_vector.shape}\n"
                            f"  Norm: {np.linalg.norm(grad_vector):.6e}\n"
                            f"  ë²”ìœ„: [{np.min(grad_vector):.6e}, {np.max(grad_vector):.6e}]\n"
                            f"  ì²˜ìŒ 5ê°œ ê°’: {grad_vector[:5]}"
                        )

                    individual_gradients.append(grad_vector)

            else:
                # ë‹¨ì¼ ì ì¬ë³€ìˆ˜ (ê¸°ì¡´ ë°©ì‹)
                self.iteration_logger.warning(
                    "ë‹¨ì¼ ì ì¬ë³€ìˆ˜ ëª¨ë¸ì˜ BHHHëŠ” ì•„ì§ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                )
                return None

            self.iteration_logger.info(
                f"\n{'='*80}\n"
                f"ê°œì¸ë³„ gradient ê³„ì‚° ì™„ë£Œ\n"
                f"{'='*80}\n"
                f"  ì´ ê°œì¸ ìˆ˜: {len(individual_gradients)}ëª…\n"
                f"  Gradient ë²¡í„° ê¸¸ì´: {len(individual_gradients[0])}ê°œ íŒŒë¼ë¯¸í„°\n"
                f"{'='*80}"
            )

            # BHHH Hessian ê³„ì‚°
            self.iteration_logger.info("\n" + "="*80)
            self.iteration_logger.info("BHHH Hessian ê³„ì‚° ì‹œì‘ (OPG ë°©ì‹)")
            self.iteration_logger.info("="*80)
            hessian_bhhh = bhhh_calc.compute_bhhh_hessian(
                individual_gradients,
                for_minimization=True  # scipy.optimize.minimizeëŠ” ìµœì†Œí™” ë¬¸ì œ
            )

            # Hessian ì—­í–‰ë ¬ ê³„ì‚°
            self.iteration_logger.info("Hessian ì—­í–‰ë ¬ ê³„ì‚° ì¤‘...")
            hess_inv = bhhh_calc.compute_hessian_inverse(
                hessian_bhhh,
                regularization=1e-8
            )

            # í‘œì¤€ì˜¤ì°¨ ê³„ì‚° (ê²€ì¦ìš©)
            se = bhhh_calc.compute_standard_errors(hess_inv)
            self.iteration_logger.info(
                f"BHHH í‘œì¤€ì˜¤ì°¨ ë²”ìœ„: "
                f"[{np.min(se):.6e}, {np.max(se):.6e}]"
            )

            return hess_inv

        except Exception as e:
            self.iteration_logger.error(f"BHHH Hessian ê³„ì‚° ì‹¤íŒ¨: {e}")
            import traceback
            self.iteration_logger.debug(traceback.format_exc())
            return None


def estimate_iclv_simultaneous(data: pd.DataFrame, config,
                               measurement_model,
                               structural_model,
                               choice_model) -> Dict:
    """
    ICLV ëª¨ë¸ ë™ì‹œ ì¶”ì • í—¬í¼ í•¨ìˆ˜
    
    Args:
        data: í†µí•© ë°ì´í„°
        config: ICLVConfig
        measurement_model: ì¸¡ì •ëª¨ë¸
        structural_model: êµ¬ì¡°ëª¨ë¸
        choice_model: ì„ íƒëª¨ë¸
    
    Returns:
        ì¶”ì • ê²°ê³¼
    """
    estimator = SimultaneousEstimator(config)
    return estimator.estimate(data, measurement_model, structural_model, choice_model)

