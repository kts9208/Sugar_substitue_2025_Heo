"""
Simultaneous Estimation for ICLV Models

ICLV ëª¨ë¸ì˜ ë™ì‹œ ì¶”ì • ì—”ì§„ì…ë‹ˆë‹¤.
Apollo íŒ¨í‚¤ì§€ì˜ ë™ì‹œ ì¶”ì • ë°©ë²•ë¡ ì„ Pythonìœ¼ë¡œ êµ¬í˜„í•©ë‹ˆë‹¤.

ì°¸ì¡°:
- King (2022) - Apollo íŒ¨í‚¤ì§€ ì‚¬ìš©
- Train (2009) - Discrete Choice Methods with Simulation
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Tuple, Callable
from scipy import optimize
from scipy.stats import norm, qmc
from scipy.special import logsumexp
import logging
from concurrent.futures import ProcessPoolExecutor
import multiprocessing
import os

from .gradient_calculator import (
    MeasurementGradient,
    StructuralGradient,
    ChoiceGradient,
    JointGradient
)

logger = logging.getLogger(__name__)


# ============================================================================
# ë³‘ë ¬ì²˜ë¦¬ë¥¼ ìœ„í•œ ì „ì—­ í•¨ìˆ˜ (pickle ê°€ëŠ¥)
# ============================================================================

def _compute_individual_likelihood_parallel(args):
    """
    ê°œì¸ë³„ ìš°ë„ ê³„ì‚° (ë³‘ë ¬ì²˜ë¦¬ìš© ì „ì—­ í•¨ìˆ˜)

    Args:
        args: (ind_data_dict, ind_draws, param_dict, config_dict)
            - ind_data_dict: ê°œì¸ ë°ì´í„° (dict í˜•íƒœ)
            - ind_draws: Halton draws
            - param_dict: íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬
            - config_dict: ì„¤ì • ì •ë³´

    Returns:
        ê°œì¸ì˜ ë¡œê·¸ìš°ë„
    """
    # ë³‘ë ¬ í”„ë¡œì„¸ìŠ¤ì—ì„œ ë¶ˆí•„ìš”í•œ ë¡œê·¸ ì–µì œ
    import logging
    logging.getLogger('root').setLevel(logging.CRITICAL)

    from .measurement_equations import OrderedProbitMeasurement
    from .structural_equations import LatentVariableRegression
    from .choice_equations import BinaryProbitChoice
    from .iclv_config import MeasurementConfig, StructuralConfig, ChoiceConfig

    ind_data_dict, ind_draws, param_dict, config_dict = args

    # DataFrame ë³µì›
    ind_data = pd.DataFrame(ind_data_dict)

    # ëª¨ë¸ ì¬ìƒì„± (ê° í”„ë¡œì„¸ìŠ¤ì—ì„œ)
    measurement_config = MeasurementConfig(**config_dict['measurement'])
    structural_config = StructuralConfig(**config_dict['structural'])
    choice_config = ChoiceConfig(**config_dict['choice'])

    measurement_model = OrderedProbitMeasurement(measurement_config)
    structural_model = LatentVariableRegression(structural_config)
    choice_model = BinaryProbitChoice(choice_config)

    # ìš°ë„ ê³„ì‚°
    draw_lls = []

    for j, draw in enumerate(ind_draws):
        # êµ¬ì¡°ëª¨ë¸: LV = Î³*X + Î·
        lv = structural_model.predict(ind_data, param_dict['structural'], draw)

        # ì¸¡ì •ëª¨ë¸ ìš°ë„: P(Indicators|LV)
        ll_measurement = measurement_model.log_likelihood(
            ind_data, lv, param_dict['measurement']
        )

        # Panel Product: ê°œì¸ì˜ ì—¬ëŸ¬ ì„ íƒ ìƒí™©ì— ëŒ€í•œ í™•ë¥ ì„ ê³±í•¨
        choice_set_lls = []
        for idx in range(len(ind_data)):
            ll_choice_t = choice_model.log_likelihood(
                ind_data.iloc[idx:idx+1],
                lv,
                param_dict['choice']
            )
            choice_set_lls.append(ll_choice_t)

        ll_choice = sum(choice_set_lls)

        # êµ¬ì¡°ëª¨ë¸ ìš°ë„: P(LV|X)
        ll_structural = structural_model.log_likelihood(
            ind_data, lv, param_dict['structural'], draw
        )

        # ê²°í•© ë¡œê·¸ìš°ë„
        draw_ll = ll_measurement + ll_choice + ll_structural

        if not np.isfinite(draw_ll):
            draw_ll = -1e10

        draw_lls.append(draw_ll)

    # logsumexpë¥¼ ì‚¬ìš©í•˜ì—¬ í‰ê·  ê³„ì‚°
    person_ll = logsumexp(draw_lls) - np.log(len(draw_lls))

    return person_ll


class HaltonDrawGenerator:
    """
    Halton ì‹œí€€ìŠ¤ ìƒì„±ê¸°
    
    ì¤€ë‚œìˆ˜(Quasi-random) ì‹œí€€ìŠ¤ë¥¼ ìƒì„±í•˜ì—¬ ì‹œë®¬ë ˆì´ì…˜ ì •í™•ë„ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
    ì¼ë°˜ ë‚œìˆ˜ë³´ë‹¤ ê³µê°„ì„ ë” ê· ë“±í•˜ê²Œ ì»¤ë²„í•©ë‹ˆë‹¤.
    
    ì°¸ì¡°: Apollo íŒ¨í‚¤ì§€ì˜ Halton draws
    """
    
    def __init__(self, n_draws: int, n_individuals: int, 
                 scramble: bool = True, seed: Optional[int] = None):
        """
        Args:
            n_draws: ê°œì¸ë‹¹ draw ìˆ˜
            n_individuals: ê°œì¸ ìˆ˜
            scramble: ìŠ¤í¬ë¨ë¸” ì—¬ë¶€ (ê¶Œì¥)
            seed: ë‚œìˆ˜ ì‹œë“œ
        """
        self.n_draws = n_draws
        self.n_individuals = n_individuals
        self.scramble = scramble
        self.seed = seed
        
        self.draws = None
        self._generate_draws()
    
    def _generate_draws(self):
        """Halton ì‹œí€€ìŠ¤ ìƒì„±"""
        logger.info(f"Halton draws ìƒì„±: {self.n_individuals} ê°œì¸ Ã— {self.n_draws} draws")
        
        # scipyì˜ Halton ì‹œí€€ìŠ¤ ìƒì„±ê¸° ì‚¬ìš©
        sampler = qmc.Halton(d=1, scramble=self.scramble, seed=self.seed)
        
        # ê· ë“±ë¶„í¬ [0,1] ìƒ˜í”Œ ìƒì„±
        uniform_draws = sampler.random(n=self.n_individuals * self.n_draws)
        
        # í‘œì¤€ì •ê·œë¶„í¬ë¡œ ë³€í™˜ (ì—­ëˆ„ì ë¶„í¬í•¨ìˆ˜)
        normal_draws = norm.ppf(uniform_draws)
        
        # (n_individuals, n_draws) í˜•íƒœë¡œ ì¬êµ¬ì„±
        self.draws = normal_draws.reshape(self.n_individuals, self.n_draws)
        
        logger.info(f"Halton draws ìƒì„± ì™„ë£Œ: shape={self.draws.shape}")
    
    def get_draws(self) -> np.ndarray:
        """ìƒì„±ëœ draws ë°˜í™˜"""
        return self.draws
    
    def get_draw_for_individual(self, individual_idx: int) -> np.ndarray:
        """íŠ¹ì • ê°œì¸ì˜ draws ë°˜í™˜"""
        return self.draws[individual_idx, :]


class SimultaneousEstimator:
    """
    ICLV ëª¨ë¸ ë™ì‹œ ì¶”ì •ê¸°
    
    ì¸¡ì •ëª¨ë¸, êµ¬ì¡°ëª¨ë¸, ì„ íƒëª¨ë¸ì„ ë™ì‹œì— ì¶”ì •í•©ë‹ˆë‹¤.
    
    ê²°í•© ìš°ë„í•¨ìˆ˜:
    L = âˆáµ¢ âˆ« P(Choice|LV) Ã— P(Indicators|LV) Ã— P(LV|X) dLV
    
    ì‹œë®¬ë ˆì´ì…˜ ê¸°ë°˜ ì¶”ì •:
    L â‰ˆ âˆáµ¢ (1/R) Î£áµ£ P(Choice|LVáµ£) Ã— P(Indicators|LVáµ£) Ã— P(LVáµ£|X)
    """
    
    def __init__(self, config):
        """
        Args:
            config: ICLVConfig ê°ì²´
        """
        self.config = config
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")

        self.halton_generator = None
        self.data = None
        self.results = None

        # ë¡œê·¸ íŒŒì¼ í•¸ë“¤ëŸ¬ (ì¶”ì • ì‹œì‘ ì‹œ ì„¤ì •)
        self.log_file_handler = None
        self.iteration_logger = None

        # Gradient calculators (Apollo ë°©ì‹)
        self.measurement_grad = None
        self.structural_grad = None
        self.choice_grad = None
        self.joint_grad = None
        self.use_analytic_gradient = False  # ê¸°ë³¸ê°’: ìˆ˜ì¹˜ì  ê·¸ë˜ë””ì–¸íŠ¸

    def _setup_iteration_logger(self, log_file_path: str):
        """
        ë°˜ë³µ ê³¼ì • ë¡œê¹…ì„ ìœ„í•œ íŒŒì¼ í•¸ë“¤ëŸ¬ ì„¤ì •

        Args:
            log_file_path: ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
        """
        # ë°˜ë³µ ê³¼ì • ì „ìš© ë¡œê±° ìƒì„±
        self.iteration_logger = logging.getLogger('iclv_iteration')
        self.iteration_logger.setLevel(logging.INFO)

        # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±° (ì¤‘ë³µ ë°©ì§€)
        self.iteration_logger.handlers.clear()

        # íŒŒì¼ í•¸ë“¤ëŸ¬ ì¶”ê°€
        self.log_file_handler = logging.FileHandler(log_file_path, mode='w', encoding='utf-8')
        self.log_file_handler.setLevel(logging.INFO)

        # í¬ë§· ì„¤ì •
        formatter = logging.Formatter('%(asctime)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
        self.log_file_handler.setFormatter(formatter)

        self.iteration_logger.addHandler(self.log_file_handler)

        # ì½˜ì†” í•¸ë“¤ëŸ¬ë„ ì¶”ê°€ (ì„ íƒì )
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        console_handler.setFormatter(formatter)
        self.iteration_logger.addHandler(console_handler)

        self.iteration_logger.info("="*70)
        self.iteration_logger.info("ICLV ëª¨ë¸ ì¶”ì • ì‹œì‘")
        self.iteration_logger.info("="*70)

    def _close_iteration_logger(self):
        """ë°˜ë³µ ê³¼ì • ë¡œê±° ì¢…ë£Œ"""
        if self.log_file_handler:
            self.iteration_logger.removeHandler(self.log_file_handler)
            self.log_file_handler.close()
            self.log_file_handler = None
    
    def estimate(self, data: pd.DataFrame,
                measurement_model,
                structural_model,
                choice_model,
                log_file: Optional[str] = None) -> Dict:
        """
        ICLV ëª¨ë¸ ë™ì‹œ ì¶”ì •

        Args:
            data: í†µí•© ë°ì´í„°
            measurement_model: ì¸¡ì •ëª¨ë¸ ê°ì²´
            structural_model: êµ¬ì¡°ëª¨ë¸ ê°ì²´
            choice_model: ì„ íƒëª¨ë¸ ê°ì²´
            log_file: ë¡œê·¸ íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ìë™ ìƒì„±)

        Returns:
            ì¶”ì • ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        # ë¡œê·¸ íŒŒì¼ ì„¤ì •
        if log_file is None:
            from pathlib import Path
            results_dir = Path('results')
            results_dir.mkdir(exist_ok=True)
            log_file = results_dir / 'iclv_estimation_log.txt'

        self._setup_iteration_logger(str(log_file))

        self.iteration_logger.info("SimultaneousEstimator.estimate() ì‹œì‘")
        self.logger.info("ICLV ëª¨ë¸ ë™ì‹œ ì¶”ì • ì‹œì‘")

        self.data = data
        n_individuals = data[self.config.individual_id_column].nunique()

        self.iteration_logger.info(f"ë°ì´í„° shape: {data.shape}")
        self.iteration_logger.info(f"ê°œì¸ ìˆ˜: {n_individuals}")
        self.logger.info(f"ê°œì¸ ìˆ˜: {n_individuals}")

        # Halton draws ìƒì„±
        self.iteration_logger.info(f"Halton draws ìƒì„± ì‹œì‘... (n_draws={self.config.estimation.n_draws}, n_individuals={n_individuals})")
        self.logger.info(f"Halton draws ìƒì„± ì¤‘... (n_draws={self.config.estimation.n_draws})")
        self.halton_generator = HaltonDrawGenerator(
            n_draws=self.config.estimation.n_draws,
            n_individuals=n_individuals,
            scramble=self.config.estimation.scramble_halton
        )
        self.iteration_logger.info("Halton draws ìƒì„± ì™„ë£Œ")
        self.logger.info("Halton draws ìƒì„± ì™„ë£Œ")

        # Gradient calculators ì´ˆê¸°í™” (Apollo ë°©ì‹)
        use_gradient = self.config.estimation.optimizer in ['BFGS', 'L-BFGS-B']
        if use_gradient and hasattr(self.config.estimation, 'use_analytic_gradient'):
            self.use_analytic_gradient = self.config.estimation.use_analytic_gradient
        else:
            self.use_analytic_gradient = False

        if self.use_analytic_gradient:
            self.iteration_logger.info("Analytic gradient calculators ì´ˆê¸°í™” (Apollo ë°©ì‹)...")
            self.measurement_grad = MeasurementGradient(
                n_indicators=len(self.config.measurement.indicators),
                n_categories=self.config.measurement.n_categories
            )
            self.structural_grad = StructuralGradient(
                n_sociodem=len(self.config.structural.sociodemographics),
                error_variance=1.0
            )
            self.choice_grad = ChoiceGradient(
                n_attributes=len(self.config.choice.choice_attributes)
            )
            self.joint_grad = JointGradient(
                self.measurement_grad,
                self.structural_grad,
                self.choice_grad
            )
            self.iteration_logger.info("Analytic gradient calculators ì´ˆê¸°í™” ì™„ë£Œ")

        # ì´ˆê¸° íŒŒë¼ë¯¸í„° ì„¤ì •
        self.iteration_logger.info("ì´ˆê¸° íŒŒë¼ë¯¸í„° ì„¤ì • ì‹œì‘...")
        self.logger.info("ì´ˆê¸° íŒŒë¼ë¯¸í„° ì„¤ì • ì¤‘...")
        initial_params = self._get_initial_parameters(
            measurement_model, structural_model, choice_model
        )
        self.iteration_logger.info(f"ì´ˆê¸° íŒŒë¼ë¯¸í„° ì„¤ì • ì™„ë£Œ (ì´ {len(initial_params)}ê°œ)")
        self.logger.info(f"ì´ˆê¸° íŒŒë¼ë¯¸í„° ì„¤ì • ì™„ë£Œ (ì´ {len(initial_params)}ê°œ)")

        # ê²°í•© ìš°ë„í•¨ìˆ˜ ì •ì˜ (gradient check ë¡œê¹… ì¶”ê°€)
        iteration_count = [0]  # Mutable counter
        best_ll = [-np.inf]  # Track best log-likelihood

        def negative_log_likelihood(params):
            iteration_count[0] += 1
            ll = self._joint_log_likelihood(
                params, measurement_model, structural_model, choice_model
            )

            # Track best value
            if ll > best_ll[0]:
                best_ll[0] = ll
                improvement = "[NEW BEST]"
            else:
                improvement = ""

            # Log every iteration with more detail
            # ê°œì¸ ìˆ˜ì— ë”°ë¼ ë¡œê¹… ë¹ˆë„ ì¡°ì •
            n_individuals = self.data[self.config.individual_id_column].nunique()
            log_interval = 10 if n_individuals > 100 else 5

            if iteration_count[0] % log_interval == 0 or improvement:
                log_msg = (
                    f"Iter {iteration_count[0]:4d}: LL = {ll:12.4f} "
                    f"(Best: {best_ll[0]:12.4f}) {improvement}"
                )
                self.iteration_logger.info(log_msg)

            return -ll

        # Get parameter bounds
        self.iteration_logger.info("íŒŒë¼ë¯¸í„° bounds ê³„ì‚° ì‹œì‘...")
        bounds = self._get_parameter_bounds(
            measurement_model, structural_model, choice_model
        )
        self.iteration_logger.info(f"íŒŒë¼ë¯¸í„° bounds ê³„ì‚° ì™„ë£Œ (ì´ {len(bounds)}ê°œ)")

        # ìµœì í™” ë°©ë²• ì„ íƒ
        use_gradient = self.config.estimation.optimizer in ['BFGS', 'L-BFGS-B']

        # Gradient í•¨ìˆ˜ ì •ì˜ (Apollo ë°©ì‹)
        def gradient_function(params):
            """Analytic gradient ê³„ì‚° (Apollo ë°©ì‹)"""
            if not self.use_analytic_gradient:
                return None  # ìˆ˜ì¹˜ì  ê·¸ë˜ë””ì–¸íŠ¸ ì‚¬ìš©

            # íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            param_dict = self._unpack_parameters(
                params, measurement_model, structural_model, choice_model
            )

            # ë³‘ë ¬ì²˜ë¦¬ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
            use_parallel = getattr(self.config.estimation, 'use_parallel', False)
            n_cores = getattr(self.config.estimation, 'n_cores', None)

            # Analytic gradient ê³„ì‚°
            grad_dict = self.joint_grad.compute_gradient(
                data=self.data,
                params_dict=param_dict,
                draws=self.halton_generator.get_draws(),
                individual_id_column=self.config.individual_id_column,
                measurement_model=measurement_model,
                structural_model=structural_model,
                choice_model=choice_model,
                indicators=self.config.measurement.indicators,
                sociodemographics=self.config.structural.sociodemographics,
                choice_attributes=self.config.choice.choice_attributes,
                use_parallel=use_parallel,
                n_cores=n_cores
            )

            # ê·¸ë˜ë””ì–¸íŠ¸ ë²¡í„°ë¡œ ë³€í™˜ (íŒŒë¼ë¯¸í„° ìˆœì„œì™€ ë™ì¼)
            grad_vector = self._pack_gradient(grad_dict, measurement_model, structural_model, choice_model)

            # Negative gradient (minimize -LL)
            return -grad_vector

        print("=" * 70, flush=True)
        if use_gradient:
            print(f"ìµœì í™” ì‹œì‘: {self.config.estimation.optimizer} (gradient-based)", flush=True)
            if self.use_analytic_gradient:
                print("Analytic gradient ì‚¬ìš© (Apollo ë°©ì‹)", flush=True)
            else:
                print("ìˆ˜ì¹˜ì  ê·¸ë˜ë””ì–¸íŠ¸ ì‚¬ìš© (2-point finite difference)", flush=True)
        else:
            print("ìµœì í™” ì‹œì‘: Nelder-Mead (gradient-free)", flush=True)
        print(f"ì´ˆê¸° íŒŒë¼ë¯¸í„° ê°œìˆ˜: {len(initial_params)}", flush=True)
        self.iteration_logger.info(f"ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜: {self.config.estimation.max_iterations}")
        self.iteration_logger.info("=" * 70)

        # ë³‘ë ¬ì²˜ë¦¬ ì„¤ì • ë¡œê¹…
        use_parallel = getattr(self.config.estimation, 'use_parallel', False)
        if use_parallel:
            n_cores = getattr(self.config.estimation, 'n_cores', None)
            if n_cores is None:
                n_cores = max(1, multiprocessing.cpu_count() - 1)
            self.iteration_logger.info(f"ë³‘ë ¬ì²˜ë¦¬ í™œì„±í™”: {n_cores} ì½”ì–´ ì‚¬ìš©")
        else:
            self.iteration_logger.info("ìˆœì°¨ì²˜ë¦¬ ì‚¬ìš©")

        # ì¡°ê¸° ì¢…ë£Œë¥¼ ìœ„í•œ callback í´ë˜ìŠ¤
        class EarlyStoppingCallback:
            def __init__(self, patience=10, tol=1e-6, logger=None, iteration_logger=None):
                """
                ì¡°ê¸° ì¢…ë£Œ callback

                Args:
                    patience: LL ê°œì„ ì´ ì—†ëŠ” ì—°ì† ë°˜ë³µ íšŸìˆ˜
                    tol: LL ë³€í™” í—ˆìš© ì˜¤ì°¨ (ì ˆëŒ€ê°’)
                """
                self.patience = patience
                self.tol = tol
                self.best_ll = np.inf
                self.no_improvement_count = 0
                self.iteration_count = 0
                self.logger = logger
                self.iteration_logger = iteration_logger

            def __call__(self, xk):
                """ë§¤ ë°˜ë³µë§ˆë‹¤ í˜¸ì¶œë˜ëŠ” í•¨ìˆ˜"""
                self.iteration_count += 1
                current_ll = negative_log_likelihood(xk)

                # LL ê°œì„  ì²´í¬
                if current_ll < self.best_ll - self.tol:
                    # ê°œì„ ë¨
                    self.best_ll = current_ll
                    self.no_improvement_count = 0
                else:
                    # ê°œì„  ì—†ìŒ
                    self.no_improvement_count += 1

                # ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´
                if self.no_improvement_count >= self.patience:
                    msg = f"ì¡°ê¸° ì¢…ë£Œ: {self.patience}íšŒ ì—°ì† LL ê°œì„  ì—†ìŒ (LL={-self.best_ll:.4f})"
                    if self.logger:
                        self.logger.info(msg)
                    if self.iteration_logger:
                        self.iteration_logger.info(msg)
                    raise StopIteration(msg)

        if use_gradient:
            self.logger.info(f"ìµœì í™” ì‹œì‘: {self.config.estimation.optimizer} (gradient-based)")
            self.iteration_logger.info(f"ìµœì í™” ì‹œì‘: {self.config.estimation.optimizer} (gradient-based)")
            if self.use_analytic_gradient:
                self.logger.info("Analytic gradient ì‚¬ìš© (Apollo ë°©ì‹)")
                self.iteration_logger.info("Analytic gradient ì‚¬ìš© (Apollo ë°©ì‹)")
            else:
                self.logger.info("ìˆ˜ì¹˜ì  ê·¸ë˜ë””ì–¸íŠ¸ ì‚¬ìš© (2-point finite difference)")
                self.iteration_logger.info("ìˆ˜ì¹˜ì  ê·¸ë˜ë””ì–¸íŠ¸ ì‚¬ìš© (2-point finite difference)")

            # ì¡°ê¸° ì¢…ë£Œ callback ìƒì„±
            early_stopping = EarlyStoppingCallback(
                patience=10,
                tol=1e-6,
                logger=self.logger,
                iteration_logger=self.iteration_logger
            )

            self.logger.info("ì¡°ê¸° ì¢…ë£Œ í™œì„±í™”: 10íšŒ ì—°ì† LL ê°œì„  ì—†ìœ¼ë©´ ì¢…ë£Œ (tol=1e-6)")
            self.iteration_logger.info("ì¡°ê¸° ì¢…ë£Œ í™œì„±í™”: 10íšŒ ì—°ì† LL ê°œì„  ì—†ìœ¼ë©´ ì¢…ë£Œ (tol=1e-6)")

            # BFGS ë˜ëŠ” L-BFGS-B
            try:
                result = optimize.minimize(
                    negative_log_likelihood,
                    initial_params,
                    method=self.config.estimation.optimizer,
                    jac=gradient_function if self.use_analytic_gradient else '2-point',
                    bounds=bounds if self.config.estimation.optimizer == 'L-BFGS-B' else None,
                    callback=early_stopping,
                    options={
                        'maxiter': self.config.estimation.max_iterations,
                        'ftol': 1e-6,
                        'gtol': 1e-5,
                        'disp': True
                    }
                )
            except StopIteration as e:
                # ì¡°ê¸° ì¢…ë£Œëœ ê²½ìš° - ìµœì  íŒŒë¼ë¯¸í„°ë¡œ result ê°ì²´ ìƒì„±
                from scipy.optimize import OptimizeResult

                # best_paramsë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì¢… íŒŒë¼ë¯¸í„° ë²¡í„° ìƒì„±
                final_params_vector = params_to_vector(best_params)

                result = OptimizeResult(
                    x=final_params_vector,
                    success=True,
                    message=f"Early stopping: {str(e)}",
                    fun=best_ll,
                    nit=early_stopping.iteration_count,
                    nfev=early_stopping.iteration_count
                )

                self.logger.info(f"ì¡°ê¸° ì¢…ë£Œ ì™„ë£Œ: ë°˜ë³µ {early_stopping.iteration_count}íšŒ, LL={-best_ll:.4f}")
                self.iteration_logger.info(f"ì¡°ê¸° ì¢…ë£Œ ì™„ë£Œ: ë°˜ë³µ {early_stopping.iteration_count}íšŒ, LL={-best_ll:.4f}")
        else:
            self.logger.info(f"ìµœì í™” ì‹œì‘: Nelder-Mead (gradient-free)")
            self.iteration_logger.info(f"ìµœì í™” ì‹œì‘: Nelder-Mead (gradient-free)")

            result = optimize.minimize(
                negative_log_likelihood,
                initial_params,
                method='Nelder-Mead',
                options={
                    'maxiter': self.config.estimation.max_iterations,
                    'xatol': 1e-4,
                    'fatol': 1e-4,
                    'disp': True
                }
            )

        if result.success:
            self.logger.info("ìµœì í™” ì„±ê³µ")
            self.iteration_logger.info("ìµœì í™” ì„±ê³µ")
        else:
            self.logger.warning(f"ìµœì í™” ì‹¤íŒ¨: {result.message}")
            self.iteration_logger.warning(f"ìµœì í™” ì‹¤íŒ¨: {result.message}")

        self.iteration_logger.info("=" * 70)
        self.iteration_logger.info(f"ìµœì¢… ë¡œê·¸ìš°ë„: {-result.fun:.4f}")
        self.iteration_logger.info(f"ë°˜ë³µ íšŸìˆ˜: {iteration_count[0]}")
        self.iteration_logger.info("=" * 70)

        # ê²°ê³¼ ì²˜ë¦¬
        self.results = self._process_results(
            result, measurement_model, structural_model, choice_model
        )

        # ë¡œê±° ì¢…ë£Œ
        self._close_iteration_logger()

        return self.results
    
    def _compute_individual_likelihood(self, ind_id, ind_data, ind_draws,
                                       param_dict, measurement_model,
                                       structural_model, choice_model) -> float:
        """
        ê°œì¸ë³„ ìš°ë„ ê³„ì‚° (ë³‘ë ¬í™” ê°€ëŠ¥)

        Args:
            ind_id: ê°œì¸ ID
            ind_data: ê°œì¸ ë°ì´í„°
            ind_draws: ê°œì¸ì˜ Halton draws
            param_dict: íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬
            measurement_model: ì¸¡ì •ëª¨ë¸
            structural_model: êµ¬ì¡°ëª¨ë¸
            choice_model: ì„ íƒëª¨ë¸

        Returns:
            ê°œì¸ì˜ ë¡œê·¸ìš°ë„
        """
        draw_lls = []

        for j, draw in enumerate(ind_draws):
            # êµ¬ì¡°ëª¨ë¸: LV = Î³*X + Î·
            lv = structural_model.predict(ind_data, param_dict['structural'], draw)

            # ì¸¡ì •ëª¨ë¸ ìš°ë„: P(Indicators|LV)
            ll_measurement = measurement_model.log_likelihood(
                ind_data, lv, param_dict['measurement']
            )

            # Panel Product: ê°œì¸ì˜ ì—¬ëŸ¬ ì„ íƒ ìƒí™©ì— ëŒ€í•œ í™•ë¥ ì„ ê³±í•¨
            choice_set_lls = []
            for idx in range(len(ind_data)):
                ll_choice_t = choice_model.log_likelihood(
                    ind_data.iloc[idx:idx+1],  # ê° ì„ íƒ ìƒí™©
                    lv,
                    param_dict['choice']
                )
                choice_set_lls.append(ll_choice_t)

            # Panel product: log(P1 * P2 * ... * PT) = log(P1) + log(P2) + ... + log(PT)
            ll_choice = sum(choice_set_lls)

            # êµ¬ì¡°ëª¨ë¸ ìš°ë„: P(LV|X) - ì •ê·œë¶„í¬ ê°€ì •
            ll_structural = structural_model.log_likelihood(
                ind_data, lv, param_dict['structural'], draw
            )

            # ê²°í•© ë¡œê·¸ìš°ë„
            draw_ll = ll_measurement + ll_choice + ll_structural

            # ğŸ”´ ìˆ˜ì •: -infë¥¼ ë§¤ìš° ì‘ì€ ê°’ìœ¼ë¡œ ëŒ€ì²´ (ì—°ì†ì„± í™•ë³´ for gradient)
            if not np.isfinite(draw_ll):
                draw_ll = -1e10  # -inf ëŒ€ì‹  ë§¤ìš° ì‘ì€ ê°’

            draw_lls.append(draw_ll)

        # ğŸ”´ ìˆ˜ì •: logsumexpë¥¼ ì‚¬ìš©í•˜ì—¬ í‰ê·  ê³„ì‚°
        # log[(1/R) Î£áµ£ exp(ll_r)] = logsumexp(ll_r) - log(R)
        person_ll = logsumexp(draw_lls) - np.log(len(draw_lls))

        return person_ll

    def _joint_log_likelihood(self, params: np.ndarray,
                             measurement_model,
                             structural_model,
                             choice_model) -> float:
        """
        ê²°í•© ë¡œê·¸ìš°ë„ ê³„ì‚°

        ì‹œë®¬ë ˆì´ì…˜ ê¸°ë°˜:
        log L â‰ˆ Î£áµ¢ log[(1/R) Î£áµ£ P(Choice|LVáµ£) Ã— P(Indicators|LVáµ£) Ã— P(LVáµ£|X)]
        """
        # íŒŒë¼ë¯¸í„° ë¶„í•´
        param_dict = self._unpack_parameters(
            params, measurement_model, structural_model, choice_model
        )

        draws = self.halton_generator.get_draws()
        individual_ids = self.data[self.config.individual_id_column].unique()

        # ë³‘ë ¬ì²˜ë¦¬ ì—¬ë¶€ í™•ì¸
        use_parallel = getattr(self.config.estimation, 'use_parallel', False)

        if use_parallel:
            # ë³‘ë ¬ì²˜ë¦¬ ì‚¬ìš© (ì „ì—­ í•¨ìˆ˜ ì‚¬ìš©)
            n_cores = getattr(self.config.estimation, 'n_cores', None)
            if n_cores is None:
                n_cores = max(1, multiprocessing.cpu_count() - 1)

            # ì„¤ì • ì •ë³´ë¥¼ dictë¡œ ë³€í™˜ (pickle ê°€ëŠ¥)
            config_dict = {
                'measurement': {
                    'latent_variable': self.config.measurement.latent_variable,
                    'indicators': self.config.measurement.indicators,
                    'n_categories': self.config.measurement.n_categories
                },
                'structural': {
                    'sociodemographics': self.config.structural.sociodemographics,
                    'error_variance': self.config.structural.error_variance
                },
                'choice': {
                    'choice_attributes': self.config.choice.choice_attributes
                }
            }

            # ê°œì¸ë³„ ë°ì´í„° ì¤€ë¹„ (dict í˜•íƒœë¡œ ë³€í™˜)
            args_list = []
            for i, ind_id in enumerate(individual_ids):
                ind_data = self.data[self.data[self.config.individual_id_column] == ind_id]
                ind_data_dict = ind_data.to_dict('list')  # pickle ê°€ëŠ¥í•œ dictë¡œ ë³€í™˜
                ind_draws = draws[i, :]
                args_list.append((ind_data_dict, ind_draws, param_dict, config_dict))

            # ë³‘ë ¬ ê³„ì‚°
            with ProcessPoolExecutor(max_workers=n_cores) as executor:
                person_lls = list(executor.map(_compute_individual_likelihood_parallel, args_list))

            total_ll = sum(person_lls)
        else:
            # ìˆœì°¨ì²˜ë¦¬
            total_ll = 0.0
            for i, ind_id in enumerate(individual_ids):
                ind_data = self.data[self.data[self.config.individual_id_column] == ind_id]
                ind_draws = draws[i, :]

                person_ll = self._compute_individual_likelihood(
                    ind_id, ind_data, ind_draws, param_dict,
                    measurement_model, structural_model, choice_model
                )
                total_ll += person_ll

        return total_ll

    def _get_parameter_bounds(self, measurement_model,
                              structural_model, choice_model) -> list:
        """
        Parameter bounds for L-BFGS-B

        Returns:
            bounds: [(lower, upper), ...] list
        """
        bounds = []

        # Measurement model parameters
        # - Factor loadings (zeta): [0.1, 10]
        n_indicators = len(self.config.measurement.indicators)
        bounds.extend([(0.1, 10.0)] * n_indicators)

        # - Thresholds (tau): [-10, 10]
        n_thresholds = self.config.measurement.n_categories - 1
        for _ in range(n_indicators):
            bounds.extend([(-10.0, 10.0)] * n_thresholds)

        # Structural model parameters (gamma): unbounded
        n_sociodem = len(self.config.structural.sociodemographics)
        bounds.extend([(None, None)] * n_sociodem)

        # Choice model parameters
        # - Intercept: unbounded
        bounds.append((None, None))

        # - Attribute coefficients (beta): unbounded
        n_attributes = len(self.config.choice.choice_attributes)
        bounds.extend([(None, None)] * n_attributes)

        # - Latent variable coefficient (lambda): unbounded
        bounds.append((None, None))

        # - Sociodemographic coefficients: unbounded
        if self.config.structural.include_in_choice:
            bounds.extend([(None, None)] * n_sociodem)

        return bounds

    def _get_initial_parameters(self, measurement_model,
                                structural_model, choice_model) -> np.ndarray:
        """ì´ˆê¸° íŒŒë¼ë¯¸í„° ì„¤ì •"""
        
        params = []
        
        # ì¸¡ì •ëª¨ë¸ íŒŒë¼ë¯¸í„°
        # - ìš”ì¸ì ì¬ëŸ‰ (zeta)
        n_indicators = len(self.config.measurement.indicators)
        params.extend([1.0] * n_indicators)  # zeta
        
        # - ì„ê³„ê°’ (tau)
        n_thresholds = self.config.measurement.n_categories - 1
        for _ in range(n_indicators):
            params.extend([-2, -1, 1, 2])  # 5ì  ì²™ë„ ê¸°ë³¸ê°’
        
        # êµ¬ì¡°ëª¨ë¸ íŒŒë¼ë¯¸í„° (gamma)
        n_sociodem = len(self.config.structural.sociodemographics)
        params.extend([0.0] * n_sociodem)
        
        # ì„ íƒëª¨ë¸ íŒŒë¼ë¯¸í„°
        # - ì ˆí¸
        params.append(0.0)
        
        # - ì†ì„± ê³„ìˆ˜ (beta)
        n_attributes = len(self.config.choice.choice_attributes)
        params.extend([0.0] * n_attributes)
        
        # - ì ì¬ë³€ìˆ˜ ê³„ìˆ˜ (lambda)
        params.append(1.0)
        
        # - ì‚¬íšŒì¸êµ¬í•™ì  ë³€ìˆ˜ ê³„ìˆ˜ (ì„ íƒëª¨ë¸ì— í¬í•¨ë˜ëŠ” ê²½ìš°)
        if self.config.structural.include_in_choice:
            params.extend([0.0] * n_sociodem)
        
        return np.array(params)
    

    
    def _get_parameter_bounds(self, measurement_model,
                              structural_model, choice_model) -> list:
        """
        Parameter bounds for L-BFGS-B
        
        Returns:
            bounds: [(lower, upper), ...] list
        """
        bounds = []
        
        # Measurement model parameters
        # - Factor loadings (zeta): [0.1, 10]
        n_indicators = len(self.config.measurement.indicators)
        bounds.extend([(0.1, 10.0)] * n_indicators)
        
        # - Thresholds (tau): [-10, 10]
        n_thresholds = self.config.measurement.n_categories - 1
        for _ in range(n_indicators):
            bounds.extend([(-10.0, 10.0)] * n_thresholds)
        
        # Structural model parameters (gamma): unbounded
        n_sociodem = len(self.config.structural.sociodemographics)
        bounds.extend([(None, None)] * n_sociodem)
        
        # Choice model parameters
        # - Intercept: unbounded
        bounds.append((None, None))
        
        # - Attribute coefficients (beta): unbounded
        n_attributes = len(self.config.choice.choice_attributes)
        bounds.extend([(None, None)] * n_attributes)
        
        # - Latent variable coefficient (lambda): unbounded
        bounds.append((None, None))
        
        # - Sociodemographic coefficients: unbounded
        if self.config.structural.include_in_choice:
            bounds.extend([(None, None)] * n_sociodem)
        
        return bounds
    def _unpack_parameters(self, params: np.ndarray,
                          measurement_model,
                          structural_model,
                          choice_model) -> Dict[str, Dict]:
        """íŒŒë¼ë¯¸í„° ë²¡í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        
        idx = 0
        param_dict = {
            'measurement': {},
            'structural': {},
            'choice': {}
        }
        
        # ì¸¡ì •ëª¨ë¸ íŒŒë¼ë¯¸í„°
        n_indicators = len(self.config.measurement.indicators)
        param_dict['measurement']['zeta'] = params[idx:idx+n_indicators]
        idx += n_indicators

        n_thresholds = self.config.measurement.n_categories - 1
        # tauë¥¼ 2D ë°°ì—´ë¡œ ì €ì¥ (n_indicators, n_thresholds)
        tau_list = []
        for i in range(n_indicators):
            tau_list.append(params[idx:idx+n_thresholds])
            idx += n_thresholds
        param_dict['measurement']['tau'] = np.array(tau_list)
        
        # êµ¬ì¡°ëª¨ë¸ íŒŒë¼ë¯¸í„°
        n_sociodem = len(self.config.structural.sociodemographics)
        param_dict['structural']['gamma'] = params[idx:idx+n_sociodem]
        idx += n_sociodem
        
        # ì„ íƒëª¨ë¸ íŒŒë¼ë¯¸í„°
        param_dict['choice']['intercept'] = params[idx]
        idx += 1
        
        n_attributes = len(self.config.choice.choice_attributes)
        param_dict['choice']['beta'] = params[idx:idx+n_attributes]
        idx += n_attributes
        
        param_dict['choice']['lambda'] = params[idx]
        idx += 1
        
        if self.config.structural.include_in_choice:
            param_dict['choice']['beta_sociodem'] = params[idx:idx+n_sociodem]
            idx += n_sociodem
        
        return param_dict

    def _pack_gradient(self, grad_dict: Dict, measurement_model,
                      structural_model, choice_model) -> np.ndarray:
        """
        ê·¸ë˜ë””ì–¸íŠ¸ ë”•ì…”ë„ˆë¦¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜ (íŒŒë¼ë¯¸í„° ìˆœì„œì™€ ë™ì¼)

        Args:
            grad_dict: ê·¸ë˜ë””ì–¸íŠ¸ ë”•ì…”ë„ˆë¦¬
            measurement_model: ì¸¡ì •ëª¨ë¸
            structural_model: êµ¬ì¡°ëª¨ë¸
            choice_model: ì„ íƒëª¨ë¸

        Returns:
            gradient_vector: ê·¸ë˜ë””ì–¸íŠ¸ ë²¡í„°
        """
        gradient_list = []

        # ì¸¡ì •ëª¨ë¸ ê·¸ë˜ë””ì–¸íŠ¸
        gradient_list.append(grad_dict['grad_zeta'])
        gradient_list.append(grad_dict['grad_tau'].flatten())

        # êµ¬ì¡°ëª¨ë¸ ê·¸ë˜ë””ì–¸íŠ¸
        gradient_list.append(grad_dict['grad_gamma'])

        # ì„ íƒëª¨ë¸ ê·¸ë˜ë””ì–¸íŠ¸
        gradient_list.append(np.array([grad_dict['grad_intercept']]))
        gradient_list.append(grad_dict['grad_beta'])
        gradient_list.append(np.array([grad_dict['grad_lambda']]))

        # ì‚¬íšŒì¸êµ¬í•™ì  ë³€ìˆ˜ê°€ ì„ íƒëª¨ë¸ì— í¬í•¨ë˜ëŠ” ê²½ìš°
        if self.config.structural.include_in_choice:
            # í˜„ì¬ëŠ” êµ¬í˜„ë˜ì§€ ì•ŠìŒ
            n_sociodem = len(self.config.structural.sociodemographics)
            gradient_list.append(np.zeros(n_sociodem))

        # ë²¡í„°ë¡œ ê²°í•©
        gradient_vector = np.concatenate(gradient_list)

        return gradient_vector

    def _process_results(self, optimization_result,
                        measurement_model,
                        structural_model,
                        choice_model) -> Dict:
        """ìµœì í™” ê²°ê³¼ ì²˜ë¦¬"""
        
        param_dict = self._unpack_parameters(
            optimization_result.x,
            measurement_model,
            structural_model,
            choice_model
        )
        
        results = {
            'success': optimization_result.success,
            'message': optimization_result.message,
            'log_likelihood': -optimization_result.fun,
            'n_iterations': optimization_result.nit,
            'parameters': param_dict,
            'raw_params': optimization_result.x,
            
            # ëª¨ë¸ ì í•©ë„
            'n_observations': len(self.data),
            'n_parameters': len(optimization_result.x),
        }
        
        # AIC, BIC ê³„ì‚°
        ll = results['log_likelihood']
        k = results['n_parameters']
        n = results['n_observations']
        
        results['aic'] = -2 * ll + 2 * k
        results['bic'] = -2 * ll + k * np.log(n)
        
        # í‘œì¤€ì˜¤ì°¨ ê³„ì‚° (Hessian ê¸°ë°˜)
        if self.config.estimation.calculate_se:
            try:
                hessian = optimization_result.hess_inv
                if hasattr(hessian, 'todense'):
                    hessian = hessian.todense()
                
                se = np.sqrt(np.diag(hessian))
                results['standard_errors'] = se
                
                # t-í†µê³„ëŸ‰
                results['t_statistics'] = optimization_result.x / se
                
                # p-ê°’
                from scipy.stats import t
                results['p_values'] = 2 * (1 - t.cdf(np.abs(results['t_statistics']), n - k))
                
            except Exception as e:
                self.logger.warning(f"í‘œì¤€ì˜¤ì°¨ ê³„ì‚° ì‹¤íŒ¨: {e}")
        
        return results


def estimate_iclv_simultaneous(data: pd.DataFrame, config,
                               measurement_model,
                               structural_model,
                               choice_model) -> Dict:
    """
    ICLV ëª¨ë¸ ë™ì‹œ ì¶”ì • í—¬í¼ í•¨ìˆ˜
    
    Args:
        data: í†µí•© ë°ì´í„°
        config: ICLVConfig
        measurement_model: ì¸¡ì •ëª¨ë¸
        structural_model: êµ¬ì¡°ëª¨ë¸
        choice_model: ì„ íƒëª¨ë¸
    
    Returns:
        ì¶”ì • ê²°ê³¼
    """
    estimator = SimultaneousEstimator(config)
    return estimator.estimate(data, measurement_model, structural_model, choice_model)

