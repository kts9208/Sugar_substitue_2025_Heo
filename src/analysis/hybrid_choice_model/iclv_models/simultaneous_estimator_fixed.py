"""
Simultaneous Estimation for ICLV Models

ICLV ëª¨ë¸ì˜ ë™ì‹œ ì¶”ì • ì—”ì§„ì…ë‹ˆë‹¤.
Apollo íŒ¨í‚¤ì§€ì˜ ë™ì‹œ ì¶”ì • ë°©ë²•ë¡ ì„ Pythonìœ¼ë¡œ êµ¬í˜„í•©ë‹ˆë‹¤.

ì°¸ì¡°:
- King (2022) - Apollo íŒ¨í‚¤ì§€ ì‚¬ìš©
- Train (2009) - Discrete Choice Methods with Simulation
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Tuple, Callable
from scipy import optimize
from scipy.stats import norm, qmc
from scipy.special import logsumexp
import logging
from concurrent.futures import ProcessPoolExecutor
import multiprocessing
import os

from .gradient_calculator import (
    MeasurementGradient,
    StructuralGradient,
    ChoiceGradient,
    JointGradient
)

logger = logging.getLogger(__name__)


# ============================================================================
# ë³‘ë ¬ì²˜ë¦¬ë¥¼ ìœ„í•œ ì „ì—­ í•¨ìˆ˜ (pickle ê°€ëŠ¥)
# ============================================================================

def _compute_individual_likelihood_parallel(args):
    """
    ê°œì¸ë³„ ìš°ë„ ê³„ì‚° (ë³‘ë ¬ì²˜ë¦¬ìš© ì „ì—­ í•¨ìˆ˜)

    Args:
        args: (ind_data_dict, ind_draws, param_dict, config_dict)
            - ind_data_dict: ê°œì¸ ë°ì´í„° (dict í˜•íƒœ)
            - ind_draws: Halton draws
            - param_dict: íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬
            - config_dict: ì„¤ì • ì •ë³´

    Returns:
        ê°œì¸ì˜ ë¡œê·¸ìš°ë„
    """
    # ë³‘ë ¬ í”„ë¡œì„¸ìŠ¤ì—ì„œ ë¶ˆí•„ìš”í•œ ë¡œê·¸ ì–µì œ
    import logging
    logging.getLogger('root').setLevel(logging.CRITICAL)

    from .measurement_equations import OrderedProbitMeasurement
    from .structural_equations import LatentVariableRegression
    from .choice_equations import BinaryProbitChoice
    from .iclv_config import MeasurementConfig, StructuralConfig, ChoiceConfig

    ind_data_dict, ind_draws, param_dict, config_dict = args

    # DataFrame ë³µì›
    ind_data = pd.DataFrame(ind_data_dict)

    # ëª¨ë¸ ì¬ìƒì„± (ê° í”„ë¡œì„¸ìŠ¤ì—ì„œ)
    measurement_config = MeasurementConfig(**config_dict['measurement'])
    structural_config = StructuralConfig(**config_dict['structural'])
    choice_config = ChoiceConfig(**config_dict['choice'])

    measurement_model = OrderedProbitMeasurement(measurement_config)
    structural_model = LatentVariableRegression(structural_config)
    choice_model = BinaryProbitChoice(choice_config)

    # ìš°ë„ ê³„ì‚°
    draw_lls = []

    for j, draw in enumerate(ind_draws):
        # êµ¬ì¡°ëª¨ë¸: LV = Î³*X + Î·
        lv = structural_model.predict(ind_data, param_dict['structural'], draw)

        # ì¸¡ì •ëª¨ë¸ ìš°ë„: P(Indicators|LV)
        ll_measurement = measurement_model.log_likelihood(
            ind_data, lv, param_dict['measurement']
        )

        # Panel Product: ê°œì¸ì˜ ì—¬ëŸ¬ ì„ íƒ ìƒí™©ì— ëŒ€í•œ í™•ë¥ ì„ ê³±í•¨
        choice_set_lls = []
        for idx in range(len(ind_data)):
            ll_choice_t = choice_model.log_likelihood(
                ind_data.iloc[idx:idx+1],
                lv,
                param_dict['choice']
            )
            choice_set_lls.append(ll_choice_t)

        ll_choice = sum(choice_set_lls)

        # êµ¬ì¡°ëª¨ë¸ ìš°ë„: P(LV|X)
        ll_structural = structural_model.log_likelihood(
            ind_data, lv, param_dict['structural'], draw
        )

        # ê²°í•© ë¡œê·¸ìš°ë„
        draw_ll = ll_measurement + ll_choice + ll_structural

        if not np.isfinite(draw_ll):
            draw_ll = -1e10

        draw_lls.append(draw_ll)

    # logsumexpë¥¼ ì‚¬ìš©í•˜ì—¬ í‰ê·  ê³„ì‚°
    person_ll = logsumexp(draw_lls) - np.log(len(draw_lls))

    return person_ll


class HaltonDrawGenerator:
    """
    Halton ì‹œí€€ìŠ¤ ìƒì„±ê¸°
    
    ì¤€ë‚œìˆ˜(Quasi-random) ì‹œí€€ìŠ¤ë¥¼ ìƒì„±í•˜ì—¬ ì‹œë®¬ë ˆì´ì…˜ ì •í™•ë„ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
    ì¼ë°˜ ë‚œìˆ˜ë³´ë‹¤ ê³µê°„ì„ ë” ê· ë“±í•˜ê²Œ ì»¤ë²„í•©ë‹ˆë‹¤.
    
    ì°¸ì¡°: Apollo íŒ¨í‚¤ì§€ì˜ Halton draws
    """
    
    def __init__(self, n_draws: int, n_individuals: int, 
                 scramble: bool = True, seed: Optional[int] = None):
        """
        Args:
            n_draws: ê°œì¸ë‹¹ draw ìˆ˜
            n_individuals: ê°œì¸ ìˆ˜
            scramble: ìŠ¤í¬ë¨ë¸” ì—¬ë¶€ (ê¶Œì¥)
            seed: ë‚œìˆ˜ ì‹œë“œ
        """
        self.n_draws = n_draws
        self.n_individuals = n_individuals
        self.scramble = scramble
        self.seed = seed
        
        self.draws = None
        self._generate_draws()
    
    def _generate_draws(self):
        """Halton ì‹œí€€ìŠ¤ ìƒì„±"""
        logger.info(f"Halton draws ìƒì„±: {self.n_individuals} ê°œì¸ Ã— {self.n_draws} draws")
        
        # scipyì˜ Halton ì‹œí€€ìŠ¤ ìƒì„±ê¸° ì‚¬ìš©
        sampler = qmc.Halton(d=1, scramble=self.scramble, seed=self.seed)
        
        # ê· ë“±ë¶„í¬ [0,1] ìƒ˜í”Œ ìƒì„±
        uniform_draws = sampler.random(n=self.n_individuals * self.n_draws)
        
        # í‘œì¤€ì •ê·œë¶„í¬ë¡œ ë³€í™˜ (ì—­ëˆ„ì ë¶„í¬í•¨ìˆ˜)
        normal_draws = norm.ppf(uniform_draws)
        
        # (n_individuals, n_draws) í˜•íƒœë¡œ ì¬êµ¬ì„±
        self.draws = normal_draws.reshape(self.n_individuals, self.n_draws)
        
        logger.info(f"Halton draws ìƒì„± ì™„ë£Œ: shape={self.draws.shape}")
    
    def get_draws(self) -> np.ndarray:
        """ìƒì„±ëœ draws ë°˜í™˜"""
        return self.draws
    
    def get_draw_for_individual(self, individual_idx: int) -> np.ndarray:
        """íŠ¹ì • ê°œì¸ì˜ draws ë°˜í™˜"""
        return self.draws[individual_idx, :]


class SimultaneousEstimator:
    """
    ICLV ëª¨ë¸ ë™ì‹œ ì¶”ì •ê¸°
    
    ì¸¡ì •ëª¨ë¸, êµ¬ì¡°ëª¨ë¸, ì„ íƒëª¨ë¸ì„ ë™ì‹œì— ì¶”ì •í•©ë‹ˆë‹¤.
    
    ê²°í•© ìš°ë„í•¨ìˆ˜:
    L = âˆáµ¢ âˆ« P(Choice|LV) Ã— P(Indicators|LV) Ã— P(LV|X) dLV
    
    ì‹œë®¬ë ˆì´ì…˜ ê¸°ë°˜ ì¶”ì •:
    L â‰ˆ âˆáµ¢ (1/R) Î£áµ£ P(Choice|LVáµ£) Ã— P(Indicators|LVáµ£) Ã— P(LVáµ£|X)
    """
    
    def __init__(self, config):
        """
        Args:
            config: ICLVConfig ê°ì²´
        """
        self.config = config
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")

        self.halton_generator = None
        self.data = None
        self.results = None

        # ë¡œê·¸ íŒŒì¼ í•¸ë“¤ëŸ¬ (ì¶”ì • ì‹œì‘ ì‹œ ì„¤ì •)
        self.log_file_handler = None
        self.iteration_logger = None

        # Gradient calculators (Apollo ë°©ì‹)
        self.measurement_grad = None
        self.structural_grad = None
        self.choice_grad = None
        self.joint_grad = None
        self.use_analytic_gradient = False  # ê¸°ë³¸ê°’: ìˆ˜ì¹˜ì  ê·¸ë˜ë””ì–¸íŠ¸

    def _setup_iteration_logger(self, log_file_path: str):
        """
        ë°˜ë³µ ê³¼ì • ë¡œê¹…ì„ ìœ„í•œ íŒŒì¼ í•¸ë“¤ëŸ¬ ì„¤ì •

        Args:
            log_file_path: ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
        """
        # ë°˜ë³µ ê³¼ì • ì „ìš© ë¡œê±° ìƒì„±
        self.iteration_logger = logging.getLogger('iclv_iteration')
        self.iteration_logger.setLevel(logging.INFO)

        # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±° (ì¤‘ë³µ ë°©ì§€)
        self.iteration_logger.handlers.clear()

        # íŒŒì¼ í•¸ë“¤ëŸ¬ ì¶”ê°€
        self.log_file_handler = logging.FileHandler(log_file_path, mode='w', encoding='utf-8')
        self.log_file_handler.setLevel(logging.INFO)

        # í¬ë§· ì„¤ì •
        formatter = logging.Formatter('%(asctime)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
        self.log_file_handler.setFormatter(formatter)

        self.iteration_logger.addHandler(self.log_file_handler)

        # ì½˜ì†” í•¸ë“¤ëŸ¬ ì œê±° (ì¤‘ë³µ ë°©ì§€ - íŒŒì¼ë§Œ ì‚¬ìš©)
        # console_handler = logging.StreamHandler()
        # console_handler.setLevel(logging.INFO)
        # console_handler.setFormatter(formatter)
        # self.iteration_logger.addHandler(console_handler)

        self.iteration_logger.info("="*70)
        self.iteration_logger.info("ICLV ëª¨ë¸ ì¶”ì • ì‹œì‘")
        self.iteration_logger.info("="*70)

    def _close_iteration_logger(self):
        """ë°˜ë³µ ê³¼ì • ë¡œê±° ì¢…ë£Œ"""
        if self.log_file_handler:
            self.iteration_logger.removeHandler(self.log_file_handler)
            self.log_file_handler.close()
            self.log_file_handler = None
    
    def estimate(self, data: pd.DataFrame,
                measurement_model,
                structural_model,
                choice_model,
                log_file: Optional[str] = None) -> Dict:
        """
        ICLV ëª¨ë¸ ë™ì‹œ ì¶”ì •

        Args:
            data: í†µí•© ë°ì´í„°
            measurement_model: ì¸¡ì •ëª¨ë¸ ê°ì²´
            structural_model: êµ¬ì¡°ëª¨ë¸ ê°ì²´
            choice_model: ì„ íƒëª¨ë¸ ê°ì²´
            log_file: ë¡œê·¸ íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ìë™ ìƒì„±)

        Returns:
            ì¶”ì • ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        # ë¡œê·¸ íŒŒì¼ ì„¤ì •
        if log_file is None:
            from pathlib import Path
            results_dir = Path('results')
            results_dir.mkdir(exist_ok=True)
            log_file = results_dir / 'iclv_estimation_log.txt'

        self._setup_iteration_logger(str(log_file))

        self.iteration_logger.info("SimultaneousEstimator.estimate() ì‹œì‘")
        self.logger.info("ICLV ëª¨ë¸ ë™ì‹œ ì¶”ì • ì‹œì‘")

        # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ì˜ loggerë¥¼ iteration_loggerë¡œ ì—…ë°ì´íŠ¸
        if hasattr(self, 'memory_monitor') and self.memory_monitor is not None:
            self.memory_monitor.logger = self.iteration_logger

        self.data = data
        n_individuals = data[self.config.individual_id_column].nunique()

        self.iteration_logger.info(f"ë°ì´í„° shape: {data.shape}")
        self.iteration_logger.info(f"ê°œì¸ ìˆ˜: {n_individuals}")

        # Halton draws ìƒì„± (ì´ë¯¸ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ ê±´ë„ˆë›°ê¸°)
        if not hasattr(self, 'halton_generator') or self.halton_generator is None:
            self.iteration_logger.info(f"Halton draws ìƒì„± ì‹œì‘... (n_draws={self.config.estimation.n_draws}, n_individuals={n_individuals})")
            self.halton_generator = HaltonDrawGenerator(
                n_draws=self.config.estimation.n_draws,
                n_individuals=n_individuals,
                scramble=self.config.estimation.scramble_halton
            )
            self.iteration_logger.info("Halton draws ìƒì„± ì™„ë£Œ")
        else:
            self.iteration_logger.info("Halton draws ì´ë¯¸ ì„¤ì •ë¨ (ê±´ë„ˆë›°ê¸°)")

        # Gradient calculators ì´ˆê¸°í™” (Apollo ë°©ì‹)
        use_gradient = self.config.estimation.optimizer in ['BFGS', 'L-BFGS-B']
        if use_gradient and hasattr(self.config.estimation, 'use_analytic_gradient'):
            self.use_analytic_gradient = self.config.estimation.use_analytic_gradient
        else:
            self.use_analytic_gradient = False

        if self.use_analytic_gradient:
            self.iteration_logger.info("Analytic gradient calculators ì´ˆê¸°í™” (Apollo ë°©ì‹)...")

            # ë‹¤ì¤‘ ì ì¬ë³€ìˆ˜ ì§€ì› í™•ì¸
            from .multi_latent_config import MultiLatentConfig
            is_multi_latent = isinstance(self.config, MultiLatentConfig)

            if is_multi_latent:
                # ë‹¤ì¤‘ ì ì¬ë³€ìˆ˜: MultiLatentMeasurementGradient ì‚¬ìš©
                from .multi_latent_gradient import MultiLatentMeasurementGradient
                self.measurement_grad = MultiLatentMeasurementGradient(
                    self.config.measurement_configs
                )
                self.iteration_logger.info(f"ë‹¤ì¤‘ ì ì¬ë³€ìˆ˜ ì¸¡ì •ëª¨ë¸ gradient ì´ˆê¸°í™”: {len(self.config.measurement_configs)}ê°œ LV")
            else:
                # ë‹¨ì¼ ì ì¬ë³€ìˆ˜
                self.measurement_grad = MeasurementGradient(
                    n_indicators=len(self.config.measurement.indicators),
                    n_categories=self.config.measurement.n_categories
                )
                self.iteration_logger.info("ë‹¨ì¼ ì ì¬ë³€ìˆ˜ ì¸¡ì •ëª¨ë¸ gradient ì´ˆê¸°í™”")

            # êµ¬ì¡°ëª¨ë¸ gradient
            if is_multi_latent:
                # ë‹¤ì¤‘ ì ì¬ë³€ìˆ˜: MultiLatentStructuralGradient ì‚¬ìš©
                from .multi_latent_gradient import MultiLatentStructuralGradient
                self.structural_grad = MultiLatentStructuralGradient(
                    n_exo=self.config.structural.n_exo,
                    n_cov=self.config.structural.n_cov,
                    error_variance=self.config.structural.error_variance
                )
                self.iteration_logger.info("ë‹¤ì¤‘ ì ì¬ë³€ìˆ˜ êµ¬ì¡°ëª¨ë¸ gradient ì´ˆê¸°í™”")
            else:
                # ë‹¨ì¼ ì ì¬ë³€ìˆ˜
                self.structural_grad = StructuralGradient(
                    n_sociodem=len(self.config.structural.sociodemographics),
                    error_variance=1.0
                )
                self.iteration_logger.info("ë‹¨ì¼ ì ì¬ë³€ìˆ˜ êµ¬ì¡°ëª¨ë¸ gradient ì´ˆê¸°í™”")

            # ì„ íƒëª¨ë¸ gradient (ë‹¤ì¤‘ ì ì¬ë³€ìˆ˜ë„ ë™ì¼)
            self.choice_grad = ChoiceGradient(
                n_attributes=len(self.config.choice.choice_attributes)
            )

            # JointGradient
            if is_multi_latent:
                # ë‹¤ì¤‘ ì ì¬ë³€ìˆ˜: MultiLatentJointGradient ì‚¬ìš©
                from .multi_latent_gradient import MultiLatentJointGradient

                # GPU ì‚¬ìš© ì—¬ë¶€ í™•ì¸
                use_gpu_gradient = False
                gpu_measurement_model = None

                if hasattr(self, 'use_gpu') and self.use_gpu:
                    if hasattr(self, 'gpu_measurement_model') and self.gpu_measurement_model is not None:
                        use_gpu_gradient = True
                        gpu_measurement_model = self.gpu_measurement_model
                        self.iteration_logger.info("GPU ë°°ì¹˜ ê·¸ë˜ë””ì–¸íŠ¸ í™œì„±í™”")

                self.joint_grad = MultiLatentJointGradient(
                    self.measurement_grad,
                    self.structural_grad,
                    self.choice_grad,
                    use_gpu=use_gpu_gradient,
                    gpu_measurement_model=gpu_measurement_model
                )
                self.iteration_logger.info("ë‹¤ì¤‘ ì ì¬ë³€ìˆ˜ JointGradient ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                # ë‹¨ì¼ ì ì¬ë³€ìˆ˜
                self.joint_grad = JointGradient(
                    self.measurement_grad,
                    self.structural_grad,
                    self.choice_grad
                )
                self.iteration_logger.info("ë‹¨ì¼ ì ì¬ë³€ìˆ˜ JointGradient ì´ˆê¸°í™” ì™„ë£Œ")

        # ì´ˆê¸° íŒŒë¼ë¯¸í„° ì„¤ì •
        self.iteration_logger.info("ì´ˆê¸° íŒŒë¼ë¯¸í„° ì„¤ì • ì‹œì‘...")
        initial_params = self._get_initial_parameters(
            measurement_model, structural_model, choice_model
        )
        self.iteration_logger.info(f"ì´ˆê¸° íŒŒë¼ë¯¸í„° ì„¤ì • ì™„ë£Œ (ì´ {len(initial_params)}ê°œ)")

        # ê²°í•© ìš°ë„í•¨ìˆ˜ ì •ì˜ (ë‹¨ê³„ë³„ ë¡œê¹… ì¶”ê°€)
        iteration_count = [0]  # Mutable counter
        best_ll = [-np.inf]  # Track best log-likelihood
        func_call_count = [0]  # í•¨ìˆ˜ í˜¸ì¶œ íšŸìˆ˜ (ìš°ë„ ê³„ì‚°)
        major_iter_count = [0]  # Major iteration ì¹´ìš´í„°
        line_search_call_count = [0]  # Line search ë‚´ í•¨ìˆ˜ í˜¸ì¶œ ì¹´ìš´í„°
        last_major_iter_func_value = [None]  # ë§ˆì§€ë§‰ major iterationì˜ í•¨ìˆ˜ê°’
        current_major_iter_start_call = [0]  # í˜„ì¬ major iteration ì‹œì‘ ì‹œ í•¨ìˆ˜ í˜¸ì¶œ ë²ˆí˜¸
        line_search_func_values = []  # Line search ì¤‘ í•¨ìˆ˜ê°’ ê¸°ë¡
        line_search_start_func_value = [None]  # Line search ì‹œì‘ ì‹œ í•¨ìˆ˜ê°’
        line_search_start_params = [None]  # Line search ì‹œì‘ ì‹œ íŒŒë¼ë¯¸í„°
        line_search_gradient = [None]  # Line search ì‹œì‘ ì‹œ gradient
        line_search_directional_derivative = [None]  # âˆ‡f(x)^TÂ·d (ì‹œì‘ ì‹œ)

        def negative_log_likelihood(params):
            func_call_count[0] += 1

            # Line search ì¤‘ì¸ì§€ íŒë‹¨
            # Major iteration ì‹œì‘ ì§í›„ ì²« í˜¸ì¶œì´ ì•„ë‹ˆë©´ line search ì¤‘
            calls_since_major_start = func_call_count[0] - current_major_iter_start_call[0]

            if calls_since_major_start == 1:
                # Major iteration ì‹œì‘ ì‹œ ì²« í•¨ìˆ˜ í˜¸ì¶œ
                context = f"Major Iteration #{major_iter_count[0] + 1} ì‹œì‘"
                line_search_call_count[0] = 0
                line_search_func_values.clear()
                line_search_start_params[0] = params.copy()
            elif calls_since_major_start > 1:
                # Line search ì¤‘
                line_search_call_count[0] += 1
                context = f"Line Search í•¨ìˆ˜ í˜¸ì¶œ #{line_search_call_count[0]}"
            else:
                # ì´ˆê¸° í˜¸ì¶œ
                context = "ì´ˆê¸° í•¨ìˆ˜ê°’ ê³„ì‚°"

            # ë‹¨ê³„ ë¡œê·¸: ìš°ë„ ê³„ì‚° ì‹œì‘
            self.iteration_logger.info(f"\n[{context}] [ë‹¨ê³„ 1/2] ì „ì²´ ìš°ë„ ê³„ì‚°")

            ll = self._joint_log_likelihood(
                params, measurement_model, structural_model, choice_model
            )

            # Track best value
            if ll > best_ll[0]:
                best_ll[0] = ll
                improvement = "[NEW BEST]"
            else:
                improvement = ""

            # í•¨ìˆ˜ê°’ ì¶œë ¥
            neg_ll = -ll  # scipyê°€ ìµœì†Œí™”í•˜ëŠ” ê°’
            log_msg = f"  LL = {ll:12.4f} (Best: {best_ll[0]:12.4f}) {improvement}"
            self.iteration_logger.info(log_msg)

            # Line search ì¤‘ì´ë©´ í•¨ìˆ˜ê°’ ë³€í™” ë¡œê¹…
            if calls_since_major_start == 1:
                line_search_start_func_value[0] = neg_ll
                line_search_start_params[0] = params.copy()
            elif calls_since_major_start > 1:
                line_search_func_values.append(neg_ll)

                # íŒŒë¼ë¯¸í„° ë³€í™”ëŸ‰ê³¼ í•¨ìˆ˜ê°’ ë³€í™” ë¡œê¹…
                if line_search_start_params[0] is not None:
                    param_diff = params - line_search_start_params[0]
                    param_change_norm = np.linalg.norm(param_diff)

                    f_start = line_search_start_func_value[0]
                    f_current = neg_ll
                    f_decrease = f_start - f_current

                    self.iteration_logger.info(
                        f"  íŒŒë¼ë¯¸í„° ë³€í™”ëŸ‰ (L2 norm): {param_change_norm:.6e}\n"
                        f"  í•¨ìˆ˜ê°’ ë³€í™”: {f_decrease:+.4f} ({'ê°ì†Œ' if f_decrease > 0 else 'ì¦ê°€'})"
                    )

                # Line searchê°€ maxlsì— ë„ë‹¬í–ˆëŠ”ì§€ ì²´í¬
                if line_search_call_count[0] >= 10:  # maxls = 10
                    self.iteration_logger.info(
                        f"\nâš ï¸  [Line Search ê²½ê³ ] maxls={10}ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.\n"
                        f"  ì‹œì‘ í•¨ìˆ˜ê°’: {line_search_start_func_value[0]:.4f}\n"
                        f"  í˜„ì¬ í•¨ìˆ˜ê°’: {neg_ll:.4f}\n"
                        f"  ë³€í™”ëŸ‰: {neg_ll - line_search_start_func_value[0]:.4f}\n"
                        f"  Line searchê°€ Wolfe ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” step sizeë¥¼ ì°¾ì§€ ëª»í–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                    )

            return neg_ll

        # Get parameter bounds
        self.iteration_logger.info("íŒŒë¼ë¯¸í„° bounds ê³„ì‚° ì‹œì‘...")
        bounds = self._get_parameter_bounds(
            measurement_model, structural_model, choice_model
        )
        self.iteration_logger.info(f"íŒŒë¼ë¯¸í„° bounds ê³„ì‚° ì™„ë£Œ (ì´ {len(bounds)}ê°œ)")

        # ìµœì í™” ë°©ë²• ì„ íƒ
        use_gradient = self.config.estimation.optimizer in ['BFGS', 'L-BFGS-B']

        # Gradient í•¨ìˆ˜ ì •ì˜ (Apollo ë°©ì‹)
        grad_call_count = [0]  # ê·¸ë˜ë””ì–¸íŠ¸ í˜¸ì¶œ íšŸìˆ˜

        def gradient_function(params):
            """Analytic gradient ê³„ì‚° (Apollo ë°©ì‹)"""
            if not self.use_analytic_gradient:
                return None  # ìˆ˜ì¹˜ì  ê·¸ë˜ë””ì–¸íŠ¸ ì‚¬ìš©

            grad_call_count[0] += 1

            # ë‹¨ê³„ ë¡œê·¸: ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ì‹œì‘ (ëª¨ë“  í˜¸ì¶œì—ì„œ ì¶œë ¥)
            self.iteration_logger.info(f"\n[ë‹¨ê³„ 2/2] Analytic Gradient ê³„ì‚° #{grad_call_count[0]}")

            # ë©”ëª¨ë¦¬ ì²´í¬ (ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ì „) - 5íšŒë§ˆë‹¤ ë¡œê¹…
            if hasattr(self, 'memory_monitor'):
                # 5íšŒë§ˆë‹¤ ë©”ëª¨ë¦¬ ìƒíƒœ ë¡œê¹…
                if grad_call_count[0] % 5 == 1:
                    self.memory_monitor.log_memory_stats(f"Gradient ê³„ì‚° #{grad_call_count[0]}")

                # í•­ìƒ ì„ê³„ê°’ ì²´í¬ ë° í•„ìš”ì‹œ ì •ë¦¬
                mem_info = self.memory_monitor.check_and_cleanup(f"Gradient ê³„ì‚° #{grad_call_count[0]}")

            # íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            param_dict = self._unpack_parameters(
                params, measurement_model, structural_model, choice_model
            )

            # ë³‘ë ¬ì²˜ë¦¬ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
            use_parallel = getattr(self.config.estimation, 'use_parallel', False)
            n_cores = getattr(self.config.estimation, 'n_cores', None)

            # ë‹¤ì¤‘ ì ì¬ë³€ìˆ˜ ì—¬ë¶€ í™•ì¸
            from .multi_latent_config import MultiLatentConfig
            is_multi_latent = isinstance(self.config, MultiLatentConfig)

            if is_multi_latent:
                # ë‹¤ì¤‘ ì ì¬ë³€ìˆ˜: compute_individual_gradient ì‚¬ìš©
                from .multi_latent_gradient import MultiLatentJointGradient

                # ê°œì¸ë³„ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë° í•©ì‚°
                individual_ids = self.data[self.config.individual_id_column].unique()
                total_grad_dict = None

                for ind_id in individual_ids:
                    ind_data = self.data[self.data[self.config.individual_id_column] == ind_id]
                    ind_idx = np.where(individual_ids == ind_id)[0][0]
                    ind_draws = self.halton_generator.get_draws()[ind_idx]

                    ind_grad = self.joint_grad.compute_individual_gradient(
                        ind_data=ind_data,
                        ind_draws=ind_draws,
                        params_dict=param_dict,
                        measurement_model=measurement_model,
                        structural_model=structural_model,
                        choice_model=choice_model
                    )

                    # ê·¸ë˜ë””ì–¸íŠ¸ í•©ì‚° (ì¬ê·€ì ìœ¼ë¡œ ì²˜ë¦¬)
                    if total_grad_dict is None:
                        # ì²« ë²ˆì§¸ ê°œì¸: deep copy
                        import copy
                        total_grad_dict = copy.deepcopy(ind_grad)
                    else:
                        # ì¬ê·€ì ìœ¼ë¡œ í•©ì‚°
                        def add_gradients(total, ind):
                            for key in total:
                                if isinstance(total[key], dict):
                                    add_gradients(total[key], ind[key])
                                elif isinstance(total[key], np.ndarray):
                                    total[key] += ind[key]
                                else:
                                    total[key] += ind[key]

                        add_gradients(total_grad_dict, ind_grad)

                grad_dict = total_grad_dict
            else:
                # ë‹¨ì¼ ì ì¬ë³€ìˆ˜: compute_gradient ì‚¬ìš©
                grad_dict = self.joint_grad.compute_gradient(
                    data=self.data,
                    params_dict=param_dict,
                    draws=self.halton_generator.get_draws(),
                    individual_id_column=self.config.individual_id_column,
                    measurement_model=measurement_model,
                    structural_model=structural_model,
                    choice_model=choice_model,
                    indicators=self.config.measurement.indicators,
                    sociodemographics=self.config.structural.sociodemographics,
                    choice_attributes=self.config.choice.choice_attributes,
                    use_parallel=use_parallel,
                    n_cores=n_cores
                )

            # ê·¸ë˜ë””ì–¸íŠ¸ ë²¡í„°ë¡œ ë³€í™˜ (íŒŒë¼ë¯¸í„° ìˆœì„œì™€ ë™ì¼)
            grad_vector = self._pack_gradient(grad_dict, measurement_model, structural_model, choice_model)

            # Negative gradient (minimize -LL)
            neg_grad = -grad_vector

            # Line search ì¤‘ì¸ì§€ íŒë‹¨
            calls_since_major_start = func_call_count[0] - current_major_iter_start_call[0]

            # Gradient ë°©í–¥ ê²€ì¦ (ì²« ë²ˆì§¸ í˜¸ì¶œ ì‹œ)
            if grad_call_count[0] == 1:
                grad_norm = np.linalg.norm(neg_grad)
                self.iteration_logger.info(
                    f"\n[Gradient ë°©í–¥ ê²€ì¦]\n"
                    f"  Gradient norm: {grad_norm:.6e}\n"
                    f"  Gradient (ì²˜ìŒ 5ê°œ): {neg_grad[:5]}\n"
                    f"  Gradient (ë§ˆì§€ë§‰ 5ê°œ): {neg_grad[-5:]}\n"
                    f"  ì£¼ì˜: scipyëŠ” ì´ gradientë¥¼ ì‚¬ìš©í•˜ì—¬ descent directionì„ ê³„ì‚°í•©ë‹ˆë‹¤.\n"
                    f"       d = -H^(-1) Â· gradientì´ë¯€ë¡œ, gradientê°€ ì–‘ìˆ˜ë©´ dëŠ” ìŒìˆ˜ ë°©í–¥ì…ë‹ˆë‹¤."
                )

            # Line search ì‹œì‘ ì‹œ ë°©í–¥ ë¯¸ë¶„ ì €ì¥
            if calls_since_major_start == 1:
                # Major iteration ì‹œì‘ ì‹œ gradient ì €ì¥
                line_search_gradient[0] = neg_grad.copy()
                # ë‹¤ìŒ í•¨ìˆ˜ í˜¸ì¶œì—ì„œ íƒìƒ‰ ë°©í–¥ì„ ì•Œ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ë°©í–¥ ë¯¸ë¶„ì€ ë‚˜ì¤‘ì— ê³„ì‚°

            # Line search ì¤‘ì´ë©´ Curvature ì¡°ê±´ ê³„ì‚°
            elif calls_since_major_start > 1 and line_search_start_params[0] is not None:
                # íƒìƒ‰ ë°©í–¥ ê³„ì‚°: d = params - line_search_start_params
                search_direction = params - line_search_start_params[0]

                # í˜„ì¬ ìœ„ì¹˜ì—ì„œ ë°©í–¥ ë¯¸ë¶„: âˆ‡f(x + Î±Â·d)^TÂ·d
                directional_derivative_new = np.dot(neg_grad, search_direction)

                # Line search ì‹œì‘ ì‹œ ë°©í–¥ ë¯¸ë¶„ ê³„ì‚° (ì²« line search í˜¸ì¶œ ì‹œ)
                if line_search_directional_derivative[0] is None and line_search_gradient[0] is not None:
                    # ì‹œì‘ ìœ„ì¹˜ì—ì„œ ë°©í–¥ ë¯¸ë¶„: âˆ‡f(x)^TÂ·d
                    line_search_directional_derivative[0] = np.dot(line_search_gradient[0], search_direction)

                # Curvature ì¡°ê±´ ì²´í¬
                if line_search_directional_derivative[0] is not None:
                    dd_start = line_search_directional_derivative[0]
                    dd_new = directional_derivative_new

                    # Curvature ì¡°ê±´: |âˆ‡f(x + Î±Â·d)^TÂ·d| â‰¤ c2Â·|âˆ‡f(x)^TÂ·d|
                    c2 = 0.9  # scipy ê¸°ë³¸ê°’
                    curvature_lhs = abs(dd_new)
                    curvature_rhs = c2 * abs(dd_start)
                    curvature_satisfied = curvature_lhs <= curvature_rhs

                    self.iteration_logger.info(
                        f"\n[Curvature ì¡°ê±´ ì²´í¬]\n"
                        f"  âˆ‡f(x)^TÂ·d (ì‹œì‘): {dd_start:.6e}\n"
                        f"  âˆ‡f(x+Î±Â·d)^TÂ·d (í˜„ì¬): {dd_new:.6e}\n"
                        f"  |âˆ‡f(x+Î±Â·d)^TÂ·d|: {curvature_lhs:.6e}\n"
                        f"  c2Â·|âˆ‡f(x)^TÂ·d|: {curvature_rhs:.6e}\n"
                        f"  Curvature ì¡°ê±´: {'âœ“ ë§Œì¡±' if curvature_satisfied else 'âŒ ë¶ˆë§Œì¡±'}\n"
                        f"  â†’ Gradientê°€ {'ì¶©ë¶„íˆ í‰í‰í•´ì§' if curvature_satisfied else 'ì•„ì§ ê°€íŒŒë¦„'}"
                    )

            return neg_grad

        print("=" * 70, flush=True)
        if use_gradient:
            print(f"ìµœì í™” ì‹œì‘: {self.config.estimation.optimizer} (gradient-based)", flush=True)
            if self.use_analytic_gradient:
                print("Analytic gradient ì‚¬ìš© (Apollo ë°©ì‹)", flush=True)
            else:
                print("ìˆ˜ì¹˜ì  ê·¸ë˜ë””ì–¸íŠ¸ ì‚¬ìš© (2-point finite difference)", flush=True)
        else:
            print("ìµœì í™” ì‹œì‘: Nelder-Mead (gradient-free)", flush=True)
        print(f"ì´ˆê¸° íŒŒë¼ë¯¸í„° ê°œìˆ˜: {len(initial_params)}", flush=True)
        self.iteration_logger.info(f"ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜: {self.config.estimation.max_iterations}")
        self.iteration_logger.info("=" * 70)

        # ë³‘ë ¬ì²˜ë¦¬ ì„¤ì • ë¡œê¹…
        use_parallel = getattr(self.config.estimation, 'use_parallel', False)
        if use_parallel:
            n_cores = getattr(self.config.estimation, 'n_cores', None)
            if n_cores is None:
                n_cores = max(1, multiprocessing.cpu_count() - 1)
            self.iteration_logger.info(f"ë³‘ë ¬ì²˜ë¦¬ í™œì„±í™”: {n_cores} ì½”ì–´ ì‚¬ìš©")
        else:
            self.iteration_logger.info("ìˆœì°¨ì²˜ë¦¬ ì‚¬ìš©")

        # ì¡°ê¸° ì¢…ë£Œë¥¼ ìœ„í•œ Wrapper í´ë˜ìŠ¤ (BFGS ì •ìƒ ì¢…ë£Œ í™œìš©)
        class EarlyStoppingWrapper:
            """
            ëª©ì  í•¨ìˆ˜ì™€ gradient í•¨ìˆ˜ë¥¼ ê°ì‹¸ì„œ ì¡°ê¸° ì¢…ë£Œ êµ¬í˜„
            StopIteration ì˜ˆì™¸ ëŒ€ì‹  ë§¤ìš° í° ê°’ì„ ë°˜í™˜í•˜ì—¬ BFGSê°€ ì •ìƒ ì¢…ë£Œí•˜ë„ë¡ ìœ ë„
            â†’ BFGSê°€ ì •ìƒ ì¢…ë£Œí•˜ë©´ result.hess_inv ìë™ ì œê³µ (ì¶”ê°€ ê³„ì‚° 0íšŒ!)
            """

            def __init__(self, func, grad_func, patience=5, tol=1e-6, logger=None, iteration_logger=None):
                """
                Args:
                    func: ëª©ì  í•¨ìˆ˜ (negative log-likelihood)
                    grad_func: Gradient í•¨ìˆ˜
                    patience: í•¨ìˆ˜ í˜¸ì¶œ ê¸°ì¤€ ê°œì„  ì—†ëŠ” íšŸìˆ˜ (ê¸°ë³¸ê°’: 5)
                    tol: LL ë³€í™” í—ˆìš© ì˜¤ì°¨ (ì ˆëŒ€ê°’)
                    logger: ë©”ì¸ ë¡œê±°
                    iteration_logger: ë°˜ë³µ ë¡œê±°
                """
                self.func = func
                self.grad_func = grad_func
                self.patience = patience
                self.tol = tol
                self.logger = logger
                self.iteration_logger = iteration_logger

                self.best_ll = np.inf
                self.best_x = None  # ìµœì  íŒŒë¼ë¯¸í„° ì €ì¥
                self.no_improvement_count = 0
                self.func_call_count = 0
                self.grad_call_count = 0
                self.early_stopped = False
                self.bfgs_iteration_count = 0  # BFGS iteration ì¹´ìš´í„°

            def objective(self, x):
                """
                ëª©ì  í•¨ìˆ˜ wrapper - ì¡°ê¸° ì¢…ë£Œ ì‹œ ë§¤ìš° í° ê°’ ë°˜í™˜
                """
                # ì´ë¯¸ ì¡°ê¸° ì¢…ë£Œëœ ê²½ìš°: ë§¤ìš° í° ê°’ ë°˜í™˜í•˜ì—¬ BFGSê°€ ì¢…ë£Œí•˜ë„ë¡ ìœ ë„
                if self.early_stopped:
                    return 1e10

                self.func_call_count += 1
                current_ll = self.func(x)

                # LL ê°œì„  ì²´í¬
                if current_ll < self.best_ll - self.tol:
                    # ëª…í™•í•œ ê°œì„ 
                    self.best_ll = current_ll
                    self.best_x = x.copy()  # ìµœì  íŒŒë¼ë¯¸í„° ì €ì¥
                    self.no_improvement_count = 0
                else:
                    # ê°œì„  ì—†ìŒ
                    self.no_improvement_count += 1

                # ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´ ì²´í¬
                if self.no_improvement_count >= self.patience:
                    self.early_stopped = True
                    msg = f"ì¡°ê¸° ì¢…ë£Œ: {self.patience}íšŒ ì—°ì† í•¨ìˆ˜ í˜¸ì¶œì—ì„œ LL ê°œì„  ì—†ìŒ (Best LL={self.best_ll:.4f})"
                    if self.logger:
                        self.logger.info(msg)
                    if self.iteration_logger:
                        self.iteration_logger.info(msg)
                    # StopIteration ëŒ€ì‹  ë§¤ìš° í° ê°’ ë°˜í™˜
                    return 1e10

                return current_ll

            def gradient(self, x):
                """
                Gradient í•¨ìˆ˜ wrapper - ì¡°ê¸° ì¢…ë£Œ ì‹œ 0 ë²¡í„° ë°˜í™˜
                """
                # ì´ë¯¸ ì¡°ê¸° ì¢…ë£Œëœ ê²½ìš°: 0 ë²¡í„° ë°˜í™˜í•˜ì—¬ BFGSê°€ ì¢…ë£Œí•˜ë„ë¡ ìœ ë„
                if self.early_stopped:
                    return np.zeros_like(x)

                self.grad_call_count += 1
                return self.grad_func(x)

            def callback(self, xk):
                """
                BFGS callback - ë§¤ Major iterationë§ˆë‹¤ í˜¸ì¶œë¨
                ì¡°ê¸° ì¢…ë£Œ ì‹œ ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ë³µì›
                """
                self.bfgs_iteration_count += 1
                major_iter_count[0] = self.bfgs_iteration_count

                # Major iteration ì™„ë£Œ ë¡œê¹…
                if self.iteration_logger:
                    # í˜„ì¬ í•¨ìˆ˜ê°’ ê³„ì‚°
                    current_f = self.func(xk)
                    current_ll = -current_f

                    # Line search í†µê³„
                    line_search_calls = line_search_call_count[0]

                    # Line search ì„±ê³µ ì—¬ë¶€ íŒë‹¨
                    if line_search_start_func_value[0] is not None:
                        f_start = line_search_start_func_value[0]
                        f_final = current_f
                        f_decrease = f_start - f_final

                        if f_decrease > 0:
                            ls_status = f"âœ“ ì„±ê³µ (í•¨ìˆ˜ê°’ ê°ì†Œ: {f_decrease:.4f})"
                        elif f_decrease == 0:
                            ls_status = f"âš ï¸  ì •ì²´ (í•¨ìˆ˜ê°’ ë³€í™” ì—†ìŒ)"
                        else:
                            ls_status = f"âŒ ì‹¤íŒ¨ (í•¨ìˆ˜ê°’ ì¦ê°€: {-f_decrease:.4f})"
                    else:
                        ls_status = "N/A (ì²« iteration)"

                    # ftol ê³„ì‚° (ì´ì „ major iterationê³¼ ë¹„êµ)
                    if last_major_iter_func_value[0] is not None:
                        f_prev = last_major_iter_func_value[0]
                        f_curr = current_f
                        rel_change = abs(f_prev - f_curr) / max(abs(f_prev), abs(f_curr), 1.0)
                        ftol_status = f"ftol = {rel_change:.6e} (ê¸°ì¤€: 1e-3)"
                        if rel_change <= 1e-3:
                            ftol_status += " âœ“ ìˆ˜ë ´ ì¡°ê±´ ë§Œì¡±"
                    else:
                        ftol_status = "ftol = N/A (ì²« iteration)"

                    # Gradient norm ê³„ì‚°
                    if self.grad_func:
                        grad = self.grad_func(xk)
                        grad_norm = np.linalg.norm(grad, ord=np.inf)
                        gtol_status = f"gtol = {grad_norm:.6e} (ê¸°ì¤€: 1e-3)"
                        if grad_norm <= 1e-3:
                            gtol_status += " âœ“ ìˆ˜ë ´ ì¡°ê±´ ë§Œì¡±"
                    else:
                        gtol_status = "gtol = N/A"

                    self.iteration_logger.info(
                        f"\n{'='*80}\n"
                        f"[Major Iteration #{self.bfgs_iteration_count} ì™„ë£Œ]\n"
                        f"  ìµœì¢… LL: {current_ll:.4f}\n"
                        f"  Line Search: {line_search_calls}íšŒ í•¨ìˆ˜ í˜¸ì¶œ - {ls_status}\n"
                        f"  í•¨ìˆ˜ í˜¸ì¶œ: {self.func_call_count}íšŒ (ëˆ„ì )\n"
                        f"  ê·¸ë˜ë””ì–¸íŠ¸ í˜¸ì¶œ: {self.grad_call_count}íšŒ (ëˆ„ì )\n"
                        f"  ìˆ˜ë ´ ì¡°ê±´:\n"
                        f"    - {ftol_status}\n"
                        f"    - {gtol_status}\n"
                        f"  Hessian ê·¼ì‚¬: BFGS ê³µì‹ìœ¼ë¡œ ì—…ë°ì´íŠ¸ ì™„ë£Œ\n"
                        f"{'='*80}"
                    )

                    # ë‹¤ìŒ major iterationì„ ìœ„í•œ ì¤€ë¹„
                    last_major_iter_func_value[0] = current_f
                    current_major_iter_start_call[0] = func_call_count[0]
                    line_search_call_count[0] = 0  # Line search ì¹´ìš´í„° ë¦¬ì…‹
                    line_search_func_values.clear()
                    line_search_directional_derivative[0] = None  # ë°©í–¥ ë¯¸ë¶„ ë¦¬ì…‹

                if self.early_stopped and self.best_x is not None:
                    # ì¡°ê¸° ì¢…ë£Œ í›„ì—ëŠ” ìµœì  íŒŒë¼ë¯¸í„°ë¥¼ ìœ ì§€
                    xk[:] = self.best_x

        if use_gradient:
            self.logger.info(f"ìµœì í™” ì‹œì‘: {self.config.estimation.optimizer} (gradient-based)")
            self.iteration_logger.info(f"ìµœì í™” ì‹œì‘: {self.config.estimation.optimizer} (gradient-based)")
            if self.use_analytic_gradient:
                self.logger.info("Analytic gradient ì‚¬ìš© (Apollo ë°©ì‹)")
                self.iteration_logger.info("Analytic gradient ì‚¬ìš© (Apollo ë°©ì‹)")
            else:
                self.logger.info("ìˆ˜ì¹˜ì  ê·¸ë˜ë””ì–¸íŠ¸ ì‚¬ìš© (2-point finite difference)")
                self.iteration_logger.info("ìˆ˜ì¹˜ì  ê·¸ë˜ë””ì–¸íŠ¸ ì‚¬ìš© (2-point finite difference)")

            # ì¡°ê¸° ì¢…ë£Œ ì„¤ì • í™•ì¸
            use_early_stopping = getattr(self.config.estimation, 'early_stopping', False)
            early_stopping_patience = getattr(self.config.estimation, 'early_stopping_patience', 5)
            early_stopping_tol = getattr(self.config.estimation, 'early_stopping_tol', 1e-6)

            # ì¡°ê¸° ì¢…ë£Œ Wrapper ìƒì„±
            early_stopping_wrapper = EarlyStoppingWrapper(
                func=negative_log_likelihood,
                grad_func=gradient_function if self.use_analytic_gradient else None,
                patience=early_stopping_patience if use_early_stopping else 999999,  # ë¹„í™œì„±í™” ì‹œ ë§¤ìš° í° ê°’
                tol=early_stopping_tol,
                logger=self.logger,
                iteration_logger=self.iteration_logger
            )

            # ì´ˆê¸° í•¨ìˆ˜ í˜¸ì¶œ ì‹œì‘ ìœ„ì¹˜ ì„¤ì •
            current_major_iter_start_call[0] = func_call_count[0]

            if use_early_stopping:
                self.logger.info(f"ì¡°ê¸° ì¢…ë£Œ í™œì„±í™”: {early_stopping_patience}íšŒ ì—°ì† í•¨ìˆ˜ í˜¸ì¶œì—ì„œ LL ê°œì„  ì—†ìœ¼ë©´ ì¢…ë£Œ (tol={early_stopping_tol})")
                self.iteration_logger.info(f"ì¡°ê¸° ì¢…ë£Œ í™œì„±í™”: {early_stopping_patience}íšŒ ì—°ì† í•¨ìˆ˜ í˜¸ì¶œì—ì„œ LL ê°œì„  ì—†ìœ¼ë©´ ì¢…ë£Œ (tol={early_stopping_tol})")
            else:
                self.logger.info("ì¡°ê¸° ì¢…ë£Œ ë¹„í™œì„±í™” (ì •ìƒ ì¢…ë£Œë§Œ ì‚¬ìš©)")
                self.iteration_logger.info("ì¡°ê¸° ì¢…ë£Œ ë¹„í™œì„±í™” (ì •ìƒ ì¢…ë£Œë§Œ ì‚¬ìš©)")

            # BFGS ë˜ëŠ” L-BFGS-B (ì •ìƒ ì¢…ë£Œë¡œ ì²˜ë¦¬)
            # ìˆ˜ì¹˜ì  ê·¸ë˜ë””ì–¸íŠ¸ í•¨ìˆ˜ (epsilon ì œì–´)
            if not self.use_analytic_gradient:
                from scipy.optimize import approx_fprime

                # ê·¸ë˜ë””ì–¸íŠ¸ í˜¸ì¶œ ì¹´ìš´í„°
                grad_call_count = [0]

                def numerical_gradient(x):
                    grad_call_count[0] += 1
                    grad = approx_fprime(x, early_stopping_wrapper.objective, epsilon=1e-4)

                    # ì²˜ìŒ 5ë²ˆë§Œ ë¡œê¹…
                    if grad_call_count[0] <= 5:
                        self.iteration_logger.info(f"[ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° #{grad_call_count[0]}]")
                        self.iteration_logger.info(f"  íŒŒë¼ë¯¸í„° (ì²˜ìŒ 10ê°œ): {x[:10]}")
                        self.iteration_logger.info(f"  ê·¸ë˜ë””ì–¸íŠ¸ (ì²˜ìŒ 10ê°œ): {grad[:10]}")
                        self.iteration_logger.info(f"  ê·¸ë˜ë””ì–¸íŠ¸ norm: {np.linalg.norm(grad):.6f}")
                        self.iteration_logger.info(f"  ê·¸ë˜ë””ì–¸íŠ¸ max: {np.max(np.abs(grad)):.6f}")

                    return grad

                jac_function = numerical_gradient
            else:
                jac_function = early_stopping_wrapper.gradient

            result = optimize.minimize(
                early_stopping_wrapper.objective,  # Wrapperì˜ objective ì‚¬ìš©
                initial_params,
                method=self.config.estimation.optimizer,
                jac=jac_function,
                bounds=bounds if self.config.estimation.optimizer == 'L-BFGS-B' else None,
                callback=early_stopping_wrapper.callback,  # Callback ì¶”ê°€
                options={
                    'maxiter': 200,  # Major iteration ìµœëŒ€ íšŸìˆ˜
                    'ftol': 1e-3,    # í•¨ìˆ˜ê°’ ìƒëŒ€ì  ë³€í™” 0.1% ì´í•˜ë©´ ì¢…ë£Œ
                    'gtol': 1e-3,    # ê·¸ë˜ë””ì–¸íŠ¸ norm í—ˆìš© ì˜¤ì°¨
                    'maxls': 10,     # Line search ìµœëŒ€ íšŸìˆ˜ (ê¸°ë³¸ê°’: 20)
                    'disp': True
                }
            )

            # ìµœì í™” ê²°ê³¼ ë¡œê¹…
            self.logger.info(f"\nìµœì í™” ì¢…ë£Œ: {result.message}")
            self.iteration_logger.info(f"\nìµœì í™” ì¢…ë£Œ: {result.message}")
            self.logger.info(f"  ì„±ê³µ ì—¬ë¶€: {result.success}")
            self.iteration_logger.info(f"  ì„±ê³µ ì—¬ë¶€: {result.success}")
            self.logger.info(f"  Major iterations: {major_iter_count[0]}")
            self.iteration_logger.info(f"  Major iterations: {major_iter_count[0]}")
            self.logger.info(f"  í•¨ìˆ˜ í˜¸ì¶œ: {result.nfev}íšŒ")
            self.iteration_logger.info(f"  í•¨ìˆ˜ í˜¸ì¶œ: {result.nfev}íšŒ")

            # Line search ì‹¤íŒ¨ ê²½ê³ 
            if not result.success and 'ABNORMAL_TERMINATION_IN_LNSRCH' in result.message:
                self.logger.warning(
                    "\nâš ï¸  Line Search ì‹¤íŒ¨ë¡œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n"
                    "  ê°€ëŠ¥í•œ ì›ì¸:\n"
                    "    1. Gradient ê³„ì‚° ì˜¤ë¥˜\n"
                    "    2. í•¨ìˆ˜ê°€ ë„ˆë¬´ í‰í‰í•¨ (flat region)\n"
                    "    3. ìˆ˜ì¹˜ì  ë¶ˆì•ˆì •ì„±\n"
                    "  ê¶Œì¥ ì¡°ì¹˜:\n"
                    "    - maxls ê°’ì„ ì¦ê°€ (í˜„ì¬: 10)\n"
                    "    - ftol, gtol ê°’ì„ ì™„í™”\n"
                    "    - ì´ˆê¸°ê°’ ë³€ê²½"
                )
                self.iteration_logger.warning(
                    "\nâš ï¸  Line Search ì‹¤íŒ¨ë¡œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n"
                    "  ê°€ëŠ¥í•œ ì›ì¸:\n"
                    "    1. Gradient ê³„ì‚° ì˜¤ë¥˜\n"
                    "    2. í•¨ìˆ˜ê°€ ë„ˆë¬´ í‰í‰í•¨ (flat region)\n"
                    "    3. ìˆ˜ì¹˜ì  ë¶ˆì•ˆì •ì„±\n"
                    "  ê¶Œì¥ ì¡°ì¹˜:\n"
                    "    - maxls ê°’ì„ ì¦ê°€ (í˜„ì¬: 10)\n"
                    "    - ftol, gtol ê°’ì„ ì™„í™”\n"
                    "    - ì´ˆê¸°ê°’ ë³€ê²½"
                )

            # ì¡°ê¸° ì¢…ë£Œëœ ê²½ìš° ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ë³µì›
            if early_stopping_wrapper.early_stopped:
                from scipy.optimize import OptimizeResult

                # Wrapperì— ì €ì¥ëœ ìµœì  íŒŒë¼ë¯¸í„°ë¡œ result ê°ì²´ ì¬ìƒì„±
                result = OptimizeResult(
                    x=early_stopping_wrapper.best_x,
                    success=True,
                    message=f"Early stopping: {early_stopping_wrapper.patience}íšŒ ì—°ì† ê°œì„  ì—†ìŒ",
                    fun=early_stopping_wrapper.best_ll,
                    nit=early_stopping_wrapper.func_call_count,
                    nfev=early_stopping_wrapper.func_call_count,
                    njev=early_stopping_wrapper.grad_call_count,
                    hess_inv=None  # ë‚˜ì¤‘ì— ì„¤ì •
                )

            # Hessian ì—­í–‰ë ¬ ì²˜ë¦¬
            if self.config.estimation.calculate_se:
                # BFGSì˜ hess_invê°€ ìˆìœ¼ë©´ ì‚¬ìš© (ì¶”ê°€ ê³„ì‚° 0íšŒ!)
                if hasattr(result, 'hess_inv') and result.hess_inv is not None:
                    self.logger.info("Hessian ì—­í–‰ë ¬: BFGSì—ì„œ ìë™ ì œê³µ (ì¶”ê°€ ê³„ì‚° 0íšŒ)")
                    self.iteration_logger.info("Hessian ì—­í–‰ë ¬: BFGSì—ì„œ ìë™ ì œê³µ (ì¶”ê°€ ê³„ì‚° 0íšŒ)")
                else:
                    # BFGS hess_invê°€ ì—†ìœ¼ë©´ ê²½ê³ ë§Œ ì¶œë ¥ (L-BFGS-Bì˜ ê²½ìš°)
                    self.logger.warning("Hessian ì—­í–‰ë ¬ ì—†ìŒ (L-BFGS-BëŠ” hess_inv ì œê³µ ì•ˆ í•¨)")
                    self.iteration_logger.warning("Hessian ì—­í–‰ë ¬ ì—†ìŒ (L-BFGS-BëŠ” hess_inv ì œê³µ ì•ˆ í•¨)")
                    self.logger.info("í‘œì¤€ì˜¤ì°¨ ê³„ì‚°ì„ ìœ„í•´ì„œëŠ” BFGS ë°©ë²• ì‚¬ìš© ê¶Œì¥")
                    self.iteration_logger.info("í‘œì¤€ì˜¤ì°¨ ê³„ì‚°ì„ ìœ„í•´ì„œëŠ” BFGS ë°©ë²• ì‚¬ìš© ê¶Œì¥")

            # ìµœì¢… ë¡œê·¸
            if early_stopping_wrapper.early_stopped:
                self.logger.info(f"ì¡°ê¸° ì¢…ë£Œ ì™„ë£Œ: í•¨ìˆ˜ í˜¸ì¶œ {early_stopping_wrapper.func_call_count}íšŒ, LL={-early_stopping_wrapper.best_ll:.4f}")
                self.iteration_logger.info(f"ì¡°ê¸° ì¢…ë£Œ ì™„ë£Œ: í•¨ìˆ˜ í˜¸ì¶œ {early_stopping_wrapper.func_call_count}íšŒ, LL={-early_stopping_wrapper.best_ll:.4f}")
            else:
                self.logger.info(f"ì •ìƒ ì¢…ë£Œ: í•¨ìˆ˜ í˜¸ì¶œ {early_stopping_wrapper.func_call_count}íšŒ")
                self.iteration_logger.info(f"ì •ìƒ ì¢…ë£Œ: í•¨ìˆ˜ í˜¸ì¶œ {early_stopping_wrapper.func_call_count}íšŒ")
        else:
            self.logger.info(f"ìµœì í™” ì‹œì‘: Nelder-Mead (gradient-free)")
            self.iteration_logger.info(f"ìµœì í™” ì‹œì‘: Nelder-Mead (gradient-free)")

            result = optimize.minimize(
                negative_log_likelihood,
                initial_params,
                method='Nelder-Mead',
                options={
                    'maxiter': self.config.estimation.max_iterations,
                    'xatol': 1e-4,
                    'fatol': 1e-4,
                    'disp': True
                }
            )

        if result.success:
            self.logger.info("ìµœì í™” ì„±ê³µ")
            self.iteration_logger.info("ìµœì í™” ì„±ê³µ")
        else:
            self.logger.warning(f"ìµœì í™” ì‹¤íŒ¨: {result.message}")
            self.iteration_logger.warning(f"ìµœì í™” ì‹¤íŒ¨: {result.message}")

        self.iteration_logger.info("=" * 70)
        self.iteration_logger.info(f"ìµœì¢… ë¡œê·¸ìš°ë„: {-result.fun:.4f}")
        self.iteration_logger.info(f"ë°˜ë³µ íšŸìˆ˜: {iteration_count[0]}")
        self.iteration_logger.info("=" * 70)

        # ê²°ê³¼ ì²˜ë¦¬
        self.results = self._process_results(
            result, measurement_model, structural_model, choice_model
        )

        # ë¡œê±° ì¢…ë£Œ
        self._close_iteration_logger()

        return self.results
    
    def _compute_individual_likelihood(self, ind_id, ind_data, ind_draws,
                                       param_dict, measurement_model,
                                       structural_model, choice_model) -> float:
        """
        ê°œì¸ë³„ ìš°ë„ ê³„ì‚° (ë³‘ë ¬í™” ê°€ëŠ¥)

        Args:
            ind_id: ê°œì¸ ID
            ind_data: ê°œì¸ ë°ì´í„°
            ind_draws: ê°œì¸ì˜ Halton draws
            param_dict: íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬
            measurement_model: ì¸¡ì •ëª¨ë¸
            structural_model: êµ¬ì¡°ëª¨ë¸
            choice_model: ì„ íƒëª¨ë¸

        Returns:
            ê°œì¸ì˜ ë¡œê·¸ìš°ë„
        """
        draw_lls = []

        for j, draw in enumerate(ind_draws):
            # êµ¬ì¡°ëª¨ë¸: LV = Î³*X + Î·
            lv = structural_model.predict(ind_data, param_dict['structural'], draw)

            # ì¸¡ì •ëª¨ë¸ ìš°ë„: P(Indicators|LV)
            ll_measurement = measurement_model.log_likelihood(
                ind_data, lv, param_dict['measurement']
            )

            # Panel Product: ê°œì¸ì˜ ì—¬ëŸ¬ ì„ íƒ ìƒí™©ì— ëŒ€í•œ í™•ë¥ ì„ ê³±í•¨
            choice_set_lls = []
            for idx in range(len(ind_data)):
                ll_choice_t = choice_model.log_likelihood(
                    ind_data.iloc[idx:idx+1],  # ê° ì„ íƒ ìƒí™©
                    lv,
                    param_dict['choice']
                )
                choice_set_lls.append(ll_choice_t)

            # Panel product: log(P1 * P2 * ... * PT) = log(P1) + log(P2) + ... + log(PT)
            ll_choice = sum(choice_set_lls)

            # êµ¬ì¡°ëª¨ë¸ ìš°ë„: P(LV|X) - ì •ê·œë¶„í¬ ê°€ì •
            ll_structural = structural_model.log_likelihood(
                ind_data, lv, param_dict['structural'], draw
            )

            # ê²°í•© ë¡œê·¸ìš°ë„
            draw_ll = ll_measurement + ll_choice + ll_structural

            # ğŸ”´ ìˆ˜ì •: -infë¥¼ ë§¤ìš° ì‘ì€ ê°’ìœ¼ë¡œ ëŒ€ì²´ (ì—°ì†ì„± í™•ë³´ for gradient)
            if not np.isfinite(draw_ll):
                draw_ll = -1e10  # -inf ëŒ€ì‹  ë§¤ìš° ì‘ì€ ê°’

            draw_lls.append(draw_ll)

        # ğŸ”´ ìˆ˜ì •: logsumexpë¥¼ ì‚¬ìš©í•˜ì—¬ í‰ê·  ê³„ì‚°
        # log[(1/R) Î£áµ£ exp(ll_r)] = logsumexp(ll_r) - log(R)
        person_ll = logsumexp(draw_lls) - np.log(len(draw_lls))

        return person_ll

    def _joint_log_likelihood(self, params: np.ndarray,
                             measurement_model,
                             structural_model,
                             choice_model) -> float:
        """
        ê²°í•© ë¡œê·¸ìš°ë„ ê³„ì‚°

        ì‹œë®¬ë ˆì´ì…˜ ê¸°ë°˜:
        log L â‰ˆ Î£áµ¢ log[(1/R) Î£áµ£ P(Choice|LVáµ£) Ã— P(Indicators|LVáµ£) Ã— P(LVáµ£|X)]
        """
        # íŒŒë¼ë¯¸í„° ë¶„í•´
        param_dict = self._unpack_parameters(
            params, measurement_model, structural_model, choice_model
        )

        # ë©”ëª¨ë¦¬ ì²´í¬ (Halton draws ê°€ì ¸ì˜¤ê¸° ì „)
        if hasattr(self, 'memory_monitor') and hasattr(self, '_likelihood_call_count'):
            self.memory_monitor.log_memory_stats(f"Halton draws ê°€ì ¸ì˜¤ê¸° ì „ (ìš°ë„ #{self._likelihood_call_count})")

        draws = self.halton_generator.get_draws()

        # ë©”ëª¨ë¦¬ ì²´í¬ (Halton draws ê°€ì ¸ì˜¨ í›„)
        if hasattr(self, 'memory_monitor') and hasattr(self, '_likelihood_call_count'):
            self.memory_monitor.log_memory_stats(f"Halton draws ê°€ì ¸ì˜¨ í›„ (ìš°ë„ #{self._likelihood_call_count})")

        individual_ids = self.data[self.config.individual_id_column].unique()

        # ë³‘ë ¬ì²˜ë¦¬ ì—¬ë¶€ í™•ì¸
        use_parallel = getattr(self.config.estimation, 'use_parallel', False)

        if use_parallel:
            # ë³‘ë ¬ì²˜ë¦¬ ì‚¬ìš© (ì „ì—­ í•¨ìˆ˜ ì‚¬ìš©)
            n_cores = getattr(self.config.estimation, 'n_cores', None)
            if n_cores is None:
                n_cores = max(1, multiprocessing.cpu_count() - 1)

            # ì„¤ì • ì •ë³´ë¥¼ dictë¡œ ë³€í™˜ (pickle ê°€ëŠ¥)
            config_dict = {
                'measurement': {
                    'latent_variable': self.config.measurement.latent_variable,
                    'indicators': self.config.measurement.indicators,
                    'n_categories': self.config.measurement.n_categories
                },
                'structural': {
                    'sociodemographics': self.config.structural.sociodemographics,
                    'error_variance': self.config.structural.error_variance
                },
                'choice': {
                    'choice_attributes': self.config.choice.choice_attributes
                }
            }

            # ê°œì¸ë³„ ë°ì´í„° ì¤€ë¹„ (dict í˜•íƒœë¡œ ë³€í™˜)
            args_list = []
            for i, ind_id in enumerate(individual_ids):
                ind_data = self.data[self.data[self.config.individual_id_column] == ind_id]
                ind_data_dict = ind_data.to_dict('list')  # pickle ê°€ëŠ¥í•œ dictë¡œ ë³€í™˜
                ind_draws = draws[i, :]
                args_list.append((ind_data_dict, ind_draws, param_dict, config_dict))

            # ë³‘ë ¬ ê³„ì‚°
            with ProcessPoolExecutor(max_workers=n_cores) as executor:
                person_lls = list(executor.map(_compute_individual_likelihood_parallel, args_list))

            total_ll = sum(person_lls)
        else:
            # ìˆœì°¨ì²˜ë¦¬
            total_ll = 0.0
            for i, ind_id in enumerate(individual_ids):
                ind_data = self.data[self.data[self.config.individual_id_column] == ind_id]
                ind_draws = draws[i, :]

                person_ll = self._compute_individual_likelihood(
                    ind_id, ind_data, ind_draws, param_dict,
                    measurement_model, structural_model, choice_model
                )
                total_ll += person_ll

        return total_ll

    def _get_parameter_bounds(self, measurement_model,
                              structural_model, choice_model) -> list:
        """
        Parameter bounds for L-BFGS-B

        Returns:
            bounds: [(lower, upper), ...] list
        """
        bounds = []

        # Measurement model parameters
        # - Factor loadings (zeta): [0.1, 10]
        n_indicators = len(self.config.measurement.indicators)
        bounds.extend([(0.1, 10.0)] * n_indicators)

        # - Thresholds (tau): [-10, 10]
        n_thresholds = self.config.measurement.n_categories - 1
        for _ in range(n_indicators):
            bounds.extend([(-10.0, 10.0)] * n_thresholds)

        # Structural model parameters (gamma): unbounded
        n_sociodem = len(self.config.structural.sociodemographics)
        bounds.extend([(None, None)] * n_sociodem)

        # Choice model parameters
        # - Intercept: unbounded
        bounds.append((None, None))

        # - Attribute coefficients (beta): unbounded
        n_attributes = len(self.config.choice.choice_attributes)
        bounds.extend([(None, None)] * n_attributes)

        # - Latent variable coefficient (lambda): unbounded
        bounds.append((None, None))

        # - Sociodemographic coefficients: unbounded
        if self.config.structural.include_in_choice:
            bounds.extend([(None, None)] * n_sociodem)

        return bounds

    def _get_initial_parameters(self, measurement_model,
                                structural_model, choice_model) -> np.ndarray:
        """ì´ˆê¸° íŒŒë¼ë¯¸í„° ì„¤ì •"""
        
        params = []
        
        # ì¸¡ì •ëª¨ë¸ íŒŒë¼ë¯¸í„°
        # - ìš”ì¸ì ì¬ëŸ‰ (zeta)
        n_indicators = len(self.config.measurement.indicators)
        params.extend([1.0] * n_indicators)  # zeta
        
        # - ì„ê³„ê°’ (tau)
        n_thresholds = self.config.measurement.n_categories - 1
        for _ in range(n_indicators):
            params.extend([-2, -1, 1, 2])  # 5ì  ì²™ë„ ê¸°ë³¸ê°’
        
        # êµ¬ì¡°ëª¨ë¸ íŒŒë¼ë¯¸í„° (gamma)
        n_sociodem = len(self.config.structural.sociodemographics)
        params.extend([0.0] * n_sociodem)
        
        # ì„ íƒëª¨ë¸ íŒŒë¼ë¯¸í„°
        # - ì ˆí¸
        params.append(0.0)
        
        # - ì†ì„± ê³„ìˆ˜ (beta)
        n_attributes = len(self.config.choice.choice_attributes)
        params.extend([0.0] * n_attributes)
        
        # - ì ì¬ë³€ìˆ˜ ê³„ìˆ˜ (lambda)
        params.append(1.0)
        
        # - ì‚¬íšŒì¸êµ¬í•™ì  ë³€ìˆ˜ ê³„ìˆ˜ (ì„ íƒëª¨ë¸ì— í¬í•¨ë˜ëŠ” ê²½ìš°)
        if self.config.structural.include_in_choice:
            params.extend([0.0] * n_sociodem)
        
        return np.array(params)
    

    
    def _get_parameter_bounds(self, measurement_model,
                              structural_model, choice_model) -> list:
        """
        Parameter bounds for L-BFGS-B
        
        Returns:
            bounds: [(lower, upper), ...] list
        """
        bounds = []
        
        # Measurement model parameters
        # - Factor loadings (zeta): [0.1, 10]
        n_indicators = len(self.config.measurement.indicators)
        bounds.extend([(0.1, 10.0)] * n_indicators)
        
        # - Thresholds (tau): [-10, 10]
        n_thresholds = self.config.measurement.n_categories - 1
        for _ in range(n_indicators):
            bounds.extend([(-10.0, 10.0)] * n_thresholds)
        
        # Structural model parameters (gamma): unbounded
        n_sociodem = len(self.config.structural.sociodemographics)
        bounds.extend([(None, None)] * n_sociodem)
        
        # Choice model parameters
        # - Intercept: unbounded
        bounds.append((None, None))
        
        # - Attribute coefficients (beta): unbounded
        n_attributes = len(self.config.choice.choice_attributes)
        bounds.extend([(None, None)] * n_attributes)
        
        # - Latent variable coefficient (lambda): unbounded
        bounds.append((None, None))
        
        # - Sociodemographic coefficients: unbounded
        if self.config.structural.include_in_choice:
            bounds.extend([(None, None)] * n_sociodem)
        
        return bounds
    def _unpack_parameters(self, params: np.ndarray,
                          measurement_model,
                          structural_model,
                          choice_model) -> Dict[str, Dict]:
        """íŒŒë¼ë¯¸í„° ë²¡í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        
        idx = 0
        param_dict = {
            'measurement': {},
            'structural': {},
            'choice': {}
        }
        
        # ì¸¡ì •ëª¨ë¸ íŒŒë¼ë¯¸í„°
        n_indicators = len(self.config.measurement.indicators)
        param_dict['measurement']['zeta'] = params[idx:idx+n_indicators]
        idx += n_indicators

        n_thresholds = self.config.measurement.n_categories - 1
        # tauë¥¼ 2D ë°°ì—´ë¡œ ì €ì¥ (n_indicators, n_thresholds)
        tau_list = []
        for i in range(n_indicators):
            tau_list.append(params[idx:idx+n_thresholds])
            idx += n_thresholds
        param_dict['measurement']['tau'] = np.array(tau_list)
        
        # êµ¬ì¡°ëª¨ë¸ íŒŒë¼ë¯¸í„°
        n_sociodem = len(self.config.structural.sociodemographics)
        param_dict['structural']['gamma'] = params[idx:idx+n_sociodem]
        idx += n_sociodem
        
        # ì„ íƒëª¨ë¸ íŒŒë¼ë¯¸í„°
        param_dict['choice']['intercept'] = params[idx]
        idx += 1
        
        n_attributes = len(self.config.choice.choice_attributes)
        param_dict['choice']['beta'] = params[idx:idx+n_attributes]
        idx += n_attributes
        
        param_dict['choice']['lambda'] = params[idx]
        idx += 1
        
        if self.config.structural.include_in_choice:
            param_dict['choice']['beta_sociodem'] = params[idx:idx+n_sociodem]
            idx += n_sociodem
        
        return param_dict

    def _pack_gradient(self, grad_dict: Dict, measurement_model,
                      structural_model, choice_model) -> np.ndarray:
        """
        ê·¸ë˜ë””ì–¸íŠ¸ ë”•ì…”ë„ˆë¦¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜ (íŒŒë¼ë¯¸í„° ìˆœì„œì™€ ë™ì¼)

        Args:
            grad_dict: ê·¸ë˜ë””ì–¸íŠ¸ ë”•ì…”ë„ˆë¦¬
            measurement_model: ì¸¡ì •ëª¨ë¸
            structural_model: êµ¬ì¡°ëª¨ë¸
            choice_model: ì„ íƒëª¨ë¸

        Returns:
            gradient_vector: ê·¸ë˜ë””ì–¸íŠ¸ ë²¡í„°
        """
        gradient_list = []

        # ë‹¤ì¤‘ ì ì¬ë³€ìˆ˜ ì—¬ë¶€ í™•ì¸
        from .multi_latent_config import MultiLatentConfig
        is_multi_latent = isinstance(self.config, MultiLatentConfig)

        if is_multi_latent:
            # ë‹¤ì¤‘ ì ì¬ë³€ìˆ˜: ê° LVë³„ë¡œ ê·¸ë˜ë””ì–¸íŠ¸ ì¶”ì¶œ
            for lv_name in measurement_model.models.keys():
                lv_grad = grad_dict['measurement'][lv_name]
                gradient_list.append(lv_grad['grad_zeta'])
                gradient_list.append(lv_grad['grad_tau'].flatten())

            # êµ¬ì¡°ëª¨ë¸ ê·¸ë˜ë””ì–¸íŠ¸
            gradient_list.append(grad_dict['structural']['grad_gamma_lv'])
            gradient_list.append(grad_dict['structural']['grad_gamma_x'])
        else:
            # ë‹¨ì¼ ì ì¬ë³€ìˆ˜
            gradient_list.append(grad_dict['grad_zeta'])
            gradient_list.append(grad_dict['grad_tau'].flatten())
            gradient_list.append(grad_dict['grad_gamma'])

        # ì„ íƒëª¨ë¸ ê·¸ë˜ë””ì–¸íŠ¸ (ê³µí†µ)
        gradient_list.append(np.array([grad_dict['choice']['grad_intercept']]))
        gradient_list.append(grad_dict['choice']['grad_beta'])
        gradient_list.append(np.array([grad_dict['choice']['grad_lambda']]))

        # ì‚¬íšŒì¸êµ¬í•™ì  ë³€ìˆ˜ê°€ ì„ íƒëª¨ë¸ì— í¬í•¨ë˜ëŠ” ê²½ìš°
        if hasattr(self.config.structural, 'include_in_choice') and self.config.structural.include_in_choice:
            # í˜„ì¬ëŠ” êµ¬í˜„ë˜ì§€ ì•ŠìŒ
            n_sociodem = len(self.config.structural.sociodemographics)
            gradient_list.append(np.zeros(n_sociodem))

        # ë²¡í„°ë¡œ ê²°í•©
        gradient_vector = np.concatenate(gradient_list)

        return gradient_vector

    def _process_results(self, optimization_result,
                        measurement_model,
                        structural_model,
                        choice_model) -> Dict:
        """ìµœì í™” ê²°ê³¼ ì²˜ë¦¬"""
        
        param_dict = self._unpack_parameters(
            optimization_result.x,
            measurement_model,
            structural_model,
            choice_model
        )
        
        results = {
            'success': optimization_result.success,
            'message': optimization_result.message,
            'log_likelihood': -optimization_result.fun,
            'n_iterations': optimization_result.nit,
            'parameters': param_dict,
            'raw_params': optimization_result.x,
            
            # ëª¨ë¸ ì í•©ë„
            'n_observations': len(self.data),
            'n_parameters': len(optimization_result.x),
        }
        
        # AIC, BIC ê³„ì‚°
        ll = results['log_likelihood']
        k = results['n_parameters']
        n = results['n_observations']
        
        results['aic'] = -2 * ll + 2 * k
        results['bic'] = -2 * ll + k * np.log(n)
        
        # í‘œì¤€ì˜¤ì°¨ ê³„ì‚° (Hessian ê¸°ë°˜)
        if self.config.estimation.calculate_se:
            try:
                # BFGSëŠ” hess_invë¥¼ ë°˜í™˜ (ì—­ Hessian)
                # í‘œì¤€ì˜¤ì°¨ = sqrt(diag(H^-1))
                if hasattr(optimization_result, 'hess_inv'):
                    hess_inv = optimization_result.hess_inv
                    if hasattr(hess_inv, 'todense'):
                        hess_inv = hess_inv.todense()

                    # ëŒ€ê° ì›ì†Œ ì¶”ì¶œ (ë¶„ì‚°)
                    variances = np.diag(hess_inv)

                    # ìŒìˆ˜ ë¶„ì‚° ì²˜ë¦¬ (ìˆ˜ì¹˜ ì˜¤ë¥˜)
                    variances = np.maximum(variances, 1e-10)

                    se = np.sqrt(variances)
                    results['standard_errors'] = se

                    # t-í†µê³„ëŸ‰
                    results['t_statistics'] = optimization_result.x / se

                    # p-ê°’ (ì–‘ì¸¡ ê²€ì •, ëŒ€í‘œë³¸ì´ë¯€ë¡œ ì •ê·œë¶„í¬ ì‚¬ìš©)
                    from scipy.stats import norm
                    results['p_values'] = 2 * (1 - norm.cdf(np.abs(results['t_statistics'])))

                    # íŒŒë¼ë¯¸í„°ë³„ë¡œ êµ¬ì¡°í™”
                    self.logger.info("íŒŒë¼ë¯¸í„°ë³„ í†µê³„ëŸ‰ êµ¬ì¡°í™” ì¤‘...")
                    results['parameter_statistics'] = self._structure_statistics(
                        optimization_result.x, se,
                        results['t_statistics'], results['p_values'],
                        measurement_model, structural_model, choice_model
                    )
                    self.logger.info("íŒŒë¼ë¯¸í„°ë³„ í†µê³„ëŸ‰ êµ¬ì¡°í™” ì™„ë£Œ")

                else:
                    self.logger.warning("Hessian ì •ë³´ê°€ ì—†ì–´ í‘œì¤€ì˜¤ì°¨ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            except Exception as e:
                self.logger.warning(f"í‘œì¤€ì˜¤ì°¨ ê³„ì‚° ì‹¤íŒ¨: {e}")
                import traceback
                self.logger.debug(traceback.format_exc())

        return results

    def _structure_statistics(self, estimates, std_errors, t_stats, p_values,
                              measurement_model, structural_model, choice_model):
        """
        íŒŒë¼ë¯¸í„°ë³„ í†µê³„ëŸ‰ì„ êµ¬ì¡°í™”ëœ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜

        Args:
            estimates: ì¶”ì •ê°’ ë²¡í„°
            std_errors: í‘œì¤€ì˜¤ì°¨ ë²¡í„°
            t_stats: t-í†µê³„ëŸ‰ ë²¡í„°
            p_values: p-value ë²¡í„°
            measurement_model: ì¸¡ì •ëª¨ë¸
            structural_model: êµ¬ì¡°ëª¨ë¸
            choice_model: ì„ íƒëª¨ë¸

        Returns:
            êµ¬ì¡°í™”ëœ í†µê³„ëŸ‰ ë”•ì…”ë„ˆë¦¬
            {
                'measurement': {'zeta': {...}, 'tau': {...}},
                'structural': {'gamma': {...}},
                'choice': {'intercept': {...}, 'beta': {...}, 'lambda': {...}}
            }
        """
        # íŒŒë¼ë¯¸í„° ì–¸íŒ©
        param_dict = self._unpack_parameters(
            estimates, measurement_model, structural_model, choice_model
        )

        # ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ í‘œì¤€ì˜¤ì°¨, t-í†µê³„ëŸ‰, p-value ì–¸íŒ©
        se_dict = self._unpack_parameters(
            std_errors, measurement_model, structural_model, choice_model
        )
        t_dict = self._unpack_parameters(
            t_stats, measurement_model, structural_model, choice_model
        )
        p_dict = self._unpack_parameters(
            p_values, measurement_model, structural_model, choice_model
        )

        # êµ¬ì¡°í™”ëœ ê²°ê³¼ ìƒì„±
        structured = {
            'measurement': {},
            'structural': {},
            'choice': {}
        }

        # ì¸¡ì •ëª¨ë¸
        if 'measurement' in param_dict:
            for key in param_dict['measurement']:
                structured['measurement'][key] = {
                    'estimate': param_dict['measurement'][key],
                    'std_error': se_dict['measurement'][key],
                    't_statistic': t_dict['measurement'][key],
                    'p_value': p_dict['measurement'][key]
                }

        # êµ¬ì¡°ëª¨ë¸
        if 'structural' in param_dict:
            for key in param_dict['structural']:
                structured['structural'][key] = {
                    'estimate': param_dict['structural'][key],
                    'std_error': se_dict['structural'][key],
                    't_statistic': t_dict['structural'][key],
                    'p_value': p_dict['structural'][key]
                }

        # ì„ íƒëª¨ë¸
        if 'choice' in param_dict:
            for key in param_dict['choice']:
                structured['choice'][key] = {
                    'estimate': param_dict['choice'][key],
                    'std_error': se_dict['choice'][key],
                    't_statistic': t_dict['choice'][key],
                    'p_value': p_dict['choice'][key]
                }

        return structured


def estimate_iclv_simultaneous(data: pd.DataFrame, config,
                               measurement_model,
                               structural_model,
                               choice_model) -> Dict:
    """
    ICLV ëª¨ë¸ ë™ì‹œ ì¶”ì • í—¬í¼ í•¨ìˆ˜
    
    Args:
        data: í†µí•© ë°ì´í„°
        config: ICLVConfig
        measurement_model: ì¸¡ì •ëª¨ë¸
        structural_model: êµ¬ì¡°ëª¨ë¸
        choice_model: ì„ íƒëª¨ë¸
    
    Returns:
        ì¶”ì • ê²°ê³¼
    """
    estimator = SimultaneousEstimator(config)
    return estimator.estimate(data, measurement_model, structural_model, choice_model)

