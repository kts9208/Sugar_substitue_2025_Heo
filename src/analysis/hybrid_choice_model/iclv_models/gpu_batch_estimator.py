"""
GPU Batch Processing ICLV Estimator

ì™„ì „í•œ GPU ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë‹¤ì¤‘ ì ì¬ë³€ìˆ˜ ICLV ëª¨ë¸ì„ ì¶”ì •í•©ë‹ˆë‹¤.
ëª¨ë“  ê°œì¸ Ã— drawsë¥¼ í•œ ë²ˆì— GPUì—ì„œ ì²˜ë¦¬í•˜ì—¬ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple
import logging
import time
from scipy import optimize
from scipy.special import logsumexp

from .gpu_measurement_equations import GPUMultiLatentMeasurement, GPU_AVAILABLE
from .multi_latent_structural import MultiLatentStructural
from .choice_equations import BinaryProbitChoice

logger = logging.getLogger(__name__)

# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€
if GPU_AVAILABLE:
    import cupy as cp
    from cupyx.scipy.special import ndtr
    logger.info("âœ… GPU ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“œ")
else:
    logger.warning("âš ï¸ CuPy ë¯¸ì„¤ì¹˜ - CPU ëª¨ë“œë¡œ ì‘ë™")


class GPUBatchEstimator:
    """
    GPU ë°°ì¹˜ ì²˜ë¦¬ ICLV ë™ì‹œì¶”ì •
    
    ëª¨ë“  ê°œì¸ Ã— drawsë¥¼ í•˜ë‚˜ì˜ ë°°ì¹˜ë¡œ GPUì—ì„œ ì²˜ë¦¬í•˜ì—¬
    ìµœëŒ€ ì„±ëŠ¥ì„ ë‹¬ì„±í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, config, data: pd.DataFrame, use_gpu: bool = True):
        """
        ì´ˆê¸°í™”
        
        Args:
            config: MultiLatentConfig ê°ì²´
            data: í†µí•© ë°ì´í„°
            use_gpu: GPU ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
        """
        self.config = config
        self.data = data
        self.use_gpu = use_gpu and GPU_AVAILABLE
        
        # ëª¨ë¸ ìƒì„±
        self.measurement_model = GPUMultiLatentMeasurement(
            config.measurement_configs, 
            use_gpu=self.use_gpu
        )
        self.structural_model = MultiLatentStructural(config.structural)
        self.choice_model = BinaryProbitChoice(config.choice)
        
        # ê°œì¸ ID ëª©ë¡
        self.individual_ids = data[config.individual_id_column].unique()
        self.n_individuals = len(self.individual_ids)
        
        # Halton draws ìƒì„±
        n_draws = config.estimation.n_draws
        n_dimensions = config.structural.n_exo + 1  # ì™¸ìƒ LV + ë‚´ìƒ LV ì˜¤ì°¨
        
        self.halton_generator = HaltonDrawGenerator(
            self.n_individuals, n_draws, n_dimensions
        )
        
        # ë°°ì¹˜ í¬ê¸°
        self.batch_size = self.n_individuals * n_draws
        
        # ë¡œê¹…
        n_measurement_params = self.measurement_model.get_n_parameters()
        n_structural_params = config.structural.n_exo + config.structural.n_cov
        n_choice_params = 1 + len(config.choice.choice_attributes) + 1
        total_params = n_measurement_params + n_structural_params + n_choice_params
        
        gpu_status = "ğŸš€ GPU ë°°ì¹˜" if self.use_gpu else "ğŸ’» CPU"
        logger.info("=" * 70)
        logger.info(f"{gpu_status} Estimator ì´ˆê¸°í™”")
        logger.info(f"  ê°œì¸ ìˆ˜: {self.n_individuals:,}")
        logger.info(f"  ê´€ì¸¡ì¹˜ ìˆ˜: {len(data):,}")
        logger.info(f"  Halton draws: {n_draws}")
        logger.info(f"  ë°°ì¹˜ í¬ê¸°: {self.batch_size:,} (ê°œì¸ Ã— draws)")
        logger.info(f"  ì¸¡ì •ëª¨ë¸ íŒŒë¼ë¯¸í„°: {n_measurement_params}")
        logger.info(f"  êµ¬ì¡°ëª¨ë¸ íŒŒë¼ë¯¸í„°: {n_structural_params}")
        logger.info(f"  ì„ íƒëª¨ë¸ íŒŒë¼ë¯¸í„°: {n_choice_params}")
        logger.info(f"  ì´ íŒŒë¼ë¯¸í„°: {total_params}")
        logger.info("=" * 70)
    
    def _prepare_batch_data(self) -> Tuple[Dict, Dict]:
        """
        ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
        
        Returns:
            indicator_data: {lv_name: (n_individuals, n_indicators)} ì§€í‘œ ë°ì´í„°
            choice_data: (n_individuals, n_choice_situations, n_attributes) ì„ íƒ ë°ì´í„°
        """
        # ê° ê°œì¸ì˜ ì²« ë²ˆì§¸ í–‰ì—ì„œ ì§€í‘œ ë°ì´í„° ì¶”ì¶œ
        indicator_data = {}
        
        for lv_name, config in self.config.measurement_configs.items():
            n_indicators = len(config.indicators)
            data_array = np.zeros((self.n_individuals, n_indicators))
            
            for i, ind_id in enumerate(self.individual_ids):
                ind_data = self.data[self.data[self.config.individual_id_column] == ind_id]
                first_row = ind_data.iloc[0]
                
                for j, indicator in enumerate(config.indicators):
                    if indicator in first_row.index and not pd.isna(first_row[indicator]):
                        data_array[i, j] = first_row[indicator]
                    else:
                        data_array[i, j] = 0  # NaNì€ 0ìœ¼ë¡œ (ë‚˜ì¤‘ì— ë§ˆìŠ¤í‚¹)
            
            indicator_data[lv_name] = data_array
        
        # ì„ íƒ ë°ì´í„° ì¤€ë¹„
        choice_data = self._prepare_choice_data()
        
        return indicator_data, choice_data
    
    def _prepare_choice_data(self) -> Dict:
        """ì„ íƒ ë°ì´í„° ì¤€ë¹„"""
        choice_data = {
            'individual_ids': [],
            'choices': [],
            'attributes': []
        }

        for ind_id in self.individual_ids:
            ind_data = self.data[self.data[self.config.individual_id_column] == ind_id]

            # NaNì´ ìˆëŠ” í–‰ ì œê±° (alternative=3ì¸ "ì„ íƒí•˜ì§€ ì•ŠìŒ" ì˜µì…˜)
            # ì„ íƒ ì†ì„± ì¤‘ í•˜ë‚˜ë¼ë„ NaNì´ë©´ ì œì™¸
            valid_mask = ~ind_data[self.config.choice.choice_attributes].isna().any(axis=1)
            ind_data_valid = ind_data[valid_mask]

            choice_data['individual_ids'].append(ind_id)
            choice_data['choices'].append(ind_data_valid[self.config.choice_column].values)

            # ì†ì„± ë°ì´í„°
            attr_values = []
            for attr in self.config.choice.choice_attributes:
                attr_values.append(ind_data_valid[attr].values)
            choice_data['attributes'].append(np.column_stack(attr_values))

        return choice_data
    
    def _compute_batch_likelihood(self, params: np.ndarray) -> float:
        """
        ë°°ì¹˜ ìš°ë„ ê³„ì‚° (GPU ê°€ì†)

        ëª¨ë“  ê°œì¸ Ã— drawsë¥¼ í•œ ë²ˆì— ì²˜ë¦¬
        """
        t_start = time.time()

        # íŒŒë¼ë¯¸í„° ë¶„í•´
        param_dict = self._unpack_parameters(params)
        t1 = time.time()

        # Halton draws ê°€ì ¸ì˜¤ê¸°
        draws = self.halton_generator.get_draws()  # (n_individuals, n_draws, n_dimensions)
        t2 = time.time()

        # ë°ì´í„° ì¤€ë¹„
        indicator_data, choice_data = self._prepare_batch_data()
        t3 = time.time()

        # ë°°ì¹˜ í™•ì¥: (n_individuals, n_draws, ...)
        n_draws = draws.shape[1]
        n_exo = self.config.structural.n_exo

        # ëª¨ë“  ê°œì¸ Ã— drawsì— ëŒ€í•œ ì ì¬ë³€ìˆ˜ ê³„ì‚°
        all_latent_vars = {}  # {lv_name: (batch_size,)}

        # ë°°ì¹˜ ì¸ë±ìŠ¤ ìƒì„±
        batch_indices = []
        for i in range(self.n_individuals):
            for d in range(n_draws):
                batch_indices.append((i, d))

        # êµ¬ì¡°ëª¨ë¸: ëª¨ë“  ë°°ì¹˜ì— ëŒ€í•œ ì ì¬ë³€ìˆ˜ ì˜ˆì¸¡
        all_latent_vars = self._compute_batch_latent_vars(
            draws, param_dict['structural'], batch_indices
        )
        t4 = time.time()

        # ì¸¡ì •ëª¨ë¸ ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„
        measurement_batch_data = self._prepare_measurement_batch(
            indicator_data, batch_indices
        )
        t5 = time.time()

        # ì¸¡ì •ëª¨ë¸ ìš°ë„ (GPU ë°°ì¹˜)
        ll_measurement_batch = self.measurement_model.log_likelihood_batch(
            measurement_batch_data,
            all_latent_vars,
            param_dict['measurement']
        )  # (batch_size,)
        t6 = time.time()

        # ì„ íƒëª¨ë¸ ìš°ë„ (ë°°ì¹˜)
        ll_choice_batch = self._compute_choice_batch_likelihood(
            choice_data, all_latent_vars, param_dict['choice'], batch_indices
        )  # (batch_size,)
        t7 = time.time()

        # êµ¬ì¡°ëª¨ë¸ ìš°ë„ (ë°°ì¹˜)
        ll_structural_batch = self._compute_structural_batch_likelihood(
            draws, all_latent_vars, param_dict['structural'], batch_indices
        )  # (batch_size,)
        t8 = time.time()
        
        # ì „ì²´ ìš°ë„
        ll_total_batch = ll_measurement_batch + ll_choice_batch + ll_structural_batch

        # ê°œì¸ë³„ë¡œ ì¬êµ¬ì„± ë° ì‹œë®¬ë ˆì´ì…˜ í‰ê· 
        ll_total_batch = ll_total_batch.reshape(self.n_individuals, n_draws)

        # ê° ê°œì¸ë³„ ë¡œê·¸ ì‹œë®¬ë ˆì´ì…˜ í‰ê· 
        person_lls = logsumexp(ll_total_batch, axis=1) - np.log(n_draws)

        # ì „ì²´ ë¡œê·¸ìš°ë„
        total_ll = np.sum(person_lls)

        t_total = time.time() - t_start

        # íƒ€ì´ë° ë¡œê·¸ ì¶œë ¥
        print(f"  [ì‹œê°„] íŒŒë¼ë¯¸í„°:{t1-t_start:.2f}s | Draws:{t2-t1:.2f}s | ë°ì´í„°:{t3-t2:.2f}s | "
              f"ì ì¬ë³€ìˆ˜:{t4-t3:.2f}s | ì¸¡ì •ì¤€ë¹„:{t5-t4:.2f}s")
        print(f"  [ì‹œê°„] ì¸¡ì •ìš°ë„:{t6-t5:.2f}s (GPU) | ì„ íƒìš°ë„:{t7-t6:.2f}s | êµ¬ì¡°ìš°ë„:{t8-t7:.2f}s | ì´:{t_total:.2f}s")
        print(f"  [ìš°ë„] LL = {total_ll:.2f} (ì¸¡ì •:{np.sum(ll_measurement_batch):.2f}, "
              f"ì„ íƒ:{np.sum(ll_choice_batch):.2f}, êµ¬ì¡°:{np.sum(ll_structural_batch):.2f})")

        return total_ll
    
    def _compute_batch_latent_vars(self, draws, structural_params, batch_indices):
        """ë°°ì¹˜ ì ì¬ë³€ìˆ˜ ê³„ì‚°"""
        batch_size = len(batch_indices)
        n_exo = self.config.structural.n_exo
        
        # ì™¸ìƒ ì ì¬ë³€ìˆ˜
        latent_vars = {}
        exo_lvs = self.config.structural.exogenous_lvs
        
        for lv_idx, lv_name in enumerate(exo_lvs):
            lv_values = np.zeros(batch_size)
            for batch_idx, (ind_idx, draw_idx) in enumerate(batch_indices):
                lv_values[batch_idx] = draws[ind_idx, draw_idx, lv_idx]
            latent_vars[lv_name] = lv_values
        
        # ë‚´ìƒ ì ì¬ë³€ìˆ˜
        endo_lv = self.config.structural.endogenous_lv
        endo_values = np.zeros(batch_size)
        
        gamma_lv = structural_params['gamma_lv']
        gamma_x = structural_params['gamma_x']
        covariates = self.config.structural.covariates
        
        for batch_idx, (ind_idx, draw_idx) in enumerate(batch_indices):
            ind_id = self.individual_ids[ind_idx]
            ind_data = self.data[self.data[self.config.individual_id_column] == ind_id].iloc[0]
            
            # ì™¸ìƒ LV íš¨ê³¼
            endo_mean = 0.0
            for lv_idx, lv_name in enumerate(exo_lvs):
                endo_mean += gamma_lv[lv_idx] * draws[ind_idx, draw_idx, lv_idx]
            
            # ê³µë³€ëŸ‰ íš¨ê³¼
            for cov_idx, cov_name in enumerate(covariates):
                if cov_name in ind_data.index:
                    endo_mean += gamma_x[cov_idx] * ind_data[cov_name]
            
            # ì˜¤ì°¨í•­ ì¶”ê°€
            endo_error = draws[ind_idx, draw_idx, n_exo]
            endo_values[batch_idx] = endo_mean + endo_error
        
        latent_vars[endo_lv] = endo_values
        
        return latent_vars
    
    def _prepare_measurement_batch(self, indicator_data, batch_indices):
        """ì¸¡ì •ëª¨ë¸ ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„"""
        batch_size = len(batch_indices)
        measurement_batch = {}
        
        for lv_name, data_array in indicator_data.items():
            n_indicators = data_array.shape[1]
            batch_array = np.zeros((batch_size, n_indicators))
            
            for batch_idx, (ind_idx, draw_idx) in enumerate(batch_indices):
                batch_array[batch_idx] = data_array[ind_idx]
            
            measurement_batch[lv_name] = batch_array
        
        return measurement_batch

    def _compute_choice_batch_likelihood(self, choice_data, latent_vars, choice_params, batch_indices):
        """
        ì„ íƒëª¨ë¸ ë°°ì¹˜ ìš°ë„ ê³„ì‚° (GPU ê°€ì†)

        ëª¨ë“  ë°°ì¹˜ Ã— ì„ íƒ ìƒí™©ì„ í•œ ë²ˆì— ì²˜ë¦¬
        """
        batch_size = len(batch_indices)
        beta_intercept = choice_params['intercept']
        beta = choice_params['beta']
        lambda_lv = choice_params['lambda']
        endo_lv = self.config.structural.endogenous_lv

        # GPU ëª¨ë“œ: ì™„ì „ ë²¡í„°í™”
        if self.use_gpu:
            return self._compute_choice_batch_likelihood_gpu(
                choice_data, latent_vars, choice_params, batch_indices
            )

        # CPU ëª¨ë“œ: ê¸°ì¡´ ë£¨í”„ ë°©ì‹
        ll_choice = np.zeros(batch_size)
        for batch_idx, (ind_idx, draw_idx) in enumerate(batch_indices):
            choices = choice_data['choices'][ind_idx]
            attributes = choice_data['attributes'][ind_idx]
            lv_value = latent_vars[endo_lv][batch_idx]

            # ê° ì„ íƒ ìƒí™©ì— ëŒ€í•´
            for t in range(len(choices)):
                # íš¨ìš©: V = Î²0 + Î²*X + Î»*LV
                utility = beta_intercept
                utility += np.dot(beta, attributes[t])
                utility += lambda_lv * lv_value

                # Probit í™•ë¥  (ì•ˆì „í•œ ê³„ì‚°)
                from scipy.stats import norm
                cdf_val = norm.cdf(utility)
                # ìˆ˜ì¹˜ ì•ˆì •ì„±ì„ ìœ„í•´ í´ë¦¬í•‘
                cdf_val = np.clip(cdf_val, 1e-10, 1 - 1e-10)

                if choices[t] == 1:
                    prob = cdf_val
                else:
                    prob = 1 - cdf_val

                ll_choice[batch_idx] += np.log(prob)

        return ll_choice

    def _compute_choice_batch_likelihood_gpu(self, choice_data, latent_vars, choice_params, batch_indices):
        """
        ì„ íƒëª¨ë¸ GPU ë°°ì¹˜ ìš°ë„ ê³„ì‚°

        ëª¨ë“  ê°œì¸ Ã— draws Ã— ì„ íƒ ìƒí™©ì„ í•˜ë‚˜ì˜ í° ë°°ì—´ë¡œ ì²˜ë¦¬
        """
        beta_intercept = choice_params['intercept']
        beta = choice_params['beta']
        lambda_lv = choice_params['lambda']
        endo_lv = self.config.structural.endogenous_lv

        # 1. ëª¨ë“  ì„ íƒ ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ ë°°ì—´ë¡œ ìˆ˜ì§‘
        all_choices = []
        all_attributes = []
        all_lv_values = []
        batch_choice_counts = []  # ê° ë°°ì¹˜ì˜ ì„ íƒ ê°œìˆ˜

        for batch_idx, (ind_idx, draw_idx) in enumerate(batch_indices):
            choices = choice_data['choices'][ind_idx]
            attributes = choice_data['attributes'][ind_idx]
            lv_value = latent_vars[endo_lv][batch_idx]

            n_choices = len(choices)
            batch_choice_counts.append(n_choices)

            all_choices.extend(choices)
            all_attributes.append(attributes)
            all_lv_values.extend([lv_value] * n_choices)

        # NumPy ë°°ì—´ë¡œ ë³€í™˜
        all_choices = np.array(all_choices)  # (total_choices,)
        all_attributes = np.vstack(all_attributes)  # (total_choices, n_attrs)
        all_lv_values = np.array(all_lv_values)  # (total_choices,)

        # 2. GPUë¡œ ì „ì†¡
        all_choices_gpu = cp.asarray(all_choices)
        all_attributes_gpu = cp.asarray(all_attributes)
        all_lv_values_gpu = cp.asarray(all_lv_values)
        beta_gpu = cp.asarray(beta)

        # 3. íš¨ìš© ê³„ì‚° (ë²¡í„°í™”)
        # V = Î²0 + Î²*X + Î»*LV
        utilities = beta_intercept + cp.dot(all_attributes_gpu, beta_gpu) + lambda_lv * all_lv_values_gpu

        # 4. Probit í™•ë¥  ê³„ì‚° (GPU)
        cdf_vals = ndtr(utilities)  # GPUì—ì„œ í•œ ë²ˆì—!
        cdf_vals = cp.clip(cdf_vals, 1e-10, 1 - 1e-10)

        # 5. ì„ íƒì— ë”°ë¥¸ í™•ë¥ 
        probs = cp.where(all_choices_gpu == 1, cdf_vals, 1 - cdf_vals)

        # 6. ë¡œê·¸ í™•ë¥ 
        log_probs = cp.log(probs)

        # 7. ê° ë°°ì¹˜ë³„ë¡œ í•©ì‚°
        ll_choice = np.zeros(len(batch_indices))
        start_idx = 0
        for batch_idx, n_choices in enumerate(batch_choice_counts):
            end_idx = start_idx + n_choices
            ll_choice[batch_idx] = float(cp.sum(log_probs[start_idx:end_idx]))
            start_idx = end_idx

        return ll_choice

    def _compute_structural_batch_likelihood(self, draws, latent_vars, structural_params, batch_indices):
        """
        êµ¬ì¡°ëª¨ë¸ ë°°ì¹˜ ìš°ë„ ê³„ì‚° (GPU ê°€ì†)

        ëª¨ë“  ë°°ì¹˜ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬
        """
        # GPU ëª¨ë“œ: ì™„ì „ ë²¡í„°í™”
        if self.use_gpu:
            return self._compute_structural_batch_likelihood_gpu(
                draws, latent_vars, structural_params, batch_indices
            )

        # CPU ëª¨ë“œ: ê¸°ì¡´ ë£¨í”„ ë°©ì‹
        batch_size = len(batch_indices)
        n_exo = self.config.structural.n_exo
        endo_lv = self.config.structural.endogenous_lv
        exo_lvs = self.config.structural.exogenous_lvs

        gamma_lv = structural_params['gamma_lv']
        gamma_x = structural_params['gamma_x']
        covariates = self.config.structural.covariates
        error_variance = self.config.structural.error_variance

        ll_structural = np.zeros(batch_size)

        for batch_idx, (ind_idx, draw_idx) in enumerate(batch_indices):
            ind_id = self.individual_ids[ind_idx]
            ind_data = self.data[self.data[self.config.individual_id_column] == ind_id].iloc[0]

            # ë‚´ìƒ LV ì˜ˆì¸¡ê°’
            endo_mean = 0.0
            for lv_idx, lv_name in enumerate(exo_lvs):
                endo_mean += gamma_lv[lv_idx] * draws[ind_idx, draw_idx, lv_idx]

            for cov_idx, cov_name in enumerate(covariates):
                if cov_name in ind_data.index:
                    endo_mean += gamma_x[cov_idx] * ind_data[cov_name]

            # ì˜¤ì°¨í•­ ìš°ë„
            endo_error = draws[ind_idx, draw_idx, n_exo]
            endo_actual = latent_vars[endo_lv][batch_idx]
            residual = endo_actual - endo_mean

            # ì •ê·œë¶„í¬ ë¡œê·¸ìš°ë„
            ll_structural[batch_idx] = -0.5 * np.log(2 * np.pi * error_variance)
            ll_structural[batch_idx] -= 0.5 * (residual ** 2) / error_variance

        return ll_structural

    def _compute_structural_batch_likelihood_gpu(self, draws, latent_vars, structural_params, batch_indices):
        """
        êµ¬ì¡°ëª¨ë¸ GPU ë°°ì¹˜ ìš°ë„ ê³„ì‚°

        ëª¨ë“  ë°°ì¹˜ì˜ êµ¬ì¡°ëª¨ë¸ ìš°ë„ë¥¼ ë²¡í„° ì—°ì‚°ìœ¼ë¡œ í•œ ë²ˆì— ê³„ì‚°
        """
        batch_size = len(batch_indices)
        n_exo = self.config.structural.n_exo
        endo_lv = self.config.structural.endogenous_lv
        exo_lvs = self.config.structural.exogenous_lvs

        gamma_lv = structural_params['gamma_lv']
        gamma_x = structural_params['gamma_x']
        covariates = self.config.structural.covariates
        error_variance = self.config.structural.error_variance

        # 1. ì™¸ìƒ LV ê¸°ì—¬ë„ ê³„ì‚° (ë°°ì¹˜ ì „ì²´)
        # draws: (n_individuals, n_draws, n_dimensions)
        # ê° ë°°ì¹˜ì— ëŒ€í•œ ì™¸ìƒ LV ê°’ ì¶”ì¶œ
        exo_lv_values = np.zeros((batch_size, n_exo))
        for batch_idx, (ind_idx, draw_idx) in enumerate(batch_indices):
            exo_lv_values[batch_idx, :] = draws[ind_idx, draw_idx, :n_exo]

        # 2. ê³µë³€ëŸ‰ ê¸°ì—¬ë„ ê³„ì‚° (ë°°ì¹˜ ì „ì²´)
        cov_values = np.zeros((batch_size, len(covariates)))
        for batch_idx, (ind_idx, draw_idx) in enumerate(batch_indices):
            ind_id = self.individual_ids[ind_idx]
            ind_data = self.data[self.data[self.config.individual_id_column] == ind_id].iloc[0]
            for cov_idx, cov_name in enumerate(covariates):
                if cov_name in ind_data.index:
                    cov_values[batch_idx, cov_idx] = ind_data[cov_name]

        # 3. GPUë¡œ ì „ì†¡
        exo_lv_values_gpu = cp.asarray(exo_lv_values)  # (batch_size, n_exo)
        cov_values_gpu = cp.asarray(cov_values)  # (batch_size, n_cov)
        gamma_lv_gpu = cp.asarray(gamma_lv)  # (n_exo,)
        gamma_x_gpu = cp.asarray(gamma_x)  # (n_cov,)

        # 4. ë‚´ìƒ LV ì˜ˆì¸¡ê°’ ê³„ì‚° (ë²¡í„°í™”)
        # endo_mean = gamma_lv @ exo_lv + gamma_x @ covariates
        endo_means = cp.dot(exo_lv_values_gpu, gamma_lv_gpu) + cp.dot(cov_values_gpu, gamma_x_gpu)

        # 5. ì‹¤ì œ ë‚´ìƒ LV ê°’
        endo_actual = np.array([latent_vars[endo_lv][i] for i in range(batch_size)])
        endo_actual_gpu = cp.asarray(endo_actual)

        # 6. ì”ì°¨ ê³„ì‚°
        residuals = endo_actual_gpu - endo_means

        # 7. ì •ê·œë¶„í¬ ë¡œê·¸ìš°ë„ (ë²¡í„°í™”)
        # ll = -0.5 * log(2Ï€*ÏƒÂ²) - 0.5 * (residualÂ² / ÏƒÂ²)
        log_const = -0.5 * cp.log(2 * cp.pi * error_variance)
        ll_structural_gpu = log_const - 0.5 * (residuals ** 2) / error_variance

        # 8. CPUë¡œ ë°˜í™˜
        ll_structural = cp.asnumpy(ll_structural_gpu)

        return ll_structural

    def _unpack_parameters(self, params: np.ndarray) -> Dict:
        """íŒŒë¼ë¯¸í„° ë²¡í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë¶„í•´"""
        idx = 0
        param_dict = {}

        # 1. ì¸¡ì •ëª¨ë¸ íŒŒë¼ë¯¸í„°
        param_dict['measurement'] = {}
        for lv_name, model in self.measurement_model.models.items():
            n_indicators = model.n_indicators
            n_thresholds = model.n_thresholds

            zeta = params[idx:idx + n_indicators]
            idx += n_indicators

            tau = params[idx:idx + n_indicators * n_thresholds]
            tau = tau.reshape(n_indicators, n_thresholds)
            idx += n_indicators * n_thresholds

            param_dict['measurement'][lv_name] = {'zeta': zeta, 'tau': tau}

        # 2. êµ¬ì¡°ëª¨ë¸ íŒŒë¼ë¯¸í„°
        n_exo = self.config.structural.n_exo
        n_cov = self.config.structural.n_cov

        gamma_lv = params[idx:idx + n_exo]
        idx += n_exo

        gamma_x = params[idx:idx + n_cov]
        idx += n_cov

        param_dict['structural'] = {'gamma_lv': gamma_lv, 'gamma_x': gamma_x}

        # 3. ì„ íƒëª¨ë¸ íŒŒë¼ë¯¸í„°
        beta_intercept = params[idx]
        idx += 1

        n_choice_attrs = len(self.config.choice.choice_attributes)
        beta = params[idx:idx + n_choice_attrs]
        idx += n_choice_attrs

        lambda_lv = params[idx]
        idx += 1

        param_dict['choice'] = {
            'intercept': beta_intercept,
            'beta': beta,
            'lambda': lambda_lv
        }

        return param_dict

    def estimate(self, initial_params: np.ndarray = None,
                method: str = 'BFGS', maxiter: int = 100) -> Dict:
        """
        ëª¨ë¸ ì¶”ì •

        Args:
            initial_params: ì´ˆê¸° íŒŒë¼ë¯¸í„° (Noneì´ë©´ ìë™ ìƒì„±)
            method: ìµœì í™” ë°©ë²•
            maxiter: ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜

        Returns:
            ì¶”ì • ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if initial_params is None:
            initial_params = self._initialize_parameters()

        logger.info("=" * 70)
        logger.info("GPU ë°°ì¹˜ ì¶”ì • ì‹œì‘")
        logger.info(f"  ì´ˆê¸° íŒŒë¼ë¯¸í„° ìˆ˜: {len(initial_params)}")
        logger.info(f"  ìµœì í™” ë°©ë²•: {method}")
        logger.info(f"  ìµœëŒ€ ë°˜ë³µ: {maxiter}")
        logger.info("=" * 70)

        start_time = time.time()

        # ì½œë°± í•¨ìˆ˜
        self.iteration = 0
        self.best_ll = -np.inf

        def callback(params):
            self.iteration += 1
            ll = self._compute_batch_likelihood(params)

            if ll > self.best_ll:
                self.best_ll = ll

            if self.iteration % 5 == 0:
                elapsed = time.time() - start_time
                logger.info(f"  ë°˜ë³µ {self.iteration:3d} | LL = {ll:12.2f} | "
                          f"Best = {self.best_ll:12.2f} | ì‹œê°„ = {elapsed:.1f}s")

        # ëª©ì í•¨ìˆ˜ (ìŒì˜ ë¡œê·¸ìš°ë„)
        def objective(params):
            print(f"\n=== ë°˜ë³µ {self.iteration + 1} ===")
            ll = self._compute_batch_likelihood(params)
            print(f"=== ë°˜ë³µ {self.iteration + 1} ì™„ë£Œ ===\n")
            return -ll

        # ìµœì í™”
        result = optimize.minimize(
            objective,
            initial_params,
            method=method,
            callback=callback,
            options={'maxiter': maxiter, 'disp': True}
        )

        elapsed_time = time.time() - start_time

        # ê²°ê³¼ ì •ë¦¬
        final_params = result.x
        final_ll = -result.fun

        logger.info("=" * 70)
        logger.info("ì¶”ì • ì™„ë£Œ!")
        logger.info(f"  ìµœì¢… LL: {final_ll:.2f}")
        logger.info(f"  ë°˜ë³µ íšŸìˆ˜: {self.iteration}")
        logger.info(f"  ì†Œìš” ì‹œê°„: {elapsed_time:.1f}ì´ˆ")
        logger.info(f"  ìˆ˜ë ´ ì—¬ë¶€: {result.success}")
        logger.info("=" * 70)

        return {
            'params': final_params,
            'log_likelihood': final_ll,
            'iterations': self.iteration,
            'time': elapsed_time,
            'success': result.success,
            'message': result.message
        }

    def _initialize_parameters(self) -> np.ndarray:
        """íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”"""
        params_list = []

        # ì¸¡ì •ëª¨ë¸
        for lv_name, model in self.measurement_model.models.items():
            init_params = model.initialize_parameters()
            params_list.append(init_params['zeta'])
            params_list.append(init_params['tau'].flatten())

        # êµ¬ì¡°ëª¨ë¸
        n_exo = self.config.structural.n_exo
        n_cov = self.config.structural.n_cov
        params_list.append(np.ones(n_exo) * 0.5)
        params_list.append(np.zeros(n_cov))

        # ì„ íƒëª¨ë¸
        params_list.append(np.array([0.0]))  # intercept
        n_choice_attrs = len(self.config.choice.choice_attributes)
        params_list.append(np.zeros(n_choice_attrs))
        params_list.append(np.array([1.0]))  # lambda

        return np.concatenate(params_list)


class HaltonDrawGenerator:
    """Halton ì‹œí€€ìŠ¤ ìƒì„±ê¸°"""

    def __init__(self, n_individuals: int, n_draws: int, n_dimensions: int, seed: int = 42):
        self.n_individuals = n_individuals
        self.n_draws = n_draws
        self.n_dimensions = n_dimensions
        self.seed = seed
        self._draws = None

    def get_draws(self) -> np.ndarray:
        """Halton draws ìƒì„± ë˜ëŠ” ë°˜í™˜"""
        if self._draws is None:
            self._draws = self._generate_halton_draws()
        return self._draws

    def _generate_halton_draws(self) -> np.ndarray:
        """Halton ì‹œí€€ìŠ¤ ìƒì„±"""
        from scipy.stats import qmc

        # Halton ì‹œí€€ìŠ¤ ìƒì„±ê¸°
        sampler = qmc.Halton(d=self.n_dimensions, scramble=True, seed=self.seed)

        # ê· ë“±ë¶„í¬ ìƒ˜í”Œ
        n_total = self.n_individuals * self.n_draws
        uniform_samples = sampler.random(n=n_total)

        # í‘œì¤€ì •ê·œë¶„í¬ë¡œ ë³€í™˜
        from scipy.stats import norm
        normal_samples = norm.ppf(uniform_samples)

        # (n_individuals, n_draws, n_dimensions)ë¡œ ì¬êµ¬ì„±
        draws = normal_samples.reshape(self.n_individuals, self.n_draws, self.n_dimensions)

        logger.info(f"Halton draws ìƒì„±: {draws.shape}")

        return draws

