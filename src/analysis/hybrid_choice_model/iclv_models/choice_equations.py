"""
Choice Equations for ICLV Models

ICLV ì„ íƒëª¨ë¸: ì†ì„± + ì ì¬ë³€ìˆ˜ â†’ ì„ íƒ

Based on King (2022) Apollo R code implementation.

Author: Sugar Substitute Research Team
Date: 2025-11-05
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Tuple
from scipy import stats
from scipy.stats import norm
from scipy.optimize import minimize
import logging
from dataclasses import dataclass

# ChoiceConfig ì •ì˜ (import ì˜¤ë¥˜ ë°©ì§€)
try:
    from .iclv_config import ChoiceConfig
except ImportError:
    from typing import Literal
    
    @dataclass
    class ChoiceConfig:
        """ì„ íƒëª¨ë¸ ì„¤ì •"""
        choice_attributes: List[str]
        choice_type: Literal['binary', 'multinomial', 'ordered'] = 'binary'
        price_variable: str = 'price'
        initial_betas: Optional[Dict[str, float]] = None
        initial_lambda: float = 1.0
        thresholds: Optional[List[float]] = None

logger = logging.getLogger(__name__)


class BinaryProbitChoice:
    """
    Binary Probit ì„ íƒëª¨ë¸ (ICLVìš©)
    
    Model:
        V = intercept + Î²*X + Î»*LV
        P(Yes) = Î¦(V)
        P(No) = 1 - Î¦(V)
    
    ì—¬ê¸°ì„œ:
        - V: íš¨ìš© (Utility)
        - X: ì„ íƒ ì†ì„± (Choice attributes, e.g., price, quality)
        - Î²: ì†ì„± ê³„ìˆ˜ (Attribute coefficients)
        - Î»: ì ì¬ë³€ìˆ˜ ê³„ìˆ˜ (Latent variable coefficient)
        - LV: ì ì¬ë³€ìˆ˜ (Latent Variable)
        - Î¦: í‘œì¤€ì •ê·œ ëˆ„ì ë¶„í¬í•¨ìˆ˜
    
    King (2022) Apollo R ì½”ë“œ ê¸°ë°˜:
        op_settings = list(
            outcomeOrdered = Q6ResearchResponse,
            V = intercept + b_bid*Q6Bid + lambda*LV,
            tau = list(-100, 0),
            componentName = "choice",
            coding = c(-1, 0, 1)
        )
        P[['choice']] = apollo_op(op_settings, functionality)
    
    Usage:
        >>> config = ChoiceConfig(
        ...     choice_attributes=['price', 'quality'],
        ...     choice_type='binary',
        ...     price_variable='price'
        ... )
        >>> model = BinaryProbitChoice(config)
        >>> 
        >>> # Simultaneous ì¶”ì •ìš©
        >>> ll = model.log_likelihood(data, lv, params)
        >>> 
        >>> # ì˜ˆì¸¡ìš©
        >>> probs = model.predict_probabilities(data, lv, params)
    """
    
    def __init__(self, config: ChoiceConfig):
        """
        ì´ˆê¸°í™”
        
        Args:
            config: ì„ íƒëª¨ë¸ ì„¤ì •
        """
        self.config = config
        self.choice_attributes = config.choice_attributes
        self.price_variable = config.price_variable
        self.logger = logging.getLogger(__name__)
        
        self.logger.info(f"BinaryProbitChoice ì´ˆê¸°í™”")
        self.logger.info(f"  ì„ íƒ ì†ì„±: {self.choice_attributes}")
        self.logger.info(f"  ê°€ê²© ë³€ìˆ˜: {self.price_variable}")
    
    def log_likelihood(self, data: pd.DataFrame, lv: np.ndarray,
                      params: Dict) -> float:
        """
        ì„ íƒëª¨ë¸ ë¡œê·¸ìš°ë„
        
        P(Choice|X, LV) = Î¦(V) if choice=1, 1-Î¦(V) if choice=0
        
        V = intercept + Î²*X + Î»*LV
        
        Args:
            data: ì„ íƒ ë°ì´í„° (n_obs, n_vars)
                  'choice' ì—´ í•„ìˆ˜ (0 or 1)
            lv: ì ì¬ë³€ìˆ˜ ê°’ (n_obs,) ë˜ëŠ” ìŠ¤ì¹¼ë¼
            params: {
                'intercept': float,
                'beta': np.ndarray,  # ì†ì„± ê³„ìˆ˜ (n_attributes,)
                'lambda': float      # ì ì¬ë³€ìˆ˜ ê³„ìˆ˜
            }
        
        Returns:
            ë¡œê·¸ìš°ë„ ê°’ (ìŠ¤ì¹¼ë¼)
        
        Example:
            >>> params = {
            ...     'intercept': 0.5,
            ...     'beta': np.array([-2.0, 0.3]),
            ...     'lambda': 1.5
            ... }
            >>> ll = model.log_likelihood(data, lv, params)
        """
        intercept = params['intercept']
        beta = params['beta']
        lambda_lv = params['lambda']
        
        # ì„ íƒ ì†ì„± ì¶”ì¶œ
        X = data[self.choice_attributes].values

        # ì„ íƒ ê²°ê³¼ (0 or 1)
        if 'choice' in data.columns:
            choice = data['choice'].values
        elif 'Choice' in data.columns:
            choice = data['Choice'].values
        else:
            raise ValueError("ë°ì´í„°ì— 'choice' ë˜ëŠ” 'Choice' ì—´ì´ ì—†ìŠµë‹ˆë‹¤.")

        # ì ì¬ë³€ìˆ˜ ì²˜ë¦¬ (ìŠ¤ì¹¼ë¼ ë˜ëŠ” ë°°ì—´)
        if np.isscalar(lv):
            lv_array = np.full(len(data), lv)
        else:
            lv_array = lv

        # ğŸ”´ ìˆ˜ì •: NaN ì²˜ë¦¬ (opt-out ëŒ€ì•ˆ)
        # NaNì´ ìˆëŠ” í–‰ì€ íš¨ìš©ì„ 0ìœ¼ë¡œ ì„¤ì • (opt-out ì •ê·œí™”)
        has_nan = np.isnan(X).any(axis=1)

        # íš¨ìš© ê³„ì‚°
        # V = intercept + Î²*X + Î»*LV
        V = np.zeros(len(data))
        for i in range(len(data)):
            if has_nan[i]:
                V[i] = 0.0  # opt-out: íš¨ìš© = 0
            else:
                V[i] = intercept + X[i] @ beta + lambda_lv * lv_array[i]

        # í™•ë¥  ê³„ì‚°
        # P(Yes) = Î¦(V), P(No) = 1 - Î¦(V)
        prob_yes = norm.cdf(V)

        # ìˆ˜ì¹˜ ì•ˆì •ì„±ì„ ìœ„í•´ í´ë¦¬í•‘
        prob_yes = np.clip(prob_yes, 1e-10, 1 - 1e-10)

        # ë¡œê·¸ìš°ë„
        # log L = Î£ [choice * log(Î¦(V)) + (1-choice) * log(1-Î¦(V))]
        ll = np.sum(
            choice * np.log(prob_yes) +
            (1 - choice) * np.log(1 - prob_yes)
        )

        return ll
    
    def predict_probabilities(self, data: pd.DataFrame, lv: np.ndarray,
                             params: Dict) -> np.ndarray:
        """
        ì„ íƒ í™•ë¥  ì˜ˆì¸¡
        
        P(Yes) = Î¦(intercept + Î²*X + Î»*LV)
        
        Args:
            data: ì„ íƒ ë°ì´í„°
            lv: ì ì¬ë³€ìˆ˜ ê°’
            params: íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬
        
        Returns:
            ì„ íƒ í™•ë¥  (n_obs,)
        
        Example:
            >>> probs = model.predict_probabilities(data, lv, params)
        """
        intercept = params['intercept']
        beta = params['beta']
        lambda_lv = params['lambda']
        
        # ì„ íƒ ì†ì„± ì¶”ì¶œ
        X = data[self.choice_attributes].values
        
        # ì ì¬ë³€ìˆ˜ ì²˜ë¦¬
        if np.isscalar(lv):
            lv_array = np.full(len(data), lv)
        else:
            lv_array = lv
        
        # íš¨ìš© ê³„ì‚°
        V = intercept + X @ beta + lambda_lv * lv_array
        
        # í™•ë¥  ê³„ì‚°
        prob_yes = norm.cdf(V)
        
        return prob_yes
    
    def predict(self, data: pd.DataFrame, lv: np.ndarray,
               params: Dict, threshold: float = 0.5) -> np.ndarray:
        """
        ì„ íƒ ì˜ˆì¸¡
        
        Args:
            data: ì„ íƒ ë°ì´í„°
            lv: ì ì¬ë³€ìˆ˜ ê°’
            params: íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬
            threshold: ì„ íƒ ì„ê³„ê°’ (ê¸°ë³¸: 0.5)
        
        Returns:
            ì˜ˆì¸¡ëœ ì„ íƒ (n_obs,) - 0 or 1
        
        Example:
            >>> predictions = model.predict(data, lv, params)
        """
        probs = self.predict_probabilities(data, lv, params)
        predictions = (probs >= threshold).astype(int)
        
        return predictions
    
    def get_initial_params(self, data: pd.DataFrame) -> Dict:
        """
        ì´ˆê¸° íŒŒë¼ë¯¸í„° ìƒì„±
        
        Args:
            data: ì„ íƒ ë°ì´í„°
        
        Returns:
            {'intercept': float, 'beta': np.ndarray, 'lambda': float}
        
        Example:
            >>> params = model.get_initial_params(data)
        """
        n_attributes = len(self.choice_attributes)
        
        # ê¸°ë³¸ ì´ˆê¸°ê°’
        params = {
            'intercept': 0.0,
            'beta': np.zeros(n_attributes),
            'lambda': 1.0
        }
        
        # ê°€ê²© ë³€ìˆ˜ê°€ ìˆìœ¼ë©´ ìŒìˆ˜ë¡œ ì´ˆê¸°í™”
        if self.price_variable in self.choice_attributes:
            price_idx = self.choice_attributes.index(self.price_variable)
            params['beta'][price_idx] = -1.0
        
        self.logger.info(f"ì´ˆê¸° íŒŒë¼ë¯¸í„°: {params}")
        
        return params
    
    def calculate_wtp(self, params: Dict, attribute: str) -> float:
        """
        WTP (Willingness-to-Pay) ê³„ì‚°
        
        WTP = -Î²_attribute / Î²_price
        
        Args:
            params: íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬
            attribute: WTPë¥¼ ê³„ì‚°í•  ì†ì„±
        
        Returns:
            WTP ê°’
        
        Example:
            >>> wtp = model.calculate_wtp(params, 'quality')
        """
        beta = params['beta']
        
        # ê°€ê²© ê³„ìˆ˜
        price_idx = self.choice_attributes.index(self.price_variable)
        beta_price = beta[price_idx]
        
        # ì†ì„± ê³„ìˆ˜
        attr_idx = self.choice_attributes.index(attribute)
        beta_attr = beta[attr_idx]
        
        # WTP = -Î²_attr / Î²_price
        wtp = -beta_attr / beta_price
        
        return wtp


def estimate_choice_model(data: pd.DataFrame, latent_var: np.ndarray,
                         choice_attributes: List[str],
                         price_variable: str = 'price',
                         **kwargs) -> Dict:
    """
    ì„ íƒëª¨ë¸ ì¶”ì • í—¬í¼ í•¨ìˆ˜
    
    Args:
        data: ì„ íƒ ë°ì´í„°
        latent_var: ì ì¬ë³€ìˆ˜ ê°’
        choice_attributes: ì„ íƒ ì†ì„± ë¦¬ìŠ¤íŠ¸
        price_variable: ê°€ê²© ë³€ìˆ˜ëª…
        **kwargs: ì¶”ê°€ ì„¤ì •
    
    Returns:
        ì¶”ì • ê²°ê³¼
    
    Example:
        >>> results = estimate_choice_model(
        ...     data,
        ...     latent_var,
        ...     choice_attributes=['price', 'quality'],
        ...     price_variable='price'
        ... )
    """
    config = ChoiceConfig(
        choice_attributes=choice_attributes,
        choice_type='binary',
        price_variable=price_variable,
        **kwargs
    )
    
    model = BinaryProbitChoice(config)
    
    # ê°„ë‹¨í•œ ì¶”ì • (ë¡œê·¸ìš°ë„ ìµœëŒ€í™”)
    initial_params = model.get_initial_params(data)
    
    def negative_log_likelihood(params_array):
        params = {
            'intercept': params_array[0],
            'beta': params_array[1:1+len(choice_attributes)],
            'lambda': params_array[-1]
        }
        return -model.log_likelihood(data, latent_var, params)
    
    # ì´ˆê¸°ê°’ ë°°ì—´
    x0 = np.concatenate([
        [initial_params['intercept']],
        initial_params['beta'],
        [initial_params['lambda']]
    ])
    
    # ìµœì í™”
    result = minimize(negative_log_likelihood, x0, method='BFGS')
    
    # ê²°ê³¼ ì •ë¦¬
    estimated_params = {
        'intercept': result.x[0],
        'beta': result.x[1:1+len(choice_attributes)],
        'lambda': result.x[-1],
        'log_likelihood': -result.fun,
        'success': result.success
    }
    
    return estimated_params

