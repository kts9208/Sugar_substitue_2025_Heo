"""
Multi-Latent Variable Analytic Gradient Calculator

ë‹¤ì¤‘ ì ì¬ë³€ìˆ˜ ICLV ëª¨ë¸ì„ ìœ„í•œ í•´ì„ì  ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°ê¸°ì…ë‹ˆë‹¤.

êµ¬ì¡°:
- ì™¸ìƒ LV (4ê°œ): health_concern, perceived_benefit, perceived_price, nutrition_knowledge
- ë‚´ìƒ LV (1ê°œ): purchase_intention = f(ì™¸ìƒ LV, ê³µë³€ëŸ‰)

Author: Sugar Substitute Research Team
Date: 2025-11-09
"""

import numpy as np
import pandas as pd
from typing import Dict, List
from scipy.stats import norm
import logging

# âœ… ê³µí†µ gradient ê³„ì‚° í•¨ìˆ˜ import
from .gradient_core import (
    compute_score_gradient,
    compute_ordered_probit_gradient_terms,
    compute_variance_gradient
)

logger = logging.getLogger(__name__)


class MultiLatentMeasurementGradient:
    """
    ë‹¤ì¤‘ ì ì¬ë³€ìˆ˜ ì¸¡ì •ëª¨ë¸ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
    
    ê° ì ì¬ë³€ìˆ˜ë§ˆë‹¤ ë…ë¦½ì ì¸ ì¸¡ì •ëª¨ë¸ì„ ê°€ì§€ë¯€ë¡œ,
    ê° LVì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê°œë³„ì ìœ¼ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, measurement_configs: Dict):
        """
        Args:
            measurement_configs: {lv_name: MeasurementConfig}
        """
        self.measurement_configs = measurement_configs
        self.lv_names = list(measurement_configs.keys())
        
        # ê° LVë³„ ì§€í‘œ ìˆ˜ì™€ ì¹´í…Œê³ ë¦¬ ìˆ˜
        self.n_indicators = {}
        self.n_categories = {}
        self.n_thresholds = {}
        
        for lv_name, config in measurement_configs.items():
            self.n_indicators[lv_name] = len(config.indicators)
            self.n_categories[lv_name] = config.n_categories
            self.n_thresholds[lv_name] = config.n_categories - 1
    
    def compute_gradient(self, data: pd.DataFrame, 
                        latent_vars: Dict[str, float],
                        params: Dict[str, Dict]) -> Dict[str, Dict]:
        """
        ë‹¤ì¤‘ ì ì¬ë³€ìˆ˜ ì¸¡ì •ëª¨ë¸ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
        
        ê° LVì— ëŒ€í•´ ë…ë¦½ì ìœ¼ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.
        
        Args:
            data: ê´€ì¸¡ ë°ì´í„°
            latent_vars: {lv_name: lv_value}
            params: {lv_name: {'zeta': ..., 'tau': ...}}
        
        Returns:
            {lv_name: {'grad_zeta': ..., 'grad_tau': ...}}
        """
        gradients = {}
        
        for lv_name in self.lv_names:
            lv = latent_vars[lv_name]
            lv_params = params[lv_name]
            config = self.measurement_configs[lv_name]
            
            # ë‹¨ì¼ LV ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
            grad = self._compute_single_lv_gradient(
                data, lv, lv_params, config.indicators, lv_name
            )
            
            gradients[lv_name] = grad
        
        return gradients
    
    def _compute_single_lv_gradient(self, data: pd.DataFrame, lv: float,
                                   params: Dict[str, np.ndarray],
                                   indicators: List[str],
                                   lv_name: str) -> Dict[str, np.ndarray]:
        """
        ë‹¨ì¼ ì ì¬ë³€ìˆ˜ì— ëŒ€í•œ ì¸¡ì •ëª¨ë¸ ê·¸ë˜ë””ì–¸íŠ¸

        Continuous Linear:
        - Y = Î¶ * LV + Îµ, Îµ ~ N(0, ÏƒÂ²)
        - âˆ‚ log L / âˆ‚Î¶_i = (y_i - Î¶_i*LV) / ÏƒÂ²_i * LV
        - âˆ‚ log L / âˆ‚ÏƒÂ²_i = -1/(2ÏƒÂ²_i) + (y_i - Î¶_i*LV)Â² / (2Ïƒâ´_i)

        Ordered Probit:
        - âˆ‚ log L / âˆ‚Î¶_i = (Ï†(Ï„_k - Î¶*LV) - Ï†(Ï„_{k-1} - Î¶*LV)) / P(Y=k) * (-LV)
        - âˆ‚ log L / âˆ‚Ï„_k = Ï†(Ï„_k - Î¶*LV) / P(Y=k)
        """
        zeta = params['zeta']

        # ì¸¡ì • ë°©ë²• í™•ì¸
        config = self.measurement_configs[lv_name]
        measurement_method = getattr(config, 'measurement_method', 'ordered_probit')

        if measurement_method == 'continuous_linear':
            # Continuous Linear ë°©ì‹
            sigma_sq = params['sigma_sq']
            return self._compute_continuous_linear_gradient(
                data, lv, zeta, sigma_sq, indicators
            )
        else:
            # Ordered Probit ë°©ì‹ (ê¸°ì¡´)
            tau = params['tau']
            return self._compute_ordered_probit_gradient(
                data, lv, zeta, tau, indicators, lv_name
            )

    def _compute_continuous_linear_gradient(self, data: pd.DataFrame, lv: float,
                                           zeta: np.ndarray, sigma_sq: np.ndarray,
                                           indicators: List[str]) -> Dict[str, np.ndarray]:
        """
        Continuous Linear ì¸¡ì •ëª¨ë¸ ê·¸ë˜ë””ì–¸íŠ¸

        Y = Î¶ * LV + Îµ, Îµ ~ N(0, ÏƒÂ²)

        âœ… gradient_core.compute_score_gradient() ì‚¬ìš©
        """
        n_ind = len(indicators)
        grad_zeta = np.zeros(n_ind)
        grad_sigma_sq = np.zeros(n_ind)

        first_row = data.iloc[0]

        for i, indicator in enumerate(indicators):
            y = first_row[indicator]
            if pd.isna(y):
                continue

            zeta_i = zeta[i]
            sigma_sq_i = sigma_sq[i]

            # ì˜ˆì¸¡ê°’
            y_pred = zeta_i * lv

            # âœ… ê³µí†µ í•¨ìˆ˜ ì‚¬ìš©: âˆ‚ log L / âˆ‚Î¶_i
            grad_zeta[i] = compute_score_gradient(
                observed=y,
                predicted=y_pred,
                variance=sigma_sq_i,
                derivative_term=lv
            )

            # âœ… ê³µí†µ í•¨ìˆ˜ ì‚¬ìš©: âˆ‚ log L / âˆ‚ÏƒÂ²_i
            grad_sigma_sq[i] = compute_variance_gradient(
                observed=y,
                predicted=y_pred,
                variance=sigma_sq_i
            )

        return {
            'grad_zeta': grad_zeta,
            'grad_sigma_sq': grad_sigma_sq
        }

    def _compute_ordered_probit_gradient(self, data: pd.DataFrame, lv: float,
                                        zeta: np.ndarray, tau: np.ndarray,
                                        indicators: List[str],
                                        lv_name: str) -> Dict[str, np.ndarray]:
        """
        Ordered Probit ì¸¡ì •ëª¨ë¸ ê·¸ë˜ë””ì–¸íŠ¸

        âœ… gradient_core.compute_ordered_probit_gradient_terms() ì‚¬ìš©
        """

        n_ind = self.n_indicators[lv_name]
        n_thresh = self.n_thresholds[lv_name]
        n_cat = self.n_categories[lv_name]

        grad_zeta = np.zeros(n_ind)
        grad_tau = np.zeros((n_ind, n_thresh))

        first_row = data.iloc[0]

        for i, indicator in enumerate(indicators):
            y = first_row[indicator]
            if pd.isna(y):
                continue

            k = int(y) - 1  # 1-5 â†’ 0-4
            zeta_i = zeta[i]
            tau_i = tau[i]

            V = zeta_i * lv

            # âœ… ê³µí†µ í•¨ìˆ˜ ì‚¬ìš©: P(Y=k), Ï†(lower), Ï†(upper) ê³„ì‚°
            prob, phi_lower, phi_upper = compute_ordered_probit_gradient_terms(
                observed_category=k,
                latent_value=V,
                thresholds=tau_i,
                n_categories=n_cat
            )

            # âˆ‚ log L / âˆ‚Î¶_i = (Ï†_lower - Ï†_upper) / P(Y=k) Ã— LV
            grad_zeta[i] = (phi_lower - phi_upper) / prob * lv

            # âˆ‚ log L / âˆ‚Ï„
            if k == 0:
                grad_tau[i, 0] = phi_upper / prob
            elif k == n_cat - 1:
                grad_tau[i, -1] = -phi_lower / prob
            else:
                grad_tau[i, k-1] = -phi_lower / prob
                grad_tau[i, k] = phi_upper / prob

        return {
            'grad_zeta': grad_zeta,
            'grad_tau': grad_tau
        }


class MultiLatentStructuralGradient:
    """
    ë‹¤ì¤‘ ì ì¬ë³€ìˆ˜ êµ¬ì¡°ëª¨ë¸ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
    
    êµ¬ì¡°ë°©ì •ì‹:
    - ì™¸ìƒ LV: LV_i ~ N(0, 1)
    - ë‚´ìƒ LV: LV_endo = Î£(Î³_lv_i * LV_i) + Î£(Î³_x_j * X_j) + Î·
    
    ê·¸ë˜ë””ì–¸íŠ¸:
    - âˆ‚ log L / âˆ‚Î³_lv_i = (LV_endo - Î¼_endo) / ÏƒÂ² * LV_i
    - âˆ‚ log L / âˆ‚Î³_x_j = (LV_endo - Î¼_endo) / ÏƒÂ² * X_j
    """
    
    def __init__(self, n_exo: int, n_cov: int, error_variance: float = 1.0):
        """
        Args:
            n_exo: ì™¸ìƒ LV ê°œìˆ˜
            n_cov: ê³µë³€ëŸ‰ ê°œìˆ˜
            error_variance: ì˜¤ì°¨ ë¶„ì‚°
        """
        self.n_exo = n_exo
        self.n_cov = n_cov
        self.error_variance = error_variance
    
    def compute_gradient(self, data: pd.DataFrame,
                        latent_vars: Dict[str, float],
                        exo_draws: np.ndarray,
                        params: Dict[str, np.ndarray],
                        covariates: List[str],
                        endogenous_lv: str,
                        exogenous_lvs: List[str],
                        hierarchical_paths: List[Dict] = None) -> Dict[str, np.ndarray]:
        """
        ë‹¤ì¤‘ ì ì¬ë³€ìˆ˜ êµ¬ì¡°ëª¨ë¸ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°

        âœ… ê³„ì¸µì  êµ¬ì¡°ì™€ ë³‘ë ¬ êµ¬ì¡° ëª¨ë‘ ì§€ì›

        Args:
            data: ê°œì¸ ë°ì´í„°
            latent_vars: ëª¨ë“  ì ì¬ë³€ìˆ˜ ê°’ {lv_name: value}
            exo_draws: ì™¸ìƒ LV draws (n_exo,)
            params:
                - ë³‘ë ¬ êµ¬ì¡°: {'gamma_lv': ..., 'gamma_x': ...}
                - ê³„ì¸µì  êµ¬ì¡°: {'gamma_{pred}_to_{target}': ...}
            covariates: ê³µë³€ëŸ‰ ë³€ìˆ˜ëª… ë¦¬ìŠ¤íŠ¸
            endogenous_lv: ë‚´ìƒ LV ì´ë¦„
            exogenous_lvs: ì™¸ìƒ LV ì´ë¦„ ë¦¬ìŠ¤íŠ¸
            hierarchical_paths: ê³„ì¸µì  ê²½ë¡œ (Noneì´ë©´ ë³‘ë ¬ êµ¬ì¡°)

        Returns:
            - ë³‘ë ¬ êµ¬ì¡°: {'grad_gamma_lv': ..., 'grad_gamma_x': ...}
            - ê³„ì¸µì  êµ¬ì¡°: {'grad_gamma_{pred}_to_{target}': ...}
        """
        # âœ… ê³„ì¸µì  êµ¬ì¡° ì§€ì›
        if hierarchical_paths is not None and len(hierarchical_paths) > 0:
            # ê³„ì¸µì  êµ¬ì¡°: ê° ê²½ë¡œë³„ë¡œ gradient ê³„ì‚°
            gradients = {}

            for path in hierarchical_paths:
                target = path['target']
                predictors = path['predictors']

                # í˜„ì¬ëŠ” ë‹¨ì¼ predictorë§Œ ì§€ì›
                if len(predictors) != 1:
                    raise ValueError(f"í˜„ì¬ ë‹¨ì¼ predictorë§Œ ì§€ì›í•©ë‹ˆë‹¤: {predictors}")

                predictor = predictors[0]
                param_key = f"gamma_{predictor}_to_{target}"

                # íŒŒë¼ë¯¸í„° ì¶”ì¶œ
                gamma = params[param_key]

                # LV ê°’ ì¶”ì¶œ
                target_value = latent_vars[target]
                pred_value = latent_vars[predictor]

                # ì˜ˆì¸¡ê°’ ê³„ì‚°: target = gamma * predictor + error
                mu = gamma * pred_value

                # âœ… ê³µí†µ í•¨ìˆ˜ ì‚¬ìš©: âˆ‚ log L / âˆ‚Î³
                grad_gamma = compute_score_gradient(
                    observed=target_value,
                    predicted=mu,
                    variance=self.error_variance,
                    derivative_term=pred_value
                )

                gradients[f'grad_{param_key}'] = grad_gamma

            return gradients

        else:
            # ë³‘ë ¬ êµ¬ì¡° (ê¸°ì¡´ ë°©ì‹)
            gamma_lv = params['gamma_lv']
            gamma_x = params['gamma_x']

            # ë‚´ìƒ LV ì‹¤ì œê°’
            lv_endo = latent_vars[endogenous_lv]

            # ì™¸ìƒ LV íš¨ê³¼
            lv_effect = np.sum(gamma_lv * exo_draws)

            # ê³µë³€ëŸ‰ íš¨ê³¼
            first_row = data.iloc[0]
            X = np.zeros(self.n_cov)
            for j, var in enumerate(covariates):
                if var in first_row.index:
                    value = first_row[var]
                    if not pd.isna(value):
                        X[j] = value

            x_effect = np.sum(gamma_x * X)

            # ì˜ˆì¸¡ í‰ê· 
            lv_endo_mean = lv_effect + x_effect

            # âœ… ê³µí†µ í•¨ìˆ˜ ì‚¬ìš©: âˆ‚ log L / âˆ‚Î³_lv_i
            grad_gamma_lv = compute_score_gradient(
                observed=lv_endo,
                predicted=lv_endo_mean,
                variance=self.error_variance,
                derivative_term=exo_draws
            )

            # âœ… ê³µí†µ í•¨ìˆ˜ ì‚¬ìš©: âˆ‚ log L / âˆ‚Î³_x_j
            grad_gamma_x = compute_score_gradient(
                observed=lv_endo,
                predicted=lv_endo_mean,
                variance=self.error_variance,
                derivative_term=X
            )

            return {
                'grad_gamma_lv': grad_gamma_lv,
                'grad_gamma_x': grad_gamma_x
            }


class MultiLatentJointGradient:
    """
    ë‹¤ì¤‘ ì ì¬ë³€ìˆ˜ ê²°í•© ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
    
    Joint LL = Î£_i log[(1/R) Î£_r P(Choice|LV_endo_r) * P(Indicators|LV_all_r) * P(LV_all_r|X)]
    
    Apollo ë°©ì‹ì˜ analytic gradient ê³„ì‚°:
    1. ê° ëª¨ë¸ì˜ gradientë¥¼ ê°œë³„ì ìœ¼ë¡œ ê³„ì‚°
    2. Chain ruleì„ ì‚¬ìš©í•˜ì—¬ ê²°í•©
    3. ì‹œë®¬ë ˆì´ì…˜ drawsì— ëŒ€í•´ ê°€ì¤‘í‰ê· 
    """
    
    def __init__(self, measurement_grad: MultiLatentMeasurementGradient,
                 structural_grad: MultiLatentStructuralGradient,
                 choice_grad,
                 use_gpu: bool = False,
                 gpu_measurement_model = None,
                 use_full_parallel: bool = True,
                 measurement_params_fixed: bool = False):
        """
        Args:
            measurement_grad: ë‹¤ì¤‘ LV ì¸¡ì •ëª¨ë¸ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°ê¸°
            structural_grad: ë‹¤ì¤‘ LV êµ¬ì¡°ëª¨ë¸ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°ê¸°
            choice_grad: ì„ íƒëª¨ë¸ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°ê¸°
            use_gpu: GPU ë°°ì¹˜ ê·¸ë˜ë””ì–¸íŠ¸ ì‚¬ìš© ì—¬ë¶€
            gpu_measurement_model: GPU ì¸¡ì •ëª¨ë¸ (use_gpu=Trueì¼ ë•Œ í•„ìš”)
            use_full_parallel: ì™„ì „ ë³‘ë ¬ ì²˜ë¦¬ ì‚¬ìš© ì—¬ë¶€ (Advanced Indexing)
            measurement_params_fixed: ì¸¡ì •ëª¨ë¸ íŒŒë¼ë¯¸í„° ê³ ì • ì—¬ë¶€ (ë™ì‹œì¶”ì •ìš©)
        """
        self.measurement_grad = measurement_grad
        self.structural_grad = structural_grad
        self.choice_grad = choice_grad
        self.use_gpu = use_gpu
        self.gpu_measurement_model = gpu_measurement_model
        self.use_full_parallel = use_full_parallel

        # âœ… ì¸¡ì •ëª¨ë¸ íŒŒë¼ë¯¸í„° ê³ ì • ì—¬ë¶€
        self.measurement_params_fixed = measurement_params_fixed

        if self.use_gpu:
            try:
                from . import gpu_gradient_batch
                self.gpu_grad = gpu_gradient_batch

                # ì™„ì „ ë³‘ë ¬ ì²˜ë¦¬ ëª¨ë“ˆ ë¡œë“œ
                if self.use_full_parallel:
                    from . import gpu_gradient_full_parallel
                    self.gpu_grad_full = gpu_gradient_full_parallel
                    logger.info("âœ¨ GPU ì™„ì „ ë³‘ë ¬ ê·¸ë˜ë””ì–¸íŠ¸ í™œì„±í™” (Advanced Indexing)")
                else:
                    logger.info("GPU ë°°ì¹˜ ê·¸ë˜ë””ì–¸íŠ¸ í™œì„±í™”")
            except ImportError as e:
                logger.warning(f"GPU ê·¸ë˜ë””ì–¸íŠ¸ ëª¨ë“ˆì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}. CPU ëª¨ë“œë¡œ ì „í™˜.")
                self.use_gpu = False
                self.use_full_parallel = False
    
    def compute_gradients(self,
                         all_ind_data: list,
                         all_ind_draws: np.ndarray,
                         params_dict: Dict,
                         measurement_model,
                         structural_model,
                         choice_model,
                         iteration_logger=None,
                         log_level: str = 'MINIMAL',
                         structural_weight: float = 1.0) -> list:
        """
        ğŸ¯ ë‹¨ì¼ ì§„ì…ì : ëª¨ë“  ê°œì¸ì˜ gradient ê³„ì‚°

        ğŸ”´ SIGN PROTOCOL (Level 2 - Pass-through):
        ==========================================
        This function is a pass-through dispatcher that routes to GPU or CPU implementations.

        CRITICAL RULES:
        1. This function receives POSITIVE gradients (âˆ‡LL) from lower levels
        2. This function MUST NOT change signs - it only routes
        3. The output is still POSITIVE gradients (âˆ‡LL)

        GPU/CPU ë¶„ê¸°ë¥¼ ë‚´ë¶€ì—ì„œ ì²˜ë¦¬í•˜ì—¬ í˜¸ì¶œìëŠ” ëª¨ë“œë¥¼ ì‹ ê²½ ì“°ì§€ ì•ŠìŒ

        Args:
            all_ind_data: ëª¨ë“  ê°œì¸ì˜ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            all_ind_draws: ëª¨ë“  ê°œì¸ì˜ draws (N, n_draws, n_dims)
            params_dict: íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬
            measurement_model: ì¸¡ì •ëª¨ë¸
            structural_model: êµ¬ì¡°ëª¨ë¸
            choice_model: ì„ íƒëª¨ë¸
            iteration_logger: ë¡œê±°
            log_level: ë¡œê¹… ë ˆë²¨
            structural_weight: êµ¬ì¡°ëª¨ë¸ ìš°ë„ ìŠ¤ì¼€ì¼ë§ ê°€ì¤‘ì¹˜ (ê¸°ë³¸ê°’: 1.0)

        Returns:
            List[Dict]: ê°œì¸ë³„ gradient ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
                        Each gradient is POSITIVE (âˆ‚LL/âˆ‚param) - Ascent direction
        """
        # GPU ìƒíƒœ í™•ì¸
        gpu_ready = self.use_gpu and self.gpu_measurement_model is not None

        if gpu_ready:
            # GPU ëª¨ë“œ: ì™„ì „ ë³‘ë ¬ ì²˜ë¦¬
            # ğŸ”´ SIGN: Returns POSITIVE gradients (âˆ‡LL)
            total_loglike_gradient_per_individual = self.compute_all_individuals_gradients_full_batch(
                all_ind_data, all_ind_draws, params_dict,
                measurement_model, structural_model, choice_model,
                iteration_logger, log_level,
                structural_weight=structural_weight  # âœ… êµ¬ì¡°ëª¨ë¸ ìŠ¤ì¼€ì¼ë§ ì „ë‹¬
            )
        else:
            # CPU ëª¨ë“œ: ìˆœì°¨ ì²˜ë¦¬
            # ğŸ”´ SIGN: Returns POSITIVE gradients (âˆ‡LL)
            total_loglike_gradient_per_individual = self.compute_all_individuals_gradients_batch(
                all_ind_data, all_ind_draws, params_dict,
                measurement_model, structural_model, choice_model,
                iteration_logger, log_level
            )

        # ğŸ”´ SIGN PROTOCOL: Return POSITIVE gradients (âˆ‡LL) - Ascent direction
        return total_loglike_gradient_per_individual

    def compute_individual_gradient(self, ind_data: pd.DataFrame,
                                   ind_draws: np.ndarray,
                                   params_dict: Dict,
                                   measurement_model,
                                   structural_model,
                                   choice_model,
                                   ind_id: int = None) -> Dict:
        """
        ê°œì¸ë³„ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° (ë‹¤ì¤‘ ì ì¬ë³€ìˆ˜)

        Args:
            ind_data: ê°œì¸ ë°ì´í„°
            ind_draws: ê°œì¸ì˜ draws (n_draws, n_dimensions)
                      [ì™¸ìƒLV1, ì™¸ìƒLV2, ..., ë‚´ìƒLVì˜¤ì°¨]
            params_dict: íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬
            measurement_model: ì¸¡ì •ëª¨ë¸ ê°ì²´
            structural_model: êµ¬ì¡°ëª¨ë¸ ê°ì²´
            choice_model: ì„ íƒëª¨ë¸ ê°ì²´
            ind_id: ê°œì¸ ID (ë””ë²„ê¹…ìš©)

        Returns:
            ê°œì¸ì˜ ê°€ì¤‘í‰ê·  ê·¸ë˜ë””ì–¸íŠ¸
        """
        if self.use_gpu and self.gpu_measurement_model is not None:
            return self._compute_individual_gradient_gpu(
                ind_data, ind_draws, params_dict,
                measurement_model, structural_model, choice_model, ind_id
            )
        else:
            return self._compute_individual_gradient_cpu(
                ind_data, ind_draws, params_dict,
                measurement_model, structural_model, choice_model
            )

    def compute_all_individuals_gradients_batch(
        self,
        all_ind_data: List[pd.DataFrame],
        all_ind_draws: np.ndarray,
        params_dict: Dict,
        measurement_model,
        structural_model,
        choice_model,
        iteration_logger=None,
        log_level: str = 'MINIMAL'
    ) -> List[Dict]:
        """
        ëª¨ë“  ê°œì¸ì˜ gradientë¥¼ GPU batchë¡œ ë™ì‹œ ê³„ì‚°

        âœ… ì™„ì „ GPU Batch: Nëª…ì˜ ê°œì¸ì„ ë™ì‹œì— ì²˜ë¦¬

        Args:
            all_ind_data: ëª¨ë“  ê°œì¸ì˜ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ [DataFrame_1, ..., DataFrame_N]
            all_ind_draws: ëª¨ë“  ê°œì¸ì˜ draws (N, n_draws, n_dims)
            params_dict: íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬
            measurement_model: ì¸¡ì •ëª¨ë¸
            structural_model: êµ¬ì¡°ëª¨ë¸
            choice_model: ì„ íƒëª¨ë¸
            iteration_logger: ë¡œê±°
            log_level: ë¡œê¹… ë ˆë²¨

        Returns:
            ê°œì¸ë³„ gradient ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ [grad_dict_1, ..., grad_dict_N]
        """
        if self.use_gpu and self.gpu_measurement_model is not None:
            # GPU batch ëª¨ë“œ
            return self.gpu_grad.compute_all_individuals_gradients_batch_gpu(
                self.gpu_measurement_model,
                all_ind_data,
                all_ind_draws,
                params_dict,
                measurement_model,
                structural_model,
                choice_model,
                iteration_logger=iteration_logger,
                log_level=log_level
            )
        else:
            # CPU ëª¨ë“œ (ìˆœì°¨ ì²˜ë¦¬)
            if iteration_logger:
                iteration_logger.info("CPU ëª¨ë“œë¡œ ê°œì¸ë³„ gradient ìˆœì°¨ ê³„ì‚°")

            all_gradients = []
            for ind_idx, (ind_data, ind_draws) in enumerate(zip(all_ind_data, all_ind_draws)):
                ind_grad = self._compute_individual_gradient_cpu(
                    ind_data, ind_draws, params_dict,
                    measurement_model, structural_model, choice_model
                )
                all_gradients.append(ind_grad)

                # ì§„í–‰ ìƒí™© ë¡œê¹…
                if iteration_logger and log_level in ['MODERATE', 'DETAILED']:
                    if (ind_idx + 1) % max(1, len(all_ind_data) // 10) == 0:
                        progress = (ind_idx + 1) / len(all_ind_data) * 100
                        iteration_logger.info(f"  ì§„í–‰: {ind_idx + 1}/{len(all_ind_data)} ({progress:.0f}%)")

            return all_gradients

    def compute_all_individuals_gradients_full_batch(
        self,
        all_ind_data: List[pd.DataFrame],
        all_ind_draws: np.ndarray,
        params_dict: Dict,
        measurement_model,
        structural_model,
        choice_model,
        iteration_logger=None,
        log_level: str = 'MINIMAL',
        use_scaling: bool = False,  # âœ… ì¸¡ì •ëª¨ë¸ ìš°ë„ ìŠ¤ì¼€ì¼ë§ ì‚¬ìš© ì—¬ë¶€
        structural_weight: float = 1.0  # âœ… êµ¬ì¡°ëª¨ë¸ ìš°ë„ ìŠ¤ì¼€ì¼ë§ ê°€ì¤‘ì¹˜
    ) -> List[Dict]:
        """
        ëª¨ë“  ê°œì¸ì˜ gradientë¥¼ ì™„ì „ GPU batchë¡œ ë™ì‹œ ê³„ì‚°

        ğŸš€ ì™„ì „ ë³‘ë ¬ ì²˜ë¦¬ (Advanced Indexing):
        - use_full_parallel=True: ì¸¡ì •ëª¨ë¸ 38ê°œ ì§€í‘œë¥¼ 1ë²ˆ GPU í˜¸ì¶œë¡œ ê³„ì‚° (38ë°° ë¹ ë¦„)
        - use_full_parallel=False: LVë³„ ìˆœì°¨, ì§€í‘œë³„ ë³‘ë ¬ (5ë²ˆ GPU í˜¸ì¶œ)

        ì„±ëŠ¥:
        - ì¸¡ì •ëª¨ë¸: 1ë²ˆ GPU ì»¤ë„ í˜¸ì¶œ (ê¸°ì¡´ 38ë²ˆ â†’ 38ë°° ê°œì„ )
        - ë©”ëª¨ë¦¬: 9.45 MB (Zero-padding 24.87 MB ëŒ€ë¹„ 62% ì ˆì•½)

        Args:
            all_ind_data: ëª¨ë“  ê°œì¸ì˜ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ [DataFrame_1, ..., DataFrame_N]
            all_ind_draws: ëª¨ë“  ê°œì¸ì˜ draws (N, n_draws, n_dims)
            params_dict: íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬
            measurement_model: ì¸¡ì •ëª¨ë¸
            structural_model: êµ¬ì¡°ëª¨ë¸
            choice_model: ì„ íƒëª¨ë¸
            iteration_logger: ë¡œê±°
            log_level: ë¡œê¹… ë ˆë²¨
            use_scaling: ì¸¡ì •ëª¨ë¸ ìš°ë„ ìŠ¤ì¼€ì¼ë§ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
            structural_weight: êµ¬ì¡°ëª¨ë¸ ìš°ë„ ìŠ¤ì¼€ì¼ë§ ê°€ì¤‘ì¹˜ (ê¸°ë³¸ê°’: 1.0)

        Returns:
            ê°œì¸ë³„ gradient ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ [grad_dict_1, ..., grad_dict_N]
        """
        if self.use_gpu and self.gpu_measurement_model is not None:
            # âœ¨ ì™„ì „ ë³‘ë ¬ ì²˜ë¦¬ (Advanced Indexing)
            if self.use_full_parallel and hasattr(self, 'gpu_grad_full'):
                return self.gpu_grad_full.compute_all_individuals_gradients_full_parallel_gpu(
                    self.gpu_measurement_model,
                    all_ind_data,
                    all_ind_draws,
                    params_dict,
                    measurement_model,
                    structural_model,
                    choice_model,
                    iteration_logger=iteration_logger,
                    log_level=log_level,
                    use_scaling=use_scaling,  # âœ… ì¸¡ì •ëª¨ë¸ ìŠ¤ì¼€ì¼ë§ ì „ë‹¬
                    structural_weight=structural_weight  # âœ… êµ¬ì¡°ëª¨ë¸ ìŠ¤ì¼€ì¼ë§ ì „ë‹¬
                )
            else:
                # ê¸°ì¡´ ì™„ì „ GPU batch ëª¨ë“œ (LVë³„ ìˆœì°¨)
                return self.gpu_grad.compute_all_individuals_gradients_full_batch_gpu(
                    self.gpu_measurement_model,
                    all_ind_data,
                    all_ind_draws,
                    params_dict,
                    measurement_model,
                    structural_model,
                    choice_model,
                    iteration_logger=iteration_logger,
                    log_level=log_level,
                    use_scaling=use_scaling,  # âœ… ì¸¡ì •ëª¨ë¸ ìŠ¤ì¼€ì¼ë§ ì „ë‹¬
                    structural_weight=structural_weight  # âœ… êµ¬ì¡°ëª¨ë¸ ìŠ¤ì¼€ì¼ë§ ì „ë‹¬
                )
        else:
            # CPU ëª¨ë“œëŠ” ì¼ë°˜ batchë¡œ í´ë°±
            return self.compute_all_individuals_gradients_batch(
                all_ind_data,
                all_ind_draws,
                params_dict,
                measurement_model,
                structural_model,
                choice_model,
                iteration_logger,
                log_level
            )

    def _compute_individual_gradient_cpu(self, ind_data: pd.DataFrame,
                                        ind_draws: np.ndarray,
                                        params_dict: Dict,
                                        measurement_model,
                                        structural_model,
                                        choice_model) -> Dict:
        """
        ê°œì¸ë³„ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° - CPU ë²„ì „
        """
        n_draws = len(ind_draws)
        # ì™¸ìƒ LV ê°œìˆ˜ ê³„ì‚° (ê³„ì¸µì  êµ¬ì¡°ì™€ ë³‘ë ¬ êµ¬ì¡° ëª¨ë‘ ì§€ì›)
        if hasattr(structural_model, 'n_exo'):
            n_exo = structural_model.n_exo
        else:
            n_exo = len(structural_model.exogenous_lvs)

        # ê° drawì˜ likelihoodì™€ gradient ì €ì¥
        draw_likelihoods = []
        draw_gradients = []

        for draw_idx in range(n_draws):
            # Draws ë¶„ë¦¬
            exo_draws = ind_draws[draw_idx, :n_exo]
            endo_draw = ind_draws[draw_idx, n_exo]
            
            # ëª¨ë“  LV ì˜ˆì¸¡
            latent_vars = structural_model.predict(
                ind_data, exo_draws, params_dict['structural'], endo_draw
            )
            
            # ê° ëª¨ë¸ì˜ log-likelihood ê³„ì‚°
            ll_measurement = measurement_model.log_likelihood(
                ind_data, latent_vars, params_dict['measurement']
            )
            
            # ì„ íƒëª¨ë¸ (ë‚´ìƒ LVë§Œ ì‚¬ìš©)
            lv_endo = latent_vars[structural_model.endogenous_lv]
            ll_choice = 0.0
            for idx in range(len(ind_data)):
                ll_choice += choice_model.log_likelihood(
                    ind_data.iloc[idx:idx+1], lv_endo, params_dict['choice']
                )
            
            ll_structural = structural_model.log_likelihood(
                ind_data, latent_vars, exo_draws, params_dict['structural'], endo_draw
            )
            
            # ê²°í•© log-likelihood
            joint_ll = ll_measurement + ll_choice + ll_structural
            
            # Likelihood (not log)
            likelihood = np.exp(joint_ll) if np.isfinite(joint_ll) else 1e-100
            draw_likelihoods.append(likelihood)
            
            # ê° ëª¨ë¸ì˜ gradient ê³„ì‚°
            # âœ… ì¸¡ì •ëª¨ë¸ íŒŒë¼ë¯¸í„° ê³ ì • ì‹œ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ìŠ¤í‚µ
            if self.measurement_params_fixed:
                # ì¸¡ì •ëª¨ë¸ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ 0ìœ¼ë¡œ ì„¤ì • (íŒŒë¼ë¯¸í„° ê³ ì •)
                grad_meas = {}
                for lv_name in self.measurement_grad.lv_names:
                    config = self.measurement_grad.measurement_configs[lv_name]
                    measurement_method = getattr(config, 'measurement_method', 'ordered_probit')

                    n_ind = len(config.indicators)
                    grad_meas[lv_name] = {'grad_zeta': np.zeros(n_ind)}

                    if measurement_method == 'continuous_linear':
                        grad_meas[lv_name]['grad_sigma_sq'] = np.zeros(n_ind)
                    else:
                        n_thresh = config.n_categories - 1
                        grad_meas[lv_name]['grad_tau'] = np.zeros((n_ind, n_thresh))
            else:
                # íŒŒë¼ë¯¸í„°ê°€ ë³€í•˜ë¯€ë¡œ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
                grad_meas = self.measurement_grad.compute_gradient(
                    ind_data, latent_vars, params_dict['measurement']
                )
            
            # âœ… ê³„ì¸µì  ê²½ë¡œ ì „ë‹¬
            hierarchical_paths = getattr(structural_model, 'hierarchical_paths', None)

            grad_struct = self.structural_grad.compute_gradient(
                ind_data, latent_vars, exo_draws, params_dict['structural'],
                structural_model.covariates, structural_model.endogenous_lv,
                structural_model.exogenous_lvs,
                hierarchical_paths=hierarchical_paths
            )
            
            # âœ… ëª¨ë“  LV ì£¼íš¨ê³¼ ë˜ëŠ” ì¡°ì ˆíš¨ê³¼ ëª¨ë¸ì€ latent_vars ì „ì²´ë¥¼ ì „ë‹¬
            lambda_lv_keys = [key for key in params_dict['choice'].keys() if key.startswith('lambda_') and key not in ['lambda_main']]

            if len(lambda_lv_keys) > 1 or 'lambda_main' in params_dict['choice']:
                # ëª¨ë“  LV ì£¼íš¨ê³¼ ë˜ëŠ” ì¡°ì ˆíš¨ê³¼ ëª¨ë¸: ëª¨ë“  LV ì „ë‹¬
                grad_choice = self.choice_grad.compute_gradient(
                    ind_data, latent_vars, params_dict['choice'],
                    choice_model.config.choice_attributes
                )
            else:
                # ê¸°ë³¸ ëª¨ë¸: ë‚´ìƒ LVë§Œ ì „ë‹¬
                grad_choice = self.choice_grad.compute_gradient(
                    ind_data, lv_endo, params_dict['choice'],
                    choice_model.config.choice_attributes
                )
            
            # ê·¸ë˜ë””ì–¸íŠ¸ ì €ì¥
            draw_gradients.append({
                'measurement': grad_meas,
                'structural': grad_struct,
                'choice': grad_choice
            })
        
        # Importance weights ê³„ì‚°
        total_likelihood = sum(draw_likelihoods)
        if total_likelihood == 0:
            weights = np.ones(n_draws) / n_draws
        else:
            weights = np.array(draw_likelihoods) / total_likelihood
        
        # ê°€ì¤‘í‰ê·  ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
        weighted_grad = self._compute_weighted_gradient(weights, draw_gradients)
        
        return weighted_grad
    
    def _compute_weighted_gradient(self, weights: np.ndarray,
                                   draw_gradients: List[Dict]) -> Dict:
        """
        ê°€ì¤‘í‰ê·  ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°

        âœ… continuous_linearê³¼ ordered_probit ë‘˜ ë‹¤ ì§€ì›
        """
        # ì´ˆê¸°í™” (ì²« ë²ˆì§¸ drawì˜ êµ¬ì¡°ë¥¼ ì‚¬ìš©)
        first_grad = draw_gradients[0]

        # ì¸¡ì •ëª¨ë¸ ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”
        weighted_meas = {}
        for lv_name in first_grad['measurement'].keys():
            lv_grad = first_grad['measurement'][lv_name]
            weighted_meas[lv_name] = {
                'grad_zeta': np.zeros_like(lv_grad['grad_zeta'])
            }

            # âœ… continuous_linear: grad_sigma_sq, ordered_probit: grad_tau
            if 'grad_sigma_sq' in lv_grad:
                weighted_meas[lv_name]['grad_sigma_sq'] = np.zeros_like(lv_grad['grad_sigma_sq'])
            elif 'grad_tau' in lv_grad:
                weighted_meas[lv_name]['grad_tau'] = np.zeros_like(lv_grad['grad_tau'])

        # êµ¬ì¡°ëª¨ë¸ ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™” (âœ… ê³„ì¸µì  vs ë³‘ë ¬ êµ¬ì¡°)
        if 'grad_gamma_lv' in first_grad['structural']:
            # ë³‘ë ¬ êµ¬ì¡°
            weighted_struct = {
                'grad_gamma_lv': np.zeros_like(first_grad['structural']['grad_gamma_lv']),
                'grad_gamma_x': np.zeros_like(first_grad['structural']['grad_gamma_x'])
            }
        else:
            # ê³„ì¸µì  êµ¬ì¡°
            weighted_struct = {}
            for key in first_grad['structural'].keys():
                weighted_struct[key] = 0.0

        # ì„ íƒëª¨ë¸ ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™” (âœ… ì¡°ì ˆíš¨ê³¼ vs ê¸°ë³¸ ëª¨ë¸)
        weighted_choice = {
            'grad_intercept': 0.0,
            'grad_beta': np.zeros_like(first_grad['choice']['grad_beta'])
        }

        # âœ… ëª¨ë“  LV ì£¼íš¨ê³¼ vs ì¡°ì ˆíš¨ê³¼ vs ê¸°ë³¸ ëª¨ë¸
        lambda_grad_keys = [key for key in first_grad['choice'].keys() if key.startswith('grad_lambda_')]

        if len(lambda_grad_keys) > 1 and 'grad_lambda_main' not in first_grad['choice']:
            # ëª¨ë“  LV ì£¼íš¨ê³¼ ëª¨ë¸: grad_lambda_{lv_name}
            for key in lambda_grad_keys:
                weighted_choice[key] = 0.0
        elif 'grad_lambda_main' in first_grad['choice']:
            # ì¡°ì ˆíš¨ê³¼ ëª¨ë¸
            weighted_choice['grad_lambda_main'] = 0.0
            for key in first_grad['choice'].keys():
                if key.startswith('grad_lambda_mod_'):
                    weighted_choice[key] = 0.0
        else:
            # ê¸°ë³¸ ëª¨ë¸
            weighted_choice['grad_lambda'] = 0.0

        # ê°€ì¤‘í•© ê³„ì‚°
        for w, grad in zip(weights, draw_gradients):
            # ì¸¡ì •ëª¨ë¸
            for lv_name in grad['measurement'].keys():
                weighted_meas[lv_name]['grad_zeta'] += w * grad['measurement'][lv_name]['grad_zeta']

                # âœ… continuous_linear vs ordered_probit
                if 'grad_sigma_sq' in grad['measurement'][lv_name]:
                    weighted_meas[lv_name]['grad_sigma_sq'] += w * grad['measurement'][lv_name]['grad_sigma_sq']
                elif 'grad_tau' in grad['measurement'][lv_name]:
                    weighted_meas[lv_name]['grad_tau'] += w * grad['measurement'][lv_name]['grad_tau']

            # êµ¬ì¡°ëª¨ë¸ (âœ… ê³„ì¸µì  vs ë³‘ë ¬)
            for key in grad['structural'].keys():
                weighted_struct[key] += w * grad['structural'][key]

            # ì„ íƒëª¨ë¸ (âœ… ì¡°ì ˆíš¨ê³¼ vs ê¸°ë³¸)
            weighted_choice['grad_intercept'] += w * grad['choice']['grad_intercept']
            weighted_choice['grad_beta'] += w * grad['choice']['grad_beta']

            for key in grad['choice'].keys():
                if key.startswith('grad_lambda'):
                    weighted_choice[key] += w * grad['choice'][key]

        return {
            'measurement': weighted_meas,
            'structural': weighted_struct,
            'choice': weighted_choice
        }

    def _compute_individual_gradient_gpu(self, ind_data: pd.DataFrame,
                                        ind_draws: np.ndarray,
                                        params_dict: Dict,
                                        measurement_model,
                                        structural_model,
                                        choice_model,
                                        ind_id: int = None) -> Dict:
        """
        ê°œì¸ë³„ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° - GPU ë°°ì¹˜ ë²„ì „ (Importance Weighting ì ìš©)

        CPU êµ¬í˜„ê³¼ ë™ì¼í•œ ë¡œì§:
        1. ê° drawì˜ likelihood ê³„ì‚°
        2. Importance weights ê³„ì‚°
        3. ê°€ì¤‘í‰ê·  ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
        4. GPU ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì„±ëŠ¥ í–¥ìƒ
        """
        n_draws = len(ind_draws)

        # ë¡œê¹… ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        iteration_logger = getattr(self, 'iteration_logger', None)
        log_level = 'MINIMAL'  # ê¸°ë³¸ê°’
        if hasattr(self, 'config') and hasattr(self.config, 'estimation'):
            log_level = getattr(self.config.estimation, 'gradient_log_level', 'MINIMAL')

        # ì²« ë²ˆì§¸ ê°œì¸ì— ëŒ€í•´ì„œë§Œ ìƒì„¸ ë¡œê¹…
        should_log = (ind_id is not None and not hasattr(self, '_first_gradient_logged'))

        # âœ… ê³„ì¸µì  êµ¬ì¡° ì§€ì›
        is_hierarchical = hasattr(structural_model, 'is_hierarchical') and structural_model.is_hierarchical

        if is_hierarchical:
            # ê³„ì¸µì  êµ¬ì¡°: 1ì°¨ LV ê°œìˆ˜
            n_first_order = len(structural_model.exogenous_lvs)
            n_higher_order = len(structural_model.get_higher_order_lvs())
        else:
            # ë³‘ë ¬ êµ¬ì¡° (í•˜ìœ„ í˜¸í™˜)
            n_exo = structural_model.n_exo

        # ëª¨ë“  drawsì˜ LV ê°’ ë¯¸ë¦¬ ê³„ì‚°
        lvs_list = []
        exo_draws_list = []

        for draw_idx in range(n_draws):
            if is_hierarchical:
                # ê³„ì¸µì  êµ¬ì¡°: 1ì°¨ LV draws + ê³ ì°¨ LV ì˜¤ì°¨í•­
                first_order_draws = ind_draws[draw_idx, :n_first_order]
                higher_order_errors = ind_draws[draw_idx, n_first_order:]

                # ê³ ì°¨ LV ì˜¤ì°¨í•­ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
                higher_order_lvs = structural_model.get_higher_order_lvs()
                error_dict = {lv_name: higher_order_errors[i] for i, lv_name in enumerate(higher_order_lvs)}

                # âœ… ë””ë²„ê¹…: error_dict í™•ì¸ (ì²« ë²ˆì§¸ drawë§Œ)
                if should_log and draw_idx == 0 and iteration_logger:
                    iteration_logger.info(f"[ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°] Draw {draw_idx}:")
                    iteration_logger.info(f"  higher_order_lvs: {higher_order_lvs}")
                    iteration_logger.info(f"  higher_order_errors: {higher_order_errors}")
                    iteration_logger.info(f"  error_dict: {error_dict}")
                    # predict() í•¨ìˆ˜ ë‚´ë¶€ ë””ë²„ê¹… í™œì„±í™”
                    structural_model._debug_predict = True

                # âœ… ìˆ˜ì •: higher_order_drawsë¥¼ í‚¤ì›Œë“œ ì¸ìë¡œ ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬
                latent_vars = structural_model.predict(
                    ind_data, first_order_draws, params_dict['structural'],
                    endo_draw=None, higher_order_draws=error_dict
                )

                # âœ… ë””ë²„ê¹…: ì˜ˆì¸¡ëœ LV ê°’ í™•ì¸ (ì²« ë²ˆì§¸ drawë§Œ)
                if should_log and draw_idx == 0 and iteration_logger:
                    iteration_logger.info(f"  ì˜ˆì¸¡ëœ LV: {latent_vars}")
                    # predict() í•¨ìˆ˜ ë‚´ë¶€ ë””ë²„ê¹… ë¹„í™œì„±í™”
                    structural_model._debug_predict = False
                exo_draws_list.append(first_order_draws)
            else:
                # ë³‘ë ¬ êµ¬ì¡° (í•˜ìœ„ í˜¸í™˜)
                exo_draws = ind_draws[draw_idx, :n_exo]
                endo_draw = ind_draws[draw_idx, n_exo]

                latent_vars = structural_model.predict(
                    ind_data, exo_draws, params_dict['structural'], endo_draw
                )
                exo_draws_list.append(exo_draws)

            lvs_list.append(latent_vars)

        # âœ… 1. ê° drawì˜ ê²°í•© likelihood ê³„ì‚° (importance weightingìš©)
        ll_batch = self.gpu_grad.compute_joint_likelihood_batch_gpu(
            self.gpu_measurement_model,
            ind_data,
            lvs_list,
            ind_draws,
            params_dict,
            structural_model,
            choice_model
        )

        # âœ… 2. Importance weights ê³„ì‚° (Apollo ë°©ì‹)
        weights = self.gpu_grad.compute_importance_weights_gpu(ll_batch, ind_id)

        # âœ… 3. ê°€ì¤‘í‰ê·  ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
        grad_meas = self.gpu_grad.compute_measurement_gradient_batch_gpu(
            self.gpu_measurement_model,
            ind_data,
            lvs_list,
            params_dict['measurement'],
            weights,  # âœ… weights ì „ë‹¬
            iteration_logger=iteration_logger if should_log else None,
            log_level=log_level if should_log else 'MINIMAL'
        )

        # âœ… êµ¬ì¡°ëª¨ë¸ gradient: ê³„ì¸µì  êµ¬ì¡° ì§€ì›
        if is_hierarchical:
            grad_struct = self.gpu_grad.compute_structural_gradient_batch_gpu(
                ind_data,
                lvs_list,
                exo_draws_list,
                params_dict,  # âœ… ì „ì²´ íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬ ì „ë‹¬
                structural_model.covariates,
                structural_model.endogenous_lv,
                structural_model.exogenous_lvs,
                weights,
                is_hierarchical=True,
                hierarchical_paths=structural_model.hierarchical_paths,
                gpu_measurement_model=self.gpu_measurement_model,  # âœ… GPU ì¸¡ì •ëª¨ë¸ ì „ë‹¬
                choice_model=choice_model,  # âœ… ì„ íƒëª¨ë¸ ì „ë‹¬
                iteration_logger=iteration_logger if should_log else None,
                log_level=log_level if should_log else 'MINIMAL'
            )
        else:
            grad_struct = self.gpu_grad.compute_structural_gradient_batch_gpu(
                ind_data,
                lvs_list,
                exo_draws_list,
                params_dict,  # âœ… ì „ì²´ íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬ ì „ë‹¬
                structural_model.covariates,
                structural_model.endogenous_lv,
                structural_model.exogenous_lvs,
                weights,
                gpu_measurement_model=self.gpu_measurement_model,  # âœ… GPU ì¸¡ì •ëª¨ë¸ ì „ë‹¬
                choice_model=choice_model,  # âœ… ì„ íƒëª¨ë¸ ì „ë‹¬
                iteration_logger=iteration_logger if should_log else None,
                log_level=log_level if should_log else 'MINIMAL'
            )

        # âœ… ì„ íƒëª¨ë¸ gradient: ì¡°ì ˆíš¨ê³¼ ì§€ì›
        moderation_enabled = hasattr(choice_model.config, 'moderators') and choice_model.config.moderators
        if moderation_enabled:
            grad_choice = self.gpu_grad.compute_choice_gradient_batch_gpu(
                ind_data,
                lvs_list,
                params_dict['choice'],
                structural_model.endogenous_lv,
                choice_model.config.choice_attributes,
                weights,
                moderators=choice_model.config.moderators,
                iteration_logger=iteration_logger if should_log else None,
                log_level=log_level if should_log else 'MINIMAL'
            )
        else:
            grad_choice = self.gpu_grad.compute_choice_gradient_batch_gpu(
                ind_data,
                lvs_list,
                params_dict['choice'],
                structural_model.endogenous_lv,
                choice_model.config.choice_attributes,
                weights,
                iteration_logger=iteration_logger if should_log else None,
                log_level=log_level if should_log else 'MINIMAL'
            )

        # ì²« ë²ˆì§¸ ê·¸ë˜ë””ì–¸íŠ¸ ë¡œê¹… ì™„ë£Œ í‘œì‹œ
        if should_log:
            self._first_gradient_logged = True

        # ê²°í•© ê·¸ë˜ë””ì–¸íŠ¸
        return {
            'measurement': grad_meas,
            'structural': grad_struct,
            'choice': grad_choice
        }

