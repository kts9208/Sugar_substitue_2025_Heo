# ì¡°ì ˆíš¨ê³¼ êµ¬í˜„ ìƒì„¸ ê°€ì´ë“œ

**ì‘ì„±ì¼**: 2025-11-11  
**ëª©ì **: ICLV ì„ íƒëª¨ë¸ì—ì„œ ì¡°ì ˆíš¨ê³¼ë¥¼ êµ¬í˜„í•˜ëŠ” ë°©ë²• ìƒì„¸ ì„¤ëª…

---

## ğŸ“Š ì¡°ì ˆíš¨ê³¼ë€?

### ì •ì˜
**ì¡°ì ˆíš¨ê³¼ (Moderation Effect)**: ë…ë¦½ë³€ìˆ˜(X)ì™€ ì¢…ì†ë³€ìˆ˜(Y) ê°„ ê´€ê³„ì˜ ê°•ë„ë‚˜ ë°©í–¥ì´ ì œ3ì˜ ë³€ìˆ˜(M, ì¡°ì ˆë³€ìˆ˜)ì— ë”°ë¼ ë‹¬ë¼ì§€ëŠ” í˜„ìƒ

### ìˆ˜ì‹
```
Y = Î²â‚€ + Î²â‚Â·X + Î²â‚‚Â·M + Î²â‚ƒÂ·(X Ã— M) + Îµ

ì—¬ê¸°ì„œ:
- Î²â‚: Xì˜ ì£¼íš¨ê³¼ (Main Effect)
- Î²â‚‚: Mì˜ ì£¼íš¨ê³¼
- Î²â‚ƒ: ì¡°ì ˆíš¨ê³¼ (Moderation Effect) â† í•µì‹¬!
```

**í•´ì„**:
- Î²â‚ƒ > 0: Mì´ í´ìˆ˜ë¡ Xâ†’Y ê´€ê³„ê°€ ê°•í•´ì§ (ì •ì  ì¡°ì ˆ)
- Î²â‚ƒ < 0: Mì´ í´ìˆ˜ë¡ Xâ†’Y ê´€ê³„ê°€ ì•½í•´ì§ (ë¶€ì  ì¡°ì ˆ)
- Î²â‚ƒ = 0: ì¡°ì ˆíš¨ê³¼ ì—†ìŒ

---

## ğŸ¯ ICLVì—ì„œì˜ ì¡°ì ˆíš¨ê³¼

### í˜„ì¬ ëª¨ë¸ (ì¡°ì ˆíš¨ê³¼ ì—†ìŒ)
```
V = intercept + Î²Â·X + Î»Â·LV_main
P(choice=1) = Î¦(V)

ì—¬ê¸°ì„œ:
- X: ì„ íƒ ì†ì„± (sugar_free, health_label, price)
- LV_main: êµ¬ë§¤ì˜ë„ (purchase_intention)
- Î»: êµ¬ë§¤ì˜ë„ì˜ ì„ íƒì— ëŒ€í•œ íš¨ê³¼ (ê³ ì •)
```

**ë¬¸ì œì **: Î»ê°€ ëª¨ë“  ì‚¬ëŒì—ê²Œ ë™ì¼ â†’ í˜„ì‹¤ì ì´ì§€ ì•ŠìŒ

---

### ì œì•ˆ ëª¨ë¸ (ì¡°ì ˆíš¨ê³¼ í¬í•¨)
```
V = intercept + Î²Â·X + Î»â‚Â·PI + Î»â‚‚Â·(PI Ã— PP) + Î»â‚ƒÂ·(PI Ã— NK)
P(choice=1) = Î¦(V)

ì—¬ê¸°ì„œ:
- PI: êµ¬ë§¤ì˜ë„ (Purchase Intention)
- PP: ê°€ê²©ìˆ˜ì¤€ (Perceived Price)
- NK: ì˜ì–‘ì§€ì‹ (Nutrition Knowledge)
- Î»â‚: êµ¬ë§¤ì˜ë„ ì£¼íš¨ê³¼
- Î»â‚‚: ê°€ê²©ìˆ˜ì¤€ ì¡°ì ˆíš¨ê³¼ (ì˜ˆìƒ: Î»â‚‚ < 0)
- Î»â‚ƒ: ì˜ì–‘ì§€ì‹ ì¡°ì ˆíš¨ê³¼ (ì˜ˆìƒ: Î»â‚ƒ > 0)
```

**ì˜ë¯¸**:
- êµ¬ë§¤ì˜ë„ê°€ ì„ íƒì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì´ ê°€ê²©ìˆ˜ì¤€ê³¼ ì˜ì–‘ì§€ì‹ì— ë”°ë¼ ë‹¬ë¼ì§
- ê°€ê²©ìˆ˜ì¤€ì´ ë†’ìœ¼ë©´ êµ¬ë§¤ì˜ë„ê°€ ìˆì–´ë„ ì„ íƒ í™•ë¥  ê°ì†Œ
- ì˜ì–‘ì§€ì‹ì´ ë†’ìœ¼ë©´ êµ¬ë§¤ì˜ë„ê°€ ì„ íƒìœ¼ë¡œ ì „í™˜ë  í™•ë¥  ì¦ê°€

---

## ğŸ’» êµ¬í˜„ ë°©ë²•

### Step 1: í˜„ì¬ ì½”ë“œ êµ¬ì¡° ì´í•´

**í˜„ì¬ `BinaryProbitChoice.log_likelihood()` í•µì‹¬ ë¶€ë¶„**:

```python
# íš¨ìš© ê³„ì‚° (í˜„ì¬)
V = intercept + X @ beta + lambda_lv * lv_array

# í™•ë¥  ê³„ì‚°
prob_yes = norm.cdf(V)

# ë¡œê·¸ìš°ë„
ll = np.sum(choice * np.log(prob_yes) + (1 - choice) * np.log(1 - prob_yes))
```

**ì…ë ¥**:
- `lv`: ë‹¨ì¼ ì ì¬ë³€ìˆ˜ (êµ¬ë§¤ì˜ë„ë§Œ)
- `params['lambda']`: ë‹¨ì¼ ê³„ìˆ˜

---

### Step 2: ì¡°ì ˆíš¨ê³¼ í¬í•¨ ìˆ˜ì •

#### **2.1 ì…ë ¥ ë³€ê²½**

**Before**:
```python
def log_likelihood(self, data: pd.DataFrame, lv: np.ndarray, params: Dict) -> float:
    # lv: êµ¬ë§¤ì˜ë„ë§Œ (ìŠ¤ì¹¼ë¼ ë˜ëŠ” 1D ë°°ì—´)
```

**After**:
```python
def log_likelihood(self, data: pd.DataFrame, 
                  latent_vars: Dict[str, float],  # â† ë³€ê²½!
                  params: Dict) -> float:
    """
    Args:
        latent_vars: ëª¨ë“  ì ì¬ë³€ìˆ˜ ê°’
            {
                'purchase_intention': 0.5,
                'perceived_price': -0.3,
                'nutrition_knowledge': 0.8
            }
    """
```

#### **2.2 íŒŒë¼ë¯¸í„° ë³€ê²½**

**Before**:
```python
params = {
    'intercept': 0.0,
    'beta': np.array([-2.0, 0.3, -1.5]),  # [sugar_free, health_label, price]
    'lambda': 1.0  # êµ¬ë§¤ì˜ë„ ê³„ìˆ˜
}
```

**After**:
```python
params = {
    'intercept': 0.0,
    'beta': np.array([-2.0, 0.3, -1.5]),
    'lambda_main': 1.0,        # êµ¬ë§¤ì˜ë„ ì£¼íš¨ê³¼
    'lambda_mod_price': -0.3,  # ê°€ê²©ìˆ˜ì¤€ ì¡°ì ˆíš¨ê³¼ (ë¶€ì )
    'lambda_mod_knowledge': 0.2  # ì˜ì–‘ì§€ì‹ ì¡°ì ˆíš¨ê³¼ (ì •ì )
}
```

#### **2.3 íš¨ìš© í•¨ìˆ˜ ìˆ˜ì •**

**Before**:
```python
# íš¨ìš© ê³„ì‚° (ì¡°ì ˆíš¨ê³¼ ì—†ìŒ)
V = intercept + X @ beta + lambda_lv * lv_array
```

**After**:
```python
# ì ì¬ë³€ìˆ˜ ì¶”ì¶œ
lv_main = latent_vars['purchase_intention']
lv_mod_price = latent_vars['perceived_price']
lv_mod_knowledge = latent_vars['nutrition_knowledge']

# íš¨ìš© ê³„ì‚° (ì¡°ì ˆíš¨ê³¼ í¬í•¨)
V = (intercept + 
     X @ beta + 
     lambda_main * lv_main +                              # ì£¼íš¨ê³¼
     lambda_mod_price * (lv_main * lv_mod_price) +        # ì¡°ì ˆíš¨ê³¼ 1
     lambda_mod_knowledge * (lv_main * lv_mod_knowledge)) # ì¡°ì ˆíš¨ê³¼ 2
```

---

### Step 3: ì™„ì „í•œ êµ¬í˜„ ì½”ë“œ

```python
class BinaryProbitChoiceWithModeration:
    """ì¡°ì ˆíš¨ê³¼ê°€ í¬í•¨ëœ Binary Probit ì„ íƒëª¨ë¸"""
    
    def __init__(self, config: ChoiceConfig,
                 main_lv: str = 'purchase_intention',
                 moderator_lvs: List[str] = None):
        """
        Args:
            config: ì„ íƒëª¨ë¸ ì„¤ì •
            main_lv: ì£¼ ì ì¬ë³€ìˆ˜ (êµ¬ë§¤ì˜ë„)
            moderator_lvs: ì¡°ì ˆë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸ ['perceived_price', 'nutrition_knowledge']
        """
        self.config = config
        self.choice_attributes = config.choice_attributes
        self.main_lv = main_lv
        self.moderator_lvs = moderator_lvs or []
        
        self.n_attributes = len(self.choice_attributes)
        self.n_moderators = len(self.moderator_lvs)
    
    def log_likelihood(self, data: pd.DataFrame,
                      latent_vars: Dict[str, float],
                      params: Dict) -> float:
        """
        ì¡°ì ˆíš¨ê³¼ í¬í•¨ ë¡œê·¸ìš°ë„
        
        V = intercept + Î²Â·X + Î»_mainÂ·LV_main + Î£(Î»_mod_i Â· LV_main Â· LV_mod_i)
        """
        # íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        intercept = params['intercept']
        beta = params['beta']
        lambda_main = params['lambda_main']
        lambda_mod = params.get('lambda_mod', np.zeros(self.n_moderators))
        
        # ë°ì´í„° ì¶”ì¶œ
        X = data[self.choice_attributes].values
        choice = data['choice'].values
        
        # ì£¼ ì ì¬ë³€ìˆ˜
        lv_main = latent_vars[self.main_lv]
        
        # íš¨ìš© ê³„ì‚° - ê¸°ë³¸ ë¶€ë¶„
        V = intercept + X @ beta + lambda_main * lv_main
        
        # ì¡°ì ˆíš¨ê³¼ ì¶”ê°€
        for i, mod_lv_name in enumerate(self.moderator_lvs):
            lv_mod = latent_vars[mod_lv_name]
            # ìƒí˜¸ì‘ìš©í•­: LV_main Ã— LV_mod
            interaction = lv_main * lv_mod
            V += lambda_mod[i] * interaction
        
        # í™•ë¥  ê³„ì‚°
        prob_yes = norm.cdf(V)
        prob_yes = np.clip(prob_yes, 1e-10, 1 - 1e-10)
        
        # ë¡œê·¸ìš°ë„
        ll = np.sum(
            choice * np.log(prob_yes) +
            (1 - choice) * np.log(1 - prob_yes)
        )
        
        return ll
    
    def initialize_parameters(self) -> Dict:
        """íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”"""
        params = {
            'intercept': 0.0,
            'beta': np.zeros(self.n_attributes),
            'lambda_main': 1.0,
            'lambda_mod': np.zeros(self.n_moderators)
        }
        
        # ê°€ê²© ë³€ìˆ˜ ìŒìˆ˜ ì´ˆê¸°í™”
        if self.config.price_variable in self.choice_attributes:
            price_idx = self.choice_attributes.index(self.config.price_variable)
            params['beta'][price_idx] = -1.0
        
        return params
```

---

## ğŸ“Š ìˆ˜ì¹˜ ì˜ˆì‹œ

### ì˜ˆì‹œ 1: ê°€ê²©ìˆ˜ì¤€ì˜ ë¶€ì  ì¡°ì ˆíš¨ê³¼

**íŒŒë¼ë¯¸í„°**:
```python
lambda_main = 1.0
lambda_mod_price = -0.3
```

**ì‹œë‚˜ë¦¬ì˜¤**:
```
êµ¬ë§¤ì˜ë„ (PI) = 1.0 (ë†’ìŒ)

Case 1: ê°€ê²©ìˆ˜ì¤€ (PP) = -1.0 (ë‚®ìŒ, ì €ë ´í•˜ë‹¤ê³  ì¸ì‹)
  V = ... + 1.0 Ã— 1.0 + (-0.3) Ã— (1.0 Ã— -1.0)
    = ... + 1.0 + 0.3
    = ... + 1.3  â† íš¨ìš© ì¦ê°€!

Case 2: ê°€ê²©ìˆ˜ì¤€ (PP) = 1.0 (ë†’ìŒ, ë¹„ì‹¸ë‹¤ê³  ì¸ì‹)
  V = ... + 1.0 Ã— 1.0 + (-0.3) Ã— (1.0 Ã— 1.0)
    = ... + 1.0 - 0.3
    = ... + 0.7  â† íš¨ìš© ê°ì†Œ!
```

**í•´ì„**: 
- êµ¬ë§¤ì˜ë„ê°€ ê°™ì•„ë„ ê°€ê²©ìˆ˜ì¤€ì´ ë†’ìœ¼ë©´ ì‹¤ì œ ì„ íƒ í™•ë¥  ê°ì†Œ
- ê°€ê²©ì´ ì¥ë²½ ì—­í• 

---

### ì˜ˆì‹œ 2: ì˜ì–‘ì§€ì‹ì˜ ì •ì  ì¡°ì ˆíš¨ê³¼

**íŒŒë¼ë¯¸í„°**:
```python
lambda_main = 1.0
lambda_mod_knowledge = 0.2
```

**ì‹œë‚˜ë¦¬ì˜¤**:
```
êµ¬ë§¤ì˜ë„ (PI) = 1.0 (ë†’ìŒ)

Case 1: ì˜ì–‘ì§€ì‹ (NK) = -1.0 (ë‚®ìŒ)
  V = ... + 1.0 Ã— 1.0 + 0.2 Ã— (1.0 Ã— -1.0)
    = ... + 1.0 - 0.2
    = ... + 0.8  â† íš¨ìš© ê°ì†Œ

Case 2: ì˜ì–‘ì§€ì‹ (NK) = 1.0 (ë†’ìŒ)
  V = ... + 1.0 Ã— 1.0 + 0.2 Ã— (1.0 Ã— 1.0)
    = ... + 1.0 + 0.2
    = ... + 1.2  â† íš¨ìš© ì¦ê°€!
```

**í•´ì„**: 
- êµ¬ë§¤ì˜ë„ê°€ ê°™ì•„ë„ ì˜ì–‘ì§€ì‹ì´ ë†’ìœ¼ë©´ ì‹¤ì œ ì„ íƒ í™•ë¥  ì¦ê°€
- ì˜ì–‘ì§€ì‹ì´ ì´‰ì§„ ì—­í• 

---

### ì˜ˆì‹œ 3: ë³µí•© íš¨ê³¼

**íŒŒë¼ë¯¸í„°**:
```python
intercept = 0.0
beta = np.array([0.5, 0.3, -1.0])  # [sugar_free, health_label, price]
lambda_main = 1.0
lambda_mod_price = -0.3
lambda_mod_knowledge = 0.2
```

**ì‹œë‚˜ë¦¬ì˜¤**:
```
ì„ íƒ ì†ì„±: X = [1, 1, 0.5]  # sugar_free=1, health_label=1, price=0.5
êµ¬ë§¤ì˜ë„: PI = 1.0
ê°€ê²©ìˆ˜ì¤€: PP = 0.5 (ì•½ê°„ ë¹„ìŒˆ)
ì˜ì–‘ì§€ì‹: NK = 1.0 (ë†’ìŒ)

V = 0.0 + 
    [1, 1, 0.5] @ [0.5, 0.3, -1.0] +  # ì†ì„± íš¨ê³¼
    1.0 Ã— 1.0 +                        # êµ¬ë§¤ì˜ë„ ì£¼íš¨ê³¼
    (-0.3) Ã— (1.0 Ã— 0.5) +             # ê°€ê²©ìˆ˜ì¤€ ì¡°ì ˆ
    0.2 Ã— (1.0 Ã— 1.0)                  # ì˜ì–‘ì§€ì‹ ì¡°ì ˆ

  = 0.0 + (0.5 + 0.3 - 0.5) + 1.0 + (-0.15) + 0.2
  = 0.3 + 1.0 - 0.15 + 0.2
  = 1.35

P(choice=1) = Î¦(1.35) â‰ˆ 0.91 (91% ì„ íƒ í™•ë¥ )
```

---

## ğŸ” ì¡°ì ˆíš¨ê³¼ í•´ì„ ë°©ë²•

### Simple Slopes Analysis

ì¡°ì ˆë³€ìˆ˜ì˜ ìˆ˜ì¤€ë³„ë¡œ ì£¼íš¨ê³¼ë¥¼ ê³„ì‚°:

```python
def calculate_simple_slopes(lambda_main, lambda_mod, moderator_values):
    """
    ë‹¨ìˆœ ê¸°ìš¸ê¸° ê³„ì‚°
    
    Args:
        lambda_main: ì£¼íš¨ê³¼ ê³„ìˆ˜
        lambda_mod: ì¡°ì ˆíš¨ê³¼ ê³„ìˆ˜
        moderator_values: ì¡°ì ˆë³€ìˆ˜ ê°’ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: [-1, 0, 1])
    
    Returns:
        ê° ì¡°ì ˆë³€ìˆ˜ ìˆ˜ì¤€ì—ì„œì˜ ì£¼íš¨ê³¼
    """
    slopes = {}
    for mod_val in moderator_values:
        # ì£¼íš¨ê³¼ = Î»_main + Î»_mod Ã— M
        slope = lambda_main + lambda_mod * mod_val
        slopes[f'M={mod_val}'] = slope
    
    return slopes

# ì˜ˆì‹œ
slopes = calculate_simple_slopes(
    lambda_main=1.0,
    lambda_mod=-0.3,
    moderator_values=[-1, 0, 1]  # ë‚®ìŒ, ì¤‘ê°„, ë†’ìŒ
)

print(slopes)
# {
#   'M=-1': 1.3,  # ê°€ê²©ìˆ˜ì¤€ ë‚®ì„ ë•Œ: êµ¬ë§¤ì˜ë„ íš¨ê³¼ ê°•í•¨
#   'M=0': 1.0,   # ê°€ê²©ìˆ˜ì¤€ ì¤‘ê°„: êµ¬ë§¤ì˜ë„ íš¨ê³¼ ë³´í†µ
#   'M=1': 0.7    # ê°€ê²©ìˆ˜ì¤€ ë†’ì„ ë•Œ: êµ¬ë§¤ì˜ë„ íš¨ê³¼ ì•½í•¨
# }
```

---

## ğŸ“ˆ ì‹œê°í™”

### ì¡°ì ˆíš¨ê³¼ ê·¸ë˜í”„

```python
import matplotlib.pyplot as plt
import numpy as np

def plot_moderation_effect(lambda_main, lambda_mod, 
                          moderator_name='Moderator',
                          main_var_name='Main Variable'):
    """ì¡°ì ˆíš¨ê³¼ ì‹œê°í™”"""
    
    # ì£¼ ë³€ìˆ˜ ë²”ìœ„
    main_var = np.linspace(-2, 2, 100)
    
    # ì¡°ì ˆë³€ìˆ˜ ìˆ˜ì¤€ (ë‚®ìŒ, ì¤‘ê°„, ë†’ìŒ)
    mod_levels = {'Low (-1SD)': -1, 'Mean': 0, 'High (+1SD)': 1}
    
    plt.figure(figsize=(10, 6))
    
    for label, mod_val in mod_levels.items():
        # íš¨ìš© = Î»_main Ã— X + Î»_mod Ã— (X Ã— M)
        #      = (Î»_main + Î»_mod Ã— M) Ã— X
        slope = lambda_main + lambda_mod * mod_val
        utility = slope * main_var
        
        plt.plot(main_var, utility, label=f'{moderator_name} {label}', linewidth=2)
    
    plt.xlabel(main_var_name, fontsize=12)
    plt.ylabel('Utility Contribution', fontsize=12)
    plt.title(f'Moderation Effect of {moderator_name}', fontsize=14)
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)
    plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)
    
    return plt

# ì˜ˆì‹œ: ê°€ê²©ìˆ˜ì¤€ì˜ ë¶€ì  ì¡°ì ˆíš¨ê³¼
plot_moderation_effect(
    lambda_main=1.0,
    lambda_mod=-0.3,
    moderator_name='Perceived Price',
    main_var_name='Purchase Intention'
)
plt.show()
```

---

## âš™ï¸ ì‹¤ì œ ì¶”ì • ì‹œ ê³ ë ¤ì‚¬í•­

### 1. ë‹¤ì¤‘ê³µì„ ì„± (Multicollinearity)

**ë¬¸ì œ**: ìƒí˜¸ì‘ìš©í•­ (PI Ã— PP)ì´ ì£¼íš¨ê³¼ (PI, PP)ì™€ ë†’ì€ ìƒê´€

**í•´ê²°ì±…**: ì¤‘ì‹¬í™” (Centering)
```python
# ì ì¬ë³€ìˆ˜ ì¤‘ì‹¬í™”
lv_main_centered = lv_main - np.mean(lv_main)
lv_mod_centered = lv_mod - np.mean(lv_mod)

# ìƒí˜¸ì‘ìš©í•­ ê³„ì‚°
interaction = lv_main_centered * lv_mod_centered
```

**ì°¸ê³ **: ICLVì—ì„œëŠ” ì ì¬ë³€ìˆ˜ê°€ ì´ë¯¸ í‘œì¤€í™”ë˜ì–´ ìˆìœ¼ë¯€ë¡œ (í‰ê· â‰ˆ0) ì¤‘ì‹¬í™” ë¶ˆí•„ìš”

---

### 2. íŒŒë¼ë¯¸í„° ì´ˆê¸°ê°’

```python
params = {
    'intercept': 0.0,
    'beta': np.zeros(n_attributes),
    'lambda_main': 1.0,      # ì£¼íš¨ê³¼: ì–‘ìˆ˜ë¡œ ì´ˆê¸°í™”
    'lambda_mod': np.zeros(n_moderators)  # ì¡°ì ˆíš¨ê³¼: 0ìœ¼ë¡œ ì´ˆê¸°í™”
}
```

---

### 3. í†µê³„ì  ìœ ì˜ì„± ê²€ì •

ì¡°ì ˆíš¨ê³¼ê°€ ìœ ì˜í•œì§€ í™•ì¸:
```python
# H0: Î»_mod = 0 (ì¡°ì ˆíš¨ê³¼ ì—†ìŒ)
# H1: Î»_mod â‰  0 (ì¡°ì ˆíš¨ê³¼ ìˆìŒ)

z_score = lambda_mod / se_lambda_mod
p_value = 2 * (1 - norm.cdf(abs(z_score)))

if p_value < 0.05:
    print("ì¡°ì ˆíš¨ê³¼ ìœ ì˜í•¨!")
```

---

## ğŸ¯ ìš”ì•½

### ì¡°ì ˆíš¨ê³¼ êµ¬í˜„ í•µì‹¬

1. **ì…ë ¥ ë³€ê²½**: `lv` (ìŠ¤ì¹¼ë¼) â†’ `latent_vars` (ë”•ì…”ë„ˆë¦¬)
2. **íŒŒë¼ë¯¸í„° ì¶”ê°€**: `lambda` â†’ `lambda_main` + `lambda_mod`
3. **íš¨ìš© í•¨ìˆ˜ ìˆ˜ì •**: ìƒí˜¸ì‘ìš©í•­ ì¶”ê°€
   ```python
   V = intercept + Î²Â·X + Î»_mainÂ·LV_main + Î»_modÂ·(LV_main Ã— LV_mod)
   ```

### ì˜ˆìƒ ê²°ê³¼

```
ê°€ê²©ìˆ˜ì¤€ ì¡°ì ˆíš¨ê³¼: Î»â‚‚ â‰ˆ -0.2 ~ -0.4 (ë¶€ì , ì¥ë²½ ì—­í• )
ì˜ì–‘ì§€ì‹ ì¡°ì ˆíš¨ê³¼: Î»â‚ƒ â‰ˆ 0.2 ~ 0.4 (ì •ì , ì´‰ì§„ ì—­í• )
```

### í•´ì„

- **ê°€ê²©ìˆ˜ì¤€ ë†’ìŒ**: êµ¬ë§¤ì˜ë„ ìˆì–´ë„ ì„ íƒ í™•ë¥  ê°ì†Œ â†’ ê°€ê²© í• ì¸ í•„ìš”
- **ì˜ì–‘ì§€ì‹ ë†’ìŒ**: êµ¬ë§¤ì˜ë„ê°€ ì„ íƒìœ¼ë¡œ ì „í™˜ ì˜ë¨ â†’ êµìœ¡ í”„ë¡œê·¸ë¨ íš¨ê³¼ì 

---

## ğŸ“š ì°¸ê³ ìë£Œ

- Aiken & West (1991). *Multiple Regression: Testing and Interpreting Interactions*
- Hayes (2018). *Introduction to Mediation, Moderation, and Conditional Process Analysis*
- Cohen et al. (2003). *Applied Multiple Regression/Correlation Analysis*

