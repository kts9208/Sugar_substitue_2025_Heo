# Numerical Gradient 계산 로직 상세 설명

## 1. Two-Point Finite Difference (2점 유한차분)

### 기본 원리

**수치적 그래디언트**는 미분의 정의를 직접 사용합니다:

```
f'(x) ≈ [f(x + ε) - f(x)] / ε
```

여기서:
- `f(x)`: 현재 점에서의 함수 값
- `f(x + ε)`: 약간 이동한 점에서의 함수 값
- `ε`: 작은 증분 (epsilon, 우리 코드에서는 1e-4 = 0.0001)

### scipy.optimize.approx_fprime의 동작

```python
def approx_fprime(xk, f, epsilon=1e-4):
    """
    xk: 현재 파라미터 벡터 [θ₁, θ₂, ..., θₙ]
    f: 목적 함수 (우도 함수)
    epsilon: 증분 크기
    """
    # 1. 현재 점에서 함수 값 계산
    f0 = f(xk)  # f(θ₁, θ₂, ..., θₙ)
    
    # 2. 각 파라미터에 대해 편미분 계산
    grad = np.zeros(len(xk))
    
    for i in range(len(xk)):
        # i번째 파라미터만 epsilon만큼 증가
        xk_plus = xk.copy()
        xk_plus[i] += epsilon
        
        # 증가된 점에서 함수 값 계산
        f_plus = f(xk_plus)
        
        # 편미분 근사
        grad[i] = (f_plus - f0) / epsilon
    
    return grad
```

### 우리 코드에서의 실제 동작 (202개 파라미터)

**초기 상태:**
```
θ = [1.0, 1.0, 1.0, ..., 0.0, 0.0, 1.0]  # 202개
     ↑    ↑    ↑         ↑    ↑    ↑
   zeta1 zeta2 zeta3   beta1 beta2 lambda
```

**그래디언트 계산 과정:**

1. **함수 호출 #1**: `f(θ)` 계산 → LL = -43728.7054
   ```
   θ = [1.0, 1.0, 1.0, ..., 0.0, 0.0, 1.0]
   ```

2. **함수 호출 #2**: `f(θ + [ε, 0, 0, ..., 0])` 계산
   ```
   θ = [1.0001, 1.0, 1.0, ..., 0.0, 0.0, 1.0]
        ↑
      첫 번째 파라미터만 +0.0001
   ```
   → `∂LL/∂θ₁ = [f(θ₁+ε) - f(θ₁)] / ε`

3. **함수 호출 #3**: `f(θ + [0, ε, 0, ..., 0])` 계산
   ```
   θ = [1.0, 1.0001, 1.0, ..., 0.0, 0.0, 1.0]
             ↑
           두 번째 파라미터만 +0.0001
   ```
   → `∂LL/∂θ₂ = [f(θ₂+ε) - f(θ₂)] / ε`

4. **함수 호출 #4 ~ #203**: 나머지 200개 파라미터에 대해 반복

**총 함수 호출 횟수**: 1 (기준점) + 202 (각 파라미터) = **203회**

---

## 2. BFGS 알고리즘의 다음 Point 설정

### BFGS (Broyden-Fletcher-Goldfarb-Shanno) 알고리즘

BFGS는 **Quasi-Newton 방법**으로, Hessian 행렬의 역행렬을 근사하여 최적화합니다.

### 다음 Point 결정 로직

```python
# Iteration k에서:
θₖ = 현재 파라미터
gₖ = ∇f(θₖ)  # 그래디언트 (위에서 계산한 202개 값)
Hₖ = Hessian 역행렬 근사 (202×202 행렬)

# 1. 탐색 방향 계산
pₖ = -Hₖ · gₖ  # Newton 방향

# 2. Line Search: 최적 step size 찾기
αₖ = argmin f(θₖ + α·pₖ)  # Wolfe 조건 만족하는 α 찾기
      α>0

# 3. 다음 점 업데이트
θₖ₊₁ = θₖ + αₖ·pₖ

# 4. Hessian 역행렬 근사 업데이트 (BFGS 공식)
sₖ = θₖ₊₁ - θₖ
yₖ = gₖ₊₁ - gₖ
Hₖ₊₁ = Hₖ + (sₖ·sₖᵀ)/(sₖᵀ·yₖ) - (Hₖ·yₖ·yₖᵀ·Hₖ)/(yₖᵀ·Hₖ·yₖ)
```

### 실제 예시 (우리 코드)

**Iteration 1:**
```
θ₁ = [1.0, 1.0, 1.0, ..., 0.0, 0.0, 1.0]
g₁ = [∂LL/∂θ₁, ∂LL/∂θ₂, ..., ∂LL/∂θ₂₀₂]  # 203회 함수 호출로 계산
H₁ = I (단위 행렬, 초기값)

p₁ = -g₁  # 첫 iteration은 steepest descent
α₁ = line_search(θ₁, p₁)  # 추가 함수 호출 (보통 5-10회)

θ₂ = θ₁ + α₁·p₁
```

**Iteration 2:**
```
θ₂ = [1.0001, 0.9998, 1.0002, ..., 0.0001, -0.0001, 1.0001]
      ↑       ↑       ↑            ↑        ↑        ↑
    변화    변화    변화         변화     변화     변화

g₂ = [∂LL/∂θ₁, ∂LL/∂θ₂, ..., ∂LL/∂θ₂₀₂]  # 다시 203회 함수 호출

# Hessian 역행렬 업데이트
s₁ = θ₂ - θ₁
y₁ = g₂ - g₁
H₂ = H₁ + BFGS_update(s₁, y₁, H₁)

p₂ = -H₂·g₂  # 이제 Hessian 정보 활용
α₂ = line_search(θ₂, p₂)

θ₃ = θ₂ + α₂·p₂
```

---

## 3. Line Search (선 탐색)

### Wolfe 조건을 만족하는 Step Size 찾기

BFGS는 방향 `p`를 결정한 후, **얼마나 멀리 갈지** (step size `α`)를 결정해야 합니다.

```python
def line_search(θ, p, f, g):
    """
    θ: 현재 점
    p: 탐색 방향
    f: 목적 함수
    g: 그래디언트
    """
    # Wolfe 조건:
    # 1. Sufficient decrease (Armijo 조건)
    #    f(θ + α·p) ≤ f(θ) + c₁·α·gᵀ·p
    # 2. Curvature 조건
    #    |g(θ + α·p)ᵀ·p| ≤ c₂·|gᵀ·p|
    
    α = 1.0  # 초기 step size
    c₁ = 1e-4
    c₂ = 0.9
    
    # Backtracking line search
    while True:
        θ_new = θ + α·p
        
        if f(θ_new) ≤ f(θ) + c₁·α·gᵀ·p:
            # Armijo 조건 만족
            g_new = gradient(θ_new)
            if |g_new·p| ≤ c₂·|g·p|:
                # Curvature 조건도 만족
                return α
        
        α = α / 2  # Step size 줄이기
```

**함수 호출 횟수:**
- 각 line search마다 보통 **5-10회** 함수 호출
- 그래디언트 계산도 필요하면 추가 **203회** 호출

---

## 4. 전체 최적화 과정 요약

### Iteration별 함수 호출 횟수

| Iteration | 작업 | 함수 호출 횟수 |
|-----------|------|----------------|
| 1 | 초기 LL 계산 | 1 |
| 1 | 그래디언트 계산 | 203 (1 + 202) |
| 1 | Line search | ~5-10 |
| **1 합계** | | **~209-214** |
| 2 | 그래디언트 계산 | 203 |
| 2 | Line search | ~5-10 |
| **2 합계** | | **~208-213** |
| ... | ... | ... |

**총 함수 호출 횟수 (100 iterations):**
- 약 **20,000 ~ 21,000회** 우도 계산
- GPU 사용 시 각 우도 계산: ~22초
- **총 예상 시간**: 20,000 × 22초 ≈ **122시간** (5일)

하지만 실제로는:
1. **조기 종료**: 20회 연속 개선 없으면 종료
2. **Line search 최적화**: scipy가 효율적으로 구현
3. **수렴 가속**: BFGS가 Hessian 정보 활용

→ 실제로는 **10-50 iterations** 정도면 수렴

---

## 5. 우리 코드의 실제 로그 분석

### 로그에서 확인된 패턴

```
2025-11-09 20:50:01 - [_unpack_parameters 호출 #1]
  params 처음 10개: [1. 1. 1. 1. 1. 1. -2. -1. 1. 2.]
  → 초기 LL 계산

2025-11-09 20:50:23 - Iter 1: LL = -43728.7054
  → 22초 후 첫 iteration 완료

2025-11-09 20:50:23 - [_unpack_parameters 호출 #2]
  params 처음 10개: [1. 1. 1. 1. 1. 1. -2. -1. 1. 2.]
  → 그래디언트 계산 시작 (기준점)

2025-11-09 20:50:46 - [_unpack_parameters 호출 #3]
  params 처음 10개: [1.0001 1. 1. 1. 1. 1. -2. -1. 1. 2.]
                      ↑
                    첫 번째 파라미터 +0.0001
  → ∂LL/∂θ₁ 계산

2025-11-09 20:51:08 - Iter 3: LL = -43728.6968
  → 개선! (ΔLL = 0.0086)

2025-11-09 20:51:08 - [_unpack_parameters 호출 #4]
  params 처음 10개: [1. 1.0001 1. 1. 1. 1. -2. -1. 1. 2.]
                         ↑
                       두 번째 파라미터 +0.0001
  → ∂LL/∂θ₂ 계산
```

### 패턴 분석

1. **함수 호출 #1**: 초기 LL 계산 (22초)
2. **함수 호출 #2**: 그래디언트 계산 기준점 (즉시, 캐시됨)
3. **함수 호출 #3**: 첫 번째 파라미터 편미분 (23초)
4. **Iter 3 출력**: Line search 완료 후 새로운 점 발견
5. **함수 호출 #4**: 두 번째 파라미터 편미분 (23초)

→ **각 파라미터마다 ~22초** 소요
→ **202개 파라미터 × 22초 ≈ 74분** (그래디언트 1회 계산)

---

## 6. BFGS의 다음 Point 결정 전략

### 핵심 아이디어

BFGS는 **2차 정보 (Hessian)**를 활용하여 Newton 방법을 근사합니다:

```
Newton 방법:
θₖ₊₁ = θₖ - [∇²f(θₖ)]⁻¹ · ∇f(θₖ)
           ↑
         Hessian 역행렬 (계산 비용 높음)

BFGS:
θₖ₊₁ = θₖ - Hₖ · ∇f(θₖ)
           ↑
         Hessian 역행렬 근사 (효율적 업데이트)
```

### Hessian 역행렬 근사 업데이트

```python
# Iteration k → k+1
sₖ = θₖ₊₁ - θₖ  # 파라미터 변화량
yₖ = gₖ₊₁ - gₖ  # 그래디언트 변화량

# BFGS 공식 (Sherman-Morrison-Woodbury)
ρₖ = 1 / (yₖᵀ · sₖ)

Hₖ₊₁ = (I - ρₖ·sₖ·yₖᵀ) · Hₖ · (I - ρₖ·yₖ·sₖᵀ) + ρₖ·sₖ·sₖᵀ
```

이 공식은:
1. **Positive definite** 유지 (항상 하강 방향 보장)
2. **Rank-2 update** (효율적 계산)
3. **Secant 조건** 만족: `Hₖ₊₁ · yₖ = sₖ`

---

## 7. 요약

### Numerical Gradient 계산

1. **2-point finite difference**: `f'(x) ≈ [f(x+ε) - f(x)] / ε`
2. **함수 호출**: 1 (기준) + n (파라미터 개수) = 203회
3. **Epsilon**: 1e-4 (0.0001)

### BFGS 다음 Point 결정

1. **그래디언트 계산**: Numerical gradient (203회 함수 호출)
2. **Hessian 근사**: 이전 iterations의 정보로 업데이트
3. **탐색 방향**: `p = -H · g` (Newton 방향)
4. **Step size**: Line search로 최적 α 찾기 (5-10회 함수 호출)
5. **다음 점**: `θ_new = θ_old + α · p`

### 효율성

- **Analytic gradient**: 1회 함수 호출로 모든 편미분 계산 (이론적으로 빠름)
- **Numerical gradient**: 203회 함수 호출 필요 (하지만 GPU 활용 가능)
- **우리 코드**: GPU 우도 계산 (22초) × 203 ≈ **74분/iteration**

### 수렴 속도

- **초기**: Steepest descent (느림)
- **중기**: Hessian 정보 축적 (가속)
- **후기**: Quasi-Newton (빠른 수렴)
- **예상**: 10-50 iterations로 수렴

