# Hessian ê³„ì‚° ë¡œì§ ìƒì„¸ ì„¤ëª…

**ë‚ ì§œ**: 2025-11-20  
**ì‘ì„±ì**: AI Assistant

---

## ğŸ“‹ ê°œìš”

í˜„ì¬ ì‹œìŠ¤í…œì—ì„œ ì‚¬ìš©í•˜ëŠ” **Hessian ê³„ì‚° ë°©ë²•**ì„ ì„¤ëª…í•©ë‹ˆë‹¤:

1. **L-BFGS-B/BFGSì˜ Hessian ê·¼ì‚¬** (ìµœì í™” ì¤‘ ìë™ ê³„ì‚°) âœ… **ì£¼ ë°©ë²•**
2. **BHHH ë°©ë²•** (Fallback - optimizerê°€ hess_invë¥¼ ì œê³µí•˜ì§€ ì•Šì„ ë•Œë§Œ)

---

## ğŸ”µ 1. L-BFGS-Bì˜ Hessian ê·¼ì‚¬ (ìµœì í™” ì¤‘)

### 1.1 ì•Œê³ ë¦¬ì¦˜ ê°œìš”

L-BFGS-BëŠ” **Limited-memory BFGS**ë¡œ, ì „ì²´ Hessian í–‰ë ¬ì„ ì €ì¥í•˜ì§€ ì•Šê³  **ìµœê·¼ mê°œì˜ (s, y) ìŒ**ë§Œ ì €ì¥í•©ë‹ˆë‹¤.

```python
# L-BFGS-B ì˜ì‚¬ ì½”ë“œ
def lbfgs_b_optimizer(fun, x0, jac, bounds, maxiter=200):
    """
    L-BFGS-B ìµœì í™” ì•Œê³ ë¦¬ì¦˜
    
    Args:
        fun: ëª©ì  í•¨ìˆ˜ (ìš°ë„ í•¨ìˆ˜)
        x0: ì´ˆê¸° íŒŒë¼ë¯¸í„°
        jac: Gradient í•¨ìˆ˜
        bounds: íŒŒë¼ë¯¸í„° bounds
    """
    # ì´ˆê¸°í™”
    x = x0
    m = 10  # ë©”ëª¨ë¦¬ í¬ê¸° (ìµœê·¼ 10ê°œ ìŒë§Œ ì €ì¥)
    s_history = []  # íŒŒë¼ë¯¸í„° ë³€í™” ì´ë ¥
    y_history = []  # Gradient ë³€í™” ì´ë ¥
    
    for k in range(maxiter):
        # 1. Gradient ê³„ì‚°
        g = jac(x)  # â† ìš°ë¦¬ì˜ analytic gradient í•¨ìˆ˜ í˜¸ì¶œ
        
        # 2. íƒìƒ‰ ë°©í–¥ ê³„ì‚° (Two-loop recursion)
        # H^(-1) Â· gë¥¼ ëª…ì‹œì ìœ¼ë¡œ ê³„ì‚°í•˜ì§€ ì•Šê³  ì•”ë¬µì ìœ¼ë¡œ ê³„ì‚°
        p = two_loop_recursion(s_history, y_history, g)
        
        # 3. Line search (Wolfe ì¡°ê±´)
        alpha = line_search(fun, jac, x, p, g, bounds)
        
        # 4. íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        x_new = x + alpha * p
        g_new = jac(x_new)
        
        # 5. (s, y) ìŒ ì €ì¥
        s = x_new - x  # íŒŒë¼ë¯¸í„° ë³€í™”
        y = g_new - g  # Gradient ë³€í™”
        
        # 6. ë©”ëª¨ë¦¬ ê´€ë¦¬ (ìµœê·¼ mê°œë§Œ ìœ ì§€)
        if len(s_history) >= m:
            s_history.pop(0)
            y_history.pop(0)
        
        s_history.append(s)
        y_history.append(y)
        
        # 7. ìˆ˜ë ´ ì²´í¬
        if converged(g_new):
            break
        
        x = x_new
    
    # âŒ Hessian ì—­í–‰ë ¬ì„ ë°˜í™˜í•˜ì§€ ì•ŠìŒ!
    return OptimizeResult(x=x, fun=fun(x), jac=g, success=True)
```

### 1.2 Two-Loop Recursion

L-BFGS-Bì˜ í•µì‹¬ì€ **Two-loop recursion**ìœ¼ë¡œ H^(-1) Â· gë¥¼ ê³„ì‚°í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤:

```python
def two_loop_recursion(s_history, y_history, g):
    """
    Two-loop recursionìœ¼ë¡œ H^(-1) Â· g ê³„ì‚°
    
    ì „ì²´ Hessian ì—­í–‰ë ¬ì„ ë§Œë“¤ì§€ ì•Šê³  ì•”ë¬µì ìœ¼ë¡œ ê³„ì‚°
    """
    m = len(s_history)
    q = g.copy()
    alpha = np.zeros(m)
    rho = np.zeros(m)
    
    # First loop (backward)
    for i in range(m-1, -1, -1):
        rho[i] = 1.0 / np.dot(y_history[i], s_history[i])
        alpha[i] = rho[i] * np.dot(s_history[i], q)
        q = q - alpha[i] * y_history[i]
    
    # Initial Hessian approximation
    if m > 0:
        gamma = np.dot(s_history[-1], y_history[-1]) / np.dot(y_history[-1], y_history[-1])
    else:
        gamma = 1.0
    
    r = gamma * q
    
    # Second loop (forward)
    for i in range(m):
        beta = rho[i] * np.dot(y_history[i], r)
        r = r + s_history[i] * (alpha[i] - beta)
    
    return -r  # íƒìƒ‰ ë°©í–¥ (ìŒìˆ˜)
```

### 1.3 L-BFGS-Bì˜ hess_inv ë°˜í™˜

#### âœ… L-BFGS-BëŠ” `hess_inv`ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤!

```python
result = scipy.optimize.minimize(..., method='L-BFGS-B')
print(type(result.hess_inv))
# <class 'scipy.optimize._lbfgsb_py.LbfgsInvHessProduct'>

# numpy ë°°ì—´ë¡œ ë³€í™˜
hess_inv_array = result.hess_inv.todense()
```

**íŠ¹ì§•**:
- íƒ€ì…: `LbfgsInvHessProduct` ê°ì²´ (BFGSëŠ” `numpy.ndarray`)
- ë©”ëª¨ë¦¬ íš¨ìœ¨: ì „ì²´ í–‰ë ¬ì„ ì €ì¥í•˜ì§€ ì•Šê³  (s, y) ìŒë§Œ ì €ì¥
- ì‚¬ìš© ë°©ë²•: `todense()` ë©”ì„œë“œë¡œ numpy ë°°ì—´ë¡œ ë³€í™˜ ê°€ëŠ¥
- ë²¡í„° ê³±: `hess_inv @ v` ì—°ì‚° ì§€ì›

### 1.4 ë¬¸ì œì 

#### âŒ ë¬¸ì œ: y_k/s_k ë¹„ìœ¨ì´ í´ ë•Œ ë¶ˆì•ˆì •
- `Ï = 1/(s_k^T Â· y_k)`ê°€ ë§¤ìš° ì‘ì•„ì§ (y_kê°€ í´ ë•Œ)
- Two-loop recursionì—ì„œ ìˆ˜ì¹˜ì  ë¶ˆì•ˆì • ë°œìƒ
- íƒìƒ‰ ë°©í–¥ì´ 0ì´ ë˜ëŠ” í˜„ìƒ
- **í•˜ì§€ë§Œ ìµœì í™”ê°€ ì„±ê³µí•˜ë©´ hess_invëŠ” ì •ìƒì ìœ¼ë¡œ ì œê³µë¨**

---

## ğŸŸ¢ 2. BHHH ë°©ë²•ì˜ Hessian ê³„ì‚° (Fallback)

### 2.1 ì´ë¡ ì  ë°°ê²½

**BHHH (Berndt-Hall-Hall-Hausman)** ë°©ë²•ì€ Maximum Likelihood Estimationì—ì„œ Hessianì„ ê·¼ì‚¬í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

**ì‚¬ìš© ì‹œì **: Optimizerê°€ `hess_inv`ë¥¼ ì œê³µí•˜ì§€ ì•Šì„ ë•Œë§Œ ì‚¬ìš© (BFGS/L-BFGS-BëŠ” ì œê³µí•¨)

#### ì •í™•í•œ Hessian (2ì°¨ ë¯¸ë¶„)
```
H = âˆ‚Â²LL/âˆ‚Î¸âˆ‚Î¸^T = Î£_i âˆ‚Â²LL_i/âˆ‚Î¸âˆ‚Î¸^T
```

#### BHHH ê·¼ì‚¬ (1ì°¨ ë¯¸ë¶„ë§Œ ì‚¬ìš©)
```
H â‰ˆ Î£_i (âˆ‚LL_i/âˆ‚Î¸) Ã— (âˆ‚LL_i/âˆ‚Î¸)^T
  = Î£_i (grad_i Ã— grad_i^T)
  = OPG (Outer Product of Gradients)
```

### 2.2 êµ¬í˜„ ì½”ë“œ

<augment_code_snippet path="src/analysis/hybrid_choice_model/iclv_models/bhhh_calculator.py" mode="EXCERPT">
````python
def compute_bhhh_hessian(
    self,
    individual_gradients: List[np.ndarray],
    for_minimization: bool = True
) -> np.ndarray:
    """
    ê°œì¸ë³„ gradientë¡œë¶€í„° BHHH Hessian ê³„ì‚°
    
    BHHH = Î£_i (grad_i Ã— grad_i^T)
    """
    n_individuals = len(individual_gradients)
    n_params = len(individual_gradients[0])
    
    # BHHH Hessian ì´ˆê¸°í™”
    hessian_bhhh = np.zeros((n_params, n_params))
    
    # Î£_i (grad_i Ã— grad_i^T)
    for i, grad in enumerate(individual_gradients):
        # Outer product: grad_i Ã— grad_i^T
        outer_prod = np.outer(grad, grad)
        hessian_bhhh += outer_prod
    
    # ìµœì†Œí™” ë¬¸ì œì˜ ê²½ìš° ìŒìˆ˜ ë¶€í˜¸
    if for_minimization:
        hessian_bhhh = -hessian_bhhh
    
    return hessian_bhhh
````
</augment_code_snippet>

### 2.3 Hessian ì—­í–‰ë ¬ ê³„ì‚°

<augment_code_snippet path="src/analysis/hybrid_choice_model/iclv_models/bhhh_calculator.py" mode="EXCERPT">
````python
def compute_hessian_inverse(
    self,
    hessian: np.ndarray,
    regularization: float = 1e-8
) -> np.ndarray:
    """
    Hessian ì—­í–‰ë ¬ ê³„ì‚° (ì •ê·œí™” í¬í•¨)
    """
    n_params = hessian.shape[0]
    
    # ì •ê·œí™” (ìˆ˜ì¹˜ ì•ˆì •ì„±)
    hessian_reg = hessian + regularization * np.eye(n_params)
    
    # ì—­í–‰ë ¬ ê³„ì‚°
    hess_inv = np.linalg.inv(hessian_reg)
    
    return hess_inv
````
</augment_code_snippet>

### 2.4 ì¥ì 

âœ… **ê³„ì‚° íš¨ìœ¨ì„±**: 2ì°¨ ë¯¸ë¶„ ë¶ˆí•„ìš” (1ì°¨ ë¯¸ë¶„ë§Œ ì‚¬ìš©)  
âœ… **ì „ì²´ Hessian**: ëª¨ë“  íŒŒë¼ë¯¸í„° ê°„ ìƒê´€ê´€ê³„ í¬í•¨  
âœ… **í‘œì¤€ì˜¤ì°¨ ê³„ì‚°**: SE = sqrt(diag(H^(-1)))  
âœ… **Robust SE**: Sandwich estimator ê³„ì‚° ê°€ëŠ¥

---

## ğŸ”„ 3. í˜„ì¬ ì‹œìŠ¤í…œì˜ Hessian ê³„ì‚° íë¦„

### 3.1 ìµœì í™” ì¤‘ (L-BFGS-Bê°€ ìë™ìœ¼ë¡œ Hessian ê·¼ì‚¬)

```
Iteration 1:
  x0 â†’ g0 = jac(x0)
  p0 = two_loop_recursion([], [], g0) = -g0  (ì²« iterationì€ gradient descent)
  x1 = x0 + alpha * p0

  s0 = x1 - x0
  y0 = g1 - g0
  ì €ì¥: s_history = [s0], y_history = [y0]

Iteration 2:
  x1 â†’ g1 = jac(x1)
  p1 = two_loop_recursion([s0], [y0], g1)  â† Hessian ê·¼ì‚¬ ì‚¬ìš©
  x2 = x1 + alpha * p1

  s1 = x2 - x1
  y1 = g2 - g1
  ì €ì¥: s_history = [s0, s1], y_history = [y0, y1]

...

âŒ ë¬¸ì œ ë°œìƒ (Iteration 3):
  y_k/s_k ë¹„ìœ¨ì´ 690ìœ¼ë¡œ ë§¤ìš° í¼
  â†’ Ï = 1/(s_k^T Â· y_k)ê°€ ë§¤ìš° ì‘ìŒ
  â†’ two_loop_recursionì—ì„œ ìˆ˜ì¹˜ì  ë¶ˆì•ˆì •
  â†’ p = 0 (íƒìƒ‰ ë°©í–¥ì´ 0)
  â†’ ìµœì í™” ì¤‘ë‹¨

âœ… ìµœì í™” ì„±ê³µ ì‹œ:
  result.hess_inv = LbfgsInvHessProduct(s_history, y_history)
  â†’ todense()ë¡œ numpy ë°°ì—´ë¡œ ë³€í™˜
  â†’ í‘œì¤€ì˜¤ì°¨ ê³„ì‚°
```

### 3.2 ìµœì í™” í›„ Hessian ì²˜ë¦¬

```python
# ìš°ë¦¬ ì½”ë“œì˜ ì‹¤ì œ ë¡œì§
if hasattr(result, 'hess_inv') and result.hess_inv is not None:
    # âœ… L-BFGS-B/BFGSê°€ ì œê³µí•œ hess_inv ì‚¬ìš©
    if hasattr(result.hess_inv, 'todense'):
        # L-BFGS-B: LbfgsInvHessProduct â†’ numpy array
        hess_inv_array = result.hess_inv.todense()
    else:
        # BFGS: ì´ë¯¸ numpy array
        hess_inv_array = result.hess_inv

    # í‘œì¤€ì˜¤ì°¨ ê³„ì‚°
    SE = sqrt(diag(hess_inv_array))

else:
    # âŒ Optimizerê°€ hess_invë¥¼ ì œê³µí•˜ì§€ ì•ŠìŒ (ë“œë¬¸ ê²½ìš°)
    # â†’ BHHH ë°©ë²•ìœ¼ë¡œ ê³„ì‚° (Fallback)

    ê°œì¸ë³„ gradient ê³„ì‚°:
      for each individual i:
        grad_i = compute_individual_gradient(x_final, data_i)

    BHHH Hessian ê³„ì‚°:
      H = Î£_i (grad_i Ã— grad_i^T)

    Hessian ì—­í–‰ë ¬:
      H_inv = inv(H + 1e-8 * I)

    í‘œì¤€ì˜¤ì°¨:
      SE = sqrt(diag(H_inv))
```

---

## ğŸ“Š 4. ë¬¸ì œ ì§„ë‹¨: ì™œ íƒìƒ‰ ë°©í–¥ì´ 0ì´ ë˜ëŠ”ê°€?

### 4.1 ìˆ˜ì¹˜ì  ë¶„ì„

**Iteration #2 ë°ì´í„°**:
- s_k norm: 0.747
- y_k norm: 515.4
- **ë¹„ìœ¨: 690.2**
- s_k^T Â· y_k: 319.4
- **Ï = 1/319.4 = 0.00313**

**Two-loop recursionì—ì„œ**:
```python
# First loop
rho = 1 / (y^T Â· s) = 0.00313
alpha = rho * (s^T Â· q) = 0.00313 * (ë§¤ìš° í° ê°’)
q = q - alpha * y = q - (ë§¤ìš° í° ê°’) * y
```

â†’ `q`ê°€ ë§¤ìš° ì‘ì•„ì§ ë˜ëŠ” 0ì— ê°€ê¹Œì›Œì§  
â†’ `r = gamma * q â‰ˆ 0`  
â†’ **íƒìƒ‰ ë°©í–¥ p â‰ˆ 0**

### 4.2 ê·¼ë³¸ ì›ì¸

1. **Gradient í¬ê¸° ë¶ˆê· í˜•**
   - êµ¬ì¡°ëª¨ë¸: ~0.01
   - ì„ íƒëª¨ë¸: ~100~600
   - **10,000ë°° ì°¨ì´**

2. **íŒŒë¼ë¯¸í„° ìŠ¤ì¼€ì¼ ë¶ˆê· í˜•**
   - ëª¨ë“  ìŠ¤ì¼€ì¼ì´ 1.0ìœ¼ë¡œ ê³ ì •
   - ë¶ˆê· í˜• í•´ì†Œ ë¶ˆê°€

3. **Hessian ê·¼ì‚¬ì˜ ill-conditioning**
   - y_k/s_k ë¹„ìœ¨ì´ ë§¤ìš° í¼
   - Hessianì´ ë§¤ìš° í° ê³ ìœ ê°’ì„ ê°€ì§
   - ì—­í–‰ë ¬ì´ ê±°ì˜ 0

---

## ğŸ’¡ 5. í•´ê²°ì±…

### 5.1 íŒŒë¼ë¯¸í„° ìŠ¤ì¼€ì¼ë§

```python
# Gradient í¬ê¸°ì— ë”°ë¼ ìë™ ìŠ¤ì¼€ì¼ë§
scale_factors = 1.0 / np.maximum(np.abs(initial_gradient), 1.0)
x_scaled = x * scale_factors
```

### 5.2 Hessian ì£¼ê¸°ì  ë¦¬ì…‹

```python
# 10 iterationsë§ˆë‹¤ Hessianì„ ì´ˆê¸°ê°’(I)ìœ¼ë¡œ ë¦¬ì…‹
if iteration % 10 == 0:
    s_history.clear()
    y_history.clear()
```

### 5.3 Trust Region ë°©ë²•

```python
# L-BFGS-B ëŒ€ì‹  Trust Region ì‚¬ìš©
optimizer = 'trust-constr'
```

---

## ğŸ“ ê²°ë¡ 

### âœ… L-BFGS-BëŠ” `hess_inv`ë¥¼ ì œê³µí•©ë‹ˆë‹¤!

í˜„ì¬ ì‹œìŠ¤í…œì€:
- **L-BFGS-B/BFGS**: ìµœì í™” ì¤‘ Hessian ê·¼ì‚¬ ê³„ì‚° â†’ `hess_inv` ì œê³µ âœ…
  - L-BFGS-B: `LbfgsInvHessProduct` ê°ì²´ (todense()ë¡œ ë³€í™˜)
  - BFGS: `numpy.ndarray`
- **BHHH**: Fallback (optimizerê°€ hess_invë¥¼ ì œê³µí•˜ì§€ ì•Šì„ ë•Œë§Œ)

### ğŸ”´ í˜„ì¬ ë¬¸ì œ

ë¬¸ì œëŠ” **L-BFGS-Bì˜ Hessian ê·¼ì‚¬ê°€ ill-conditioned** ìƒíƒœê°€ ë˜ì–´ íƒìƒ‰ ë°©í–¥ì´ 0ì´ ë˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
- ìµœì í™”ê°€ ì¤‘ë‹¨ë¨ â†’ `hess_inv`ë¥¼ ë°›ì„ ìˆ˜ ì—†ìŒ
- í•´ê²°ì±…: **íŒŒë¼ë¯¸í„° ìŠ¤ì¼€ì¼ë§ + Hessian ë¦¬ì…‹**

### ğŸ“Œ ì½”ë“œ ìˆ˜ì • ì‚¬í•­

1. **ì˜ëª»ëœ ì£¼ì„ ìˆ˜ì •**: "L-BFGS-BëŠ” hess_inv ì œê³µ ì•ˆ í•¨" â†’ "L-BFGS-Bë„ hess_inv ì œê³µ"
2. **ë¡œê¹… ëª…í™•í™”**: L-BFGS-B vs BFGS êµ¬ë¶„
3. **BHHHëŠ” Fallback**: optimizerê°€ hess_invë¥¼ ì œê³µí•˜ì§€ ì•Šì„ ë•Œë§Œ ì‚¬ìš©

