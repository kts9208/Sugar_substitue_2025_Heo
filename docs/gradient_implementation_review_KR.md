# κ·Έλλ””μ–ΈνΈ κµ¬ν„ λ””ν…μΌ κ²€ν†  κ²°κ³Ό

## π“‹ κ²€ν†  ν•­λ©

1. β… **μΈ΅μ •λ¨λΈ κ·Έλλ””μ–ΈνΈ μ°¨μ› ν•©μ‚°** (grad_meas_LV)
2. β… **μ„ νƒλ¨λΈ κ·Έλλ””μ–ΈνΈ λ¶€νΈ** (grad_choice_LV)
3. β… **κµ¬μ΅°λ¨λΈ μ¤μ°¨λ¶„μ‚° κ·Έλλ””μ–ΈνΈ** (sigma_sq_PB)

---

## β‘  μΈ΅μ •λ¨λΈ κ·Έλλ””μ–ΈνΈ μ°¨μ› ν•©μ‚° β… **μ¬λ°”λ¦„**

### κ²€ν†  λ‚΄μ©
> **μ§λ¬Έ**: 38κ° μ§€ν‘μ—μ„ μ¤λ” κ·Έλλ””μ–ΈνΈλ¥Ό λ¨λ‘ λ”ν•΄μ•Ό(Sum) LVμ— λ€ν• μ΄ κ·Έλλ””μ–ΈνΈκ°€ λλ”κ°€?

### μ½”λ“ μ„μΉ
`src/analysis/hybrid_choice_model/iclv_models/gpu_gradient_batch.py`
- Line 46-142: `compute_measurement_grad_wrt_lv_gpu()`

### κµ¬ν„ ν™•μΈ

```python
# Line 72: μ΄κΈ°ν™”
grad_ll_wrt_lv = cp.zeros(n_draws)  # (n_draws,) ν•νƒ

# Line 94-111: κ° μ§€ν‘λ³„λ΅ λ„μ  ν•©μ‚°
for i, indicator in enumerate(config.indicators):
    if measurement_method == 'continuous_linear':
        # β‚LL/β‚LV = Ξ¶_i * (y_i - Ξ¶_i*LV) / ΟƒΒ²_i
        grad_ll_wrt_lv += zeta_gpu[i] * residual / sigma_sq_gpu[i]
    else:
        # Ordered Probit
        # β‚LL/β‚LV = (Ο†_upper - Ο†_lower) / P * (-Ξ¶_i)
        grad_ll_wrt_lv += (phi_upper - phi_lower) / prob * (-zeta_gpu[i])

# Line 141: λ°ν™
return cp.asnumpy(grad_ll_wrt_lv)  # (n_draws,)
```

### μμ‹ κ²€μ¦

**Continuous Linear λ°©μ‹:**
```
Y_i = Ξ±_i + Ξ¶_i Γ— LV + Ξµ_i,  Ξµ_i ~ N(0, ΟƒΒ²_i)

log L = Ξ£_i log P(Y_i | LV)
      = Ξ£_i [-0.5 log(2Ο€ ΟƒΒ²_i) - 0.5 (Y_i - Ξ±_i - Ξ¶_i Γ— LV)Β² / ΟƒΒ²_i]

β‚ log L / β‚LV = Ξ£_i β‚ log P(Y_i | LV) / β‚LV
               = Ξ£_i Ξ¶_i Γ— (Y_i - Ξ±_i - Ξ¶_i Γ— LV) / ΟƒΒ²_i
```

**Ordered Probit λ°©μ‹:**
```
V_i = Ξ¶_i Γ— LV
P(Y_i = k) = Ξ¦(Ο„_k - V_i) - Ξ¦(Ο„_{k-1} - V_i)

β‚ log P(Y_i = k) / β‚LV = (Ο†(Ο„_k - V_i) - Ο†(Ο„_{k-1} - V_i)) / P(Y_i = k) Γ— (-Ξ¶_i)

β‚ log L / β‚LV = Ξ£_i β‚ log P(Y_i | LV) / β‚LV
```

### β… κ²°λ΅ 
- **μ¬λ°”λ¥Έ κµ¬ν„**: κ° μ§€ν‘λ³„λ΅ `+=` μ—°μ‚°μλ¥Ό μ‚¬μ©ν•μ—¬ λ„μ  ν•©μ‚°
- **μ°¨μ› ν™•μΈ**: μµμΆ… κ²°κ³Όλ” `(n_draws,)` ν•νƒ
- **λ…Όλ¦¬ κ²€μ¦**: ν•λ‚μ LVκ°€ μ—¬λ¬ μ§€ν‘μ— μν–¥μ„ μ£Όλ―€λ΅, λ¨λ“  μ§€ν‘λ΅λ¶€ν„°μ κ·Έλλ””μ–ΈνΈλ¥Ό ν•©μ‚°ν•λ” κ²ƒμ΄ μ¬λ°”λ¦„

---

## β‘΅ μ„ νƒλ¨λΈ κ·Έλλ””μ–ΈνΈ λ¶€νΈ β… **μ¬λ°”λ¦„**

### κ²€ν†  λ‚΄μ©
> **μ§λ¬Έ**: `theta_chosen - sum(P_j * theta_j)` λ¶€νΈκ°€ μ¬λ°”λ¥Έκ°€?

### μ½”λ“ μ„μΉ
`src/analysis/hybrid_choice_model/iclv_models/gpu_gradient_batch.py`
- Line 2227-2477: `_compute_multinomial_logit_gradient_gpu()`

### κµ¬ν„ ν™•μΈ

```python
# Line 2394-2399: Gradient κ³„μ‚°
diff = y_batch_gpu[:, None, :, :] - P_batch  # (y - P)
weighted_diff = all_weights_gpu[:, :, None, None] * diff  # w_r Γ— (y - P)

# Line 2435-2453: Theta κ·Έλλ””μ–ΈνΈ
for (alt_name, lv_name), theta_val in theta_params.items():
    grad_theta = cp.zeros(n_individuals)
    lv_idx = lv_names.index(lv_name)
    
    for ind_idx in range(n_individuals):
        for cs_idx in range(n_choice_sets):
            for alt_idx in range(3):
                if (λ€μ•μ΄ alt_nameκ³Ό μΌμΉ):
                    lv_values = all_lvs_gpu[ind_idx, :, lv_idx]  # (R,)
                    grad_theta[ind_idx] += cp.sum(
                        weighted_diff[ind_idx, :, cs_idx, alt_idx] * lv_values
                    )
    
    gradients[f'theta_{alt_name}_{lv_name}'] = cp.asnumpy(grad_theta)
```

### μμ‹ κ²€μ¦

**Multinomial Logit κ·Έλλ””μ–ΈνΈ:**
```
V_j = asc_j + Ξ² Γ— X + ΞΈ_j Γ— LV + Ξ³_j Γ— LV Γ— X

P(j) = exp(V_j) / Ξ£_k exp(V_k)

log L = Ξ£_t log P(y_t)

β‚ log L / β‚ΞΈ_j = Ξ£_t [I(y_t = j) - P(j)] Γ— LV
                = Ξ£_t [(y_t = jμΌ λ• 1, μ•„λ‹λ©΄ 0) - P(j)] Γ— LV
```

**λ¶€νΈ κ²€μ¦:**
- β… μ„ νƒλ λ€μ• (y = j): `I(y = j) = 1`, `1 - P(j) > 0` (P(j) < 1μ΄λ―€λ΅)
- β… μ„ νƒ μ• λ λ€μ• (y β‰  j): `I(y = j) = 0`, `0 - P(j) < 0`
- β… `ΞΈ_j > 0`μ΄κ³  LVκ°€ ν¨μ©μ„ λ†’μ΄λ©΄, μ„ νƒλ λ€μ•μ ΞΈκ°€ ν΄ λ• κ·Έλλ””μ–ΈνΈλ” μ–‘μ

**μμ‹:**
```
μ„ νƒ μƒν™©: μΌλ°λ‹Ή μ„ νƒ (y = sugar)
LV = PI = 2.0
theta_sugar_PI = 0.5
theta_sugar_free_PI = 0.3
P(sugar) = 0.6
P(sugar_free) = 0.3

β‚ log L / β‚theta_sugar_PI = (1 - 0.6) Γ— 2.0 = 0.8 > 0  β… μ–‘μ (LVλ¥Ό λ” ν‚¤μ›λΌ!)
β‚ log L / β‚theta_sugar_free_PI = (0 - 0.3) Γ— 2.0 = -0.6 < 0  β… μμ (LVλ¥Ό μ¤„μ—¬λΌ!)
```

### β… κ²°λ΅ 
- **μ¬λ°”λ¥Έ κµ¬ν„**: `(y - P) Γ— LV` ν•νƒλ΅ κ³„μ‚°
- **λ¶€νΈ κ²€μ¦**: λ…Όλ¦¬μ μΌλ΅ μ¬λ°”λ¦„
- **κ°€μ¤‘ν‰κ· **: Importance weighting μ μ© (`w_r Γ— gradient`)

---

## β‘Ά κµ¬μ΅°λ¨λΈ μ¤μ°¨λ¶„μ‚° κ·Έλλ””μ–ΈνΈ β… **ν„μ¬λ” μ¶”μ • μ• ν•¨ (κ³ μ •)**

### κ²€ν†  λ‚΄μ©
> **μ§λ¬Έ**: κµ¬μ΅°λ¨λΈ μ¤μ°¨λ¶„μ‚° (ΟƒΒ²_PB)λ„ μ¶”μ •ν•λ”κ°€?

### μ½”λ“ μ„μΉ
`src/analysis/hybrid_choice_model/iclv_models/gpu_gradient_batch.py`
- Line 594-826: `compute_structural_gradient_batch_gpu()`

### κµ¬ν„ ν™•μΈ

```python
# Line 603: error_varianceλ” νλΌλ―Έν„°λ΅ λ°›μ§€λ§ κ³ μ •κ°’
def compute_structural_gradient_batch_gpu(
    ...
    error_variance: float = 1.0,  # κ³ μ •κ°’
    ...
):
```

**ν„μ¬ κµ¬ν„:**
- β… `gamma` (κ²½λ΅κ³„μ)λ§ μ¶”μ •
- β… `error_variance`λ” 1.0μΌλ΅ κ³ μ •
- β… μ¤μ°¨λ¶„μ‚°μ— λ€ν• κ·Έλλ””μ–ΈνΈ κ³„μ‚° μ—†μ

### λ§μ•½ μ¶”μ •ν•λ‹¤λ©΄?

**μμ‹:**
```
Ξ·_target = Ξ³ Γ— Ξ·_predictor + Ξµ,  Ξµ ~ N(0, ΟƒΒ²)

log L = -0.5 log(2Ο€ ΟƒΒ²) - 0.5 (Ξ·_target - Ξ³ Γ— Ξ·_predictor)Β² / ΟƒΒ²

β‚ log L / β‚ΟƒΒ² = -0.5 / ΟƒΒ² + 0.5 (Ξ·_target - Ξ³ Γ— Ξ·_predictor)Β² / Οƒβ΄
```

**μ²΄μΈλ£° μ μ©:**
```
β‚LL / β‚ΟƒΒ²_PB = Ξ£_r w_r Γ— β‚LL_r / β‚ΟƒΒ²_PB

μ—¬κΈ°μ„:
β‚LL_r / β‚ΟƒΒ²_PB = -0.5 / ΟƒΒ²_PB + 0.5 (PB - Ξ³ Γ— HC)Β² / (ΟƒΒ²_PB)Β²
```

**κµ¬ν„ μμ‹ (μ°Έκ³ μ©):**
```python
# μ”μ°¨
residual = target_gpu - gamma_gpu * pred_gpu  # (n_draws,)

# β‚LL/β‚ΟƒΒ² = Ξ£_r w_r Γ— [-0.5 / ΟƒΒ² + 0.5 Γ— residualΒ² / Οƒβ΄]
grad_sigma_sq = cp.sum(
    weights_gpu * (-0.5 / error_variance + 0.5 * (residual ** 2) / (error_variance ** 2))
)
```

### β… κ²°λ΅ 
- **ν„μ¬ κµ¬ν„**: μ¤μ°¨λ¶„μ‚° κ³ μ • (1.0)
- **κ·Έλλ””μ–ΈνΈ**: κ³„μ‚° μ• ν•¨ (μ¶”μ • λ€μƒ μ•„λ‹)
- **ν–¥ν›„ ν™•μ¥**: μ„ μμ‹λ€λ΅ κµ¬ν„ν•λ©΄ λ¨

---

## π“ μµμΆ… κ²€ν†  μ”μ•½

| ν•­λ© | ν„μ¬ κµ¬ν„ | μ¬λ°”λ¥Έμ§€ | λΉ„κ³  |
|------|----------|---------|------|
| **β‘  μΈ΅μ •λ¨λΈ μ°¨μ› ν•©μ‚°** | `grad_ll_wrt_lv += ...` | β… μ¬λ°”λ¦„ | 38κ° μ§€ν‘ λ„μ  ν•©μ‚° |
| **β‘΅ μ„ νƒλ¨λΈ λ¶€νΈ** | `(y - P) Γ— LV` | β… μ¬λ°”λ¦„ | Multinomial Logit μ •ν™• |
| **β‘Ά κµ¬μ΅°λ¨λΈ μ¤μ°¨λ¶„μ‚°** | κ³ μ • (1.0) | β… μ¬λ°”λ¦„ | μ¶”μ • μ• ν•¨ |

---

## π― ν•µμ‹¬ ν¬μΈνΈ

### 1. μΈ΅μ •λ¨λΈ κ·Έλλ””μ–ΈνΈ
```python
# β… μ¬λ°”λ¥Έ κµ¬ν„
grad_ll_wrt_lv = cp.zeros(n_draws)
for i in range(n_indicators):
    grad_ll_wrt_lv += Ξ¶_i Γ— (Y_i - Ξ¶_i Γ— LV) / ΟƒΒ²_i  # λ„μ  ν•©μ‚°
```

### 2. μ„ νƒλ¨λΈ κ·Έλλ””μ–ΈνΈ
```python
# β… μ¬λ°”λ¥Έ κµ¬ν„
diff = y - P  # (μ„ νƒ μ§€μ‹μ) - (μ„ νƒ ν™•λ¥ )
grad_theta = Ξ£_r w_r Γ— Ξ£_t diff Γ— LV  # κ°€μ¤‘ν‰κ· 
```

### 3. κµ¬μ΅°λ¨λΈ μ¤μ°¨λ¶„μ‚°
```python
# β… ν„μ¬λ” κ³ μ •
error_variance = 1.0  # μ¶”μ • μ• ν•¨

# λ§μ•½ μ¶”μ •ν•λ‹¤λ©΄:
# grad_sigma_sq = Ξ£_r w_r Γ— [-0.5/ΟƒΒ² + 0.5Γ—residualΒ²/Οƒβ΄]
```

---

## β… κ²°λ΅ 

**λ¨λ“  κ·Έλλ””μ–ΈνΈ κµ¬ν„μ΄ μ¬λ°”λ¦…λ‹λ‹¤!**

1. β… μΈ΅μ •λ¨λΈ: 38κ° μ§€ν‘μ κ·Έλλ””μ–ΈνΈλ¥Ό μ¬λ°”λ¥΄κ² ν•©μ‚°
2. β… μ„ νƒλ¨λΈ: Multinomial Logit κ·Έλλ””μ–ΈνΈ λ¶€νΈ μ •ν™•
3. β… κµ¬μ΅°λ¨λΈ: μ¤μ°¨λ¶„μ‚°μ€ κ³ μ • (μ¶”μ • μ• ν•¨)

μ¶”κ°€ μμ •μ΄ ν•„μ”ν•μ§€ μ•μµλ‹λ‹¤. ν„μ¬ κµ¬ν„μ€ μ΄λ΅ μ μΌλ΅ μ •ν™•ν•©λ‹λ‹¤.

