"""
RPL íš¨ìš©í•¨ìˆ˜ ê³„ìˆ˜ ìƒì„¸ ê²€í†  ë° ë¶„ì„
"""

import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import json
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats


def load_rpl_results():
    """RPL ë¶„ì„ ê²°ê³¼ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    
    print("=" * 80)
    print("RPL íš¨ìš©í•¨ìˆ˜ ê³„ìˆ˜ ìƒì„¸ ê²€í† ")
    print("=" * 80)
    print(f"ë¶„ì„ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    results_dir = Path("rpl_analysis_results")
    
    # ìµœì‹  íŒŒì¼ë“¤ ë¡œë“œ
    heterogeneity_file = results_dir / "heterogeneity_analysis_20250916_210256.csv"
    individual_coeffs_file = results_dir / "individual_coefficients_20250916_210256.csv"
    comprehensive_file = results_dir / "rpl_comprehensive_results_20250916_210256.json"
    
    print(f"\nğŸ“ ë¡œë“œí•  íŒŒì¼:")
    print(f"   âœ“ ì´ì§ˆì„± ë¶„ì„: {heterogeneity_file.name}")
    print(f"   âœ“ ê°œì¸ë³„ ê³„ìˆ˜: {individual_coeffs_file.name}")
    print(f"   âœ“ ì¢…í•© ê²°ê³¼: {comprehensive_file.name}")
    
    # ë°ì´í„° ë¡œë“œ
    heterogeneity_df = pd.read_csv(heterogeneity_file, index_col=0)
    individual_coeffs_df = pd.read_csv(individual_coeffs_file)
    
    with open(comprehensive_file, 'r', encoding='utf-8') as f:
        comprehensive_data = json.load(f)
    
    print(f"   âœ“ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(individual_coeffs_df)} ê°œì¸, {len(heterogeneity_df)} ë³€ìˆ˜")
    
    return heterogeneity_df, individual_coeffs_df, comprehensive_data


def analyze_coefficient_distributions(heterogeneity_df, individual_coeffs_df):
    """ê³„ìˆ˜ ë¶„í¬ë¥¼ ìƒì„¸ ë¶„ì„í•©ë‹ˆë‹¤."""
    
    print(f"\n" + "=" * 60)
    print("1. ê³„ìˆ˜ ë¶„í¬ ìƒì„¸ ë¶„ì„")
    print("=" * 60)
    
    coefficient_analysis = {}
    
    # ê° ë³€ìˆ˜ë³„ ê³„ìˆ˜ ë¶„í¬ ë¶„ì„
    for var_name in heterogeneity_df.index:
        if var_name in individual_coeffs_df.columns:
            coeffs = individual_coeffs_df[var_name].values
            
            # ê¸°ë³¸ í†µê³„
            analysis = {
                'mean': float(np.mean(coeffs)),
                'std': float(np.std(coeffs)),
                'median': float(np.median(coeffs)),
                'min': float(np.min(coeffs)),
                'max': float(np.max(coeffs)),
                'q25': float(np.percentile(coeffs, 25)),
                'q75': float(np.percentile(coeffs, 75)),
                'cv': float(np.std(coeffs) / abs(np.mean(coeffs))) if np.mean(coeffs) != 0 else 0,
                'range': float(np.max(coeffs) - np.min(coeffs)),
                'iqr': float(np.percentile(coeffs, 75) - np.percentile(coeffs, 25))
            }
            
            # ë¶„í¬ íŠ¹ì„±
            analysis['skewness'] = float(stats.skew(coeffs))
            analysis['kurtosis'] = float(stats.kurtosis(coeffs))
            
            # ì •ê·œì„± ê²€ì •
            _, p_value = stats.shapiro(coeffs[:50])  # ìƒ˜í”Œ í¬ê¸° ì œí•œ
            analysis['normality_p_value'] = float(p_value)
            analysis['is_normal'] = p_value > 0.05
            
            # ê³„ìˆ˜ ë¶€í˜¸ ë¶„ì„
            positive_pct = (coeffs > 0).mean() * 100
            negative_pct = (coeffs < 0).mean() * 100
            zero_pct = (coeffs == 0).mean() * 100
            
            analysis['sign_distribution'] = {
                'positive_pct': float(positive_pct),
                'negative_pct': float(negative_pct),
                'zero_pct': float(zero_pct)
            }
            
            # ê·¹ê°’ ë¶„ì„
            analysis['outliers'] = {
                'lower_fence': analysis['q25'] - 1.5 * analysis['iqr'],
                'upper_fence': analysis['q75'] + 1.5 * analysis['iqr'],
                'outlier_count': int(np.sum((coeffs < analysis['q25'] - 1.5 * analysis['iqr']) | 
                                          (coeffs > analysis['q75'] + 1.5 * analysis['iqr'])))
            }
            
            coefficient_analysis[var_name] = analysis
            
            print(f"\nğŸ“Š {var_name}:")
            print(f"   í‰ê· : {analysis['mean']:.4f} Â± {analysis['std']:.4f}")
            print(f"   ì¤‘ì•™ê°’: {analysis['median']:.4f}")
            print(f"   ë²”ìœ„: [{analysis['min']:.4f}, {analysis['max']:.4f}]")
            print(f"   ì‚¬ë¶„ìœ„ìˆ˜: [{analysis['q25']:.4f}, {analysis['q75']:.4f}]")
            print(f"   ë³€ì´ê³„ìˆ˜: {analysis['cv']:.3f}")
            print(f"   ì™œë„: {analysis['skewness']:.3f}, ì²¨ë„: {analysis['kurtosis']:.3f}")
            print(f"   ì •ê·œì„±: {'ì •ê·œë¶„í¬' if analysis['is_normal'] else 'ë¹„ì •ê·œë¶„í¬'} (p={analysis['normality_p_value']:.3f})")
            print(f"   ë¶€í˜¸ë¶„í¬: ì–‘ìˆ˜ {positive_pct:.1f}%, ìŒìˆ˜ {negative_pct:.1f}%, ì˜ {zero_pct:.1f}%")
            print(f"   ì´ìƒì¹˜: {analysis['outliers']['outlier_count']}ê°œ")
    
    return coefficient_analysis


def analyze_sem_effects(comprehensive_data):
    """SEM ìš”ì¸ì˜ ê³„ìˆ˜ ì˜í–¥ì„ ë¶„ì„í•©ë‹ˆë‹¤."""
    
    print(f"\n" + "=" * 60)
    print("2. SEM ìš”ì¸ì˜ ê³„ìˆ˜ ì˜í–¥ ë¶„ì„")
    print("=" * 60)
    
    rpl_distributions = comprehensive_data['rpl_distributions']
    
    sem_effect_analysis = {}
    
    print("ğŸ”— SEM ìš”ì¸ì´ DCE ê³„ìˆ˜ì— ë¯¸ì¹˜ëŠ” ì˜í–¥:")
    
    for dce_var, config in rpl_distributions.items():
        if config['sem_effects']:  # SEM íš¨ê³¼ê°€ ìˆëŠ” ê²½ìš°ë§Œ
            base_mean = config['mean']
            base_std = config['std']
            
            print(f"\nğŸ“‹ {dce_var}:")
            print(f"   ê¸°ë³¸ ê³„ìˆ˜: Î¼={base_mean:.4f}, Ïƒ={base_std:.4f}")
            print(f"   SEM ì˜í–¥:")
            
            effect_analysis = {
                'base_coefficient': base_mean,
                'base_std': base_std,
                'sem_effects': config['sem_effects'],
                'total_sem_effect': sum(abs(effect) for effect in config['sem_effects'].values()),
                'effect_interpretation': {}
            }
            
            for sem_factor, effect_size in config['sem_effects'].items():
                direction = "ì¦ê°€" if effect_size > 0 else "ê°ì†Œ"
                magnitude = "ê°•í•œ" if abs(effect_size) > 0.3 else "ì¤‘ê°„" if abs(effect_size) > 0.1 else "ì•½í•œ"
                
                interpretation = f"{sem_factor} 1 í‘œì¤€í¸ì°¨ ì¦ê°€ ì‹œ ê³„ìˆ˜ {abs(effect_size):.2f} {direction} ({magnitude} íš¨ê³¼)"
                effect_analysis['effect_interpretation'][sem_factor] = interpretation
                
                print(f"      â€¢ {sem_factor}: {effect_size:+.2f} ({interpretation})")
            
            # ê³„ìˆ˜ ë³€ë™ ë²”ìœ„ ê³„ì‚°
            max_positive_effect = sum(effect for effect in config['sem_effects'].values() if effect > 0)
            max_negative_effect = sum(effect for effect in config['sem_effects'].values() if effect < 0)
            
            theoretical_min = base_mean + max_negative_effect - 2*base_std
            theoretical_max = base_mean + max_positive_effect + 2*base_std
            
            effect_analysis['theoretical_range'] = {
                'min': theoretical_min,
                'max': theoretical_max,
                'range_width': theoretical_max - theoretical_min
            }
            
            print(f"   ì´ë¡ ì  ê³„ìˆ˜ ë²”ìœ„: [{theoretical_min:.4f}, {theoretical_max:.4f}]")
            print(f"   ì´ SEM íš¨ê³¼ í¬ê¸°: {effect_analysis['total_sem_effect']:.3f}")
            
            sem_effect_analysis[dce_var] = effect_analysis
    
    return sem_effect_analysis


def compare_coefficient_magnitudes(coefficient_analysis):
    """ê³„ìˆ˜ í¬ê¸°ë¥¼ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤."""
    
    print(f"\n" + "=" * 60)
    print("3. ê³„ìˆ˜ í¬ê¸° ë¹„êµ ë¶„ì„")
    print("=" * 60)
    
    # í‰ê·  ê³„ìˆ˜ í¬ê¸°ë¡œ ì •ë ¬
    sorted_coeffs = sorted(coefficient_analysis.items(), 
                          key=lambda x: abs(x[1]['mean']), reverse=True)
    
    print("ğŸ“Š ê³„ìˆ˜ í¬ê¸° ìˆœìœ„ (ì ˆëŒ“ê°’ ê¸°ì¤€):")
    
    magnitude_analysis = {}
    
    for rank, (var_name, analysis) in enumerate(sorted_coeffs, 1):
        mean_coeff = analysis['mean']
        abs_mean = abs(mean_coeff)
        direction = "ê¸ì •ì " if mean_coeff > 0 else "ë¶€ì •ì "
        
        # íš¨ê³¼ í¬ê¸° ë¶„ë¥˜
        if abs_mean > 1.0:
            effect_size = "ë§¤ìš° í°"
        elif abs_mean > 0.5:
            effect_size = "í°"
        elif abs_mean > 0.2:
            effect_size = "ì¤‘ê°„"
        elif abs_mean > 0.1:
            effect_size = "ì‘ì€"
        else:
            effect_size = "ë§¤ìš° ì‘ì€"
        
        magnitude_analysis[var_name] = {
            'rank': rank,
            'mean_coefficient': mean_coeff,
            'absolute_magnitude': abs_mean,
            'direction': direction,
            'effect_size': effect_size,
            'cv': analysis['cv']
        }
        
        print(f"   {rank}. {var_name}: {mean_coeff:+.4f} ({direction}, {effect_size})")
        print(f"      ë³€ì´ê³„ìˆ˜: {analysis['cv']:.3f}, ë²”ìœ„: [{analysis['min']:.3f}, {analysis['max']:.3f}]")
    
    # DCE vs SEM ìš”ì¸ ë¹„êµ
    dce_variables = ['sugar_free', 'health_label', 'price_normalized', 'sugar_health_interaction']
    sem_variables = ['perceived_benefit', 'nutrition_knowledge', 'perceived_price', 'health_concern']
    
    dce_magnitudes = [abs(magnitude_analysis[var]['mean_coefficient']) 
                     for var in dce_variables if var in magnitude_analysis]
    sem_magnitudes = [abs(magnitude_analysis[var]['mean_coefficient']) 
                     for var in sem_variables if var in magnitude_analysis]
    
    print(f"\nâš–ï¸ DCE vs SEM ìš”ì¸ ê³„ìˆ˜ í¬ê¸° ë¹„êµ:")
    print(f"   DCE ìš”ì¸ í‰ê·  í¬ê¸°: {np.mean(dce_magnitudes):.4f}")
    print(f"   SEM ìš”ì¸ í‰ê·  í¬ê¸°: {np.mean(sem_magnitudes):.4f}")
    print(f"   DCE/SEM ë¹„ìœ¨: {np.mean(dce_magnitudes)/np.mean(sem_magnitudes):.2f}")
    
    return magnitude_analysis


def analyze_coefficient_stability(coefficient_analysis):
    """ê³„ìˆ˜ ì•ˆì •ì„±ì„ ë¶„ì„í•©ë‹ˆë‹¤."""
    
    print(f"\n" + "=" * 60)
    print("4. ê³„ìˆ˜ ì•ˆì •ì„± ë¶„ì„")
    print("=" * 60)
    
    stability_analysis = {}
    
    print("ğŸ¯ ê³„ìˆ˜ ì•ˆì •ì„± í‰ê°€:")
    
    for var_name, analysis in coefficient_analysis.items():
        cv = analysis['cv']
        outlier_pct = analysis['outliers']['outlier_count'] / 300 * 100  # ì´ 300ëª…
        sign_consistency = max(analysis['sign_distribution']['positive_pct'],
                              analysis['sign_distribution']['negative_pct'])
        
        # ì•ˆì •ì„± ì ìˆ˜ ê³„ì‚° (0-100)
        cv_score = max(0, 100 - cv * 100)  # CVê°€ ë‚®ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
        outlier_score = max(0, 100 - outlier_pct * 2)  # ì´ìƒì¹˜ê°€ ì ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
        sign_score = sign_consistency  # ë¶€í˜¸ ì¼ê´€ì„±ì´ ë†’ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
        
        stability_score = (cv_score + outlier_score + sign_score) / 3
        
        # ì•ˆì •ì„± ë“±ê¸‰
        if stability_score >= 80:
            stability_grade = "ë§¤ìš° ì•ˆì •"
        elif stability_score >= 60:
            stability_grade = "ì•ˆì •"
        elif stability_score >= 40:
            stability_grade = "ë³´í†µ"
        elif stability_score >= 20:
            stability_grade = "ë¶ˆì•ˆì •"
        else:
            stability_grade = "ë§¤ìš° ë¶ˆì•ˆì •"
        
        stability_analysis[var_name] = {
            'cv': cv,
            'outlier_percentage': outlier_pct,
            'sign_consistency': sign_consistency,
            'stability_score': stability_score,
            'stability_grade': stability_grade
        }
        
        print(f"\n   ğŸ“Š {var_name}:")
        print(f"      ë³€ì´ê³„ìˆ˜: {cv:.3f}")
        print(f"      ì´ìƒì¹˜ ë¹„ìœ¨: {outlier_pct:.1f}%")
        print(f"      ë¶€í˜¸ ì¼ê´€ì„±: {sign_consistency:.1f}%")
        print(f"      ì•ˆì •ì„± ì ìˆ˜: {stability_score:.1f}/100 ({stability_grade})")
    
    # ì „ì²´ ì•ˆì •ì„± ìˆœìœ„
    sorted_stability = sorted(stability_analysis.items(), 
                            key=lambda x: x[1]['stability_score'], reverse=True)
    
    print(f"\nğŸ† ê³„ìˆ˜ ì•ˆì •ì„± ìˆœìœ„:")
    for rank, (var_name, analysis) in enumerate(sorted_stability, 1):
        print(f"   {rank}. {var_name}: {analysis['stability_score']:.1f}ì  ({analysis['stability_grade']})")
    
    return stability_analysis


def generate_coefficient_insights(coefficient_analysis, sem_effect_analysis, 
                                magnitude_analysis, stability_analysis):
    """ê³„ìˆ˜ ë¶„ì„ ê²°ê³¼ì—ì„œ ì£¼ìš” ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•©ë‹ˆë‹¤."""
    
    print(f"\n" + "=" * 60)
    print("5. ì£¼ìš” ì¸ì‚¬ì´íŠ¸ ë° í•´ì„")
    print("=" * 60)
    
    insights = {
        'key_findings': [],
        'coefficient_patterns': [],
        'sem_integration_effects': [],
        'practical_implications': [],
        'methodological_insights': []
    }
    
    # 1. ì£¼ìš” ë°œê²¬ì‚¬í•­
    print("ğŸ” ì£¼ìš” ë°œê²¬ì‚¬í•­:")
    
    # ê°€ì¥ í° ê³„ìˆ˜
    largest_coeff = max(magnitude_analysis.items(), key=lambda x: x[1]['absolute_magnitude'])
    insights['key_findings'].append(f"ê°€ì¥ í° íš¨ê³¼: {largest_coeff[0]} (ê³„ìˆ˜={largest_coeff[1]['mean_coefficient']:.4f})")
    print(f"   â€¢ {insights['key_findings'][-1]}")
    
    # ê°€ì¥ ì•ˆì •í•œ ê³„ìˆ˜
    most_stable = max(stability_analysis.items(), key=lambda x: x[1]['stability_score'])
    insights['key_findings'].append(f"ê°€ì¥ ì•ˆì •í•œ ê³„ìˆ˜: {most_stable[0]} (ì•ˆì •ì„±={most_stable[1]['stability_score']:.1f}ì )")
    print(f"   â€¢ {insights['key_findings'][-1]}")
    
    # ê°€ì¥ ì´ì§ˆì ì¸ ê³„ìˆ˜
    most_heterogeneous = max(coefficient_analysis.items(), key=lambda x: x[1]['cv'])
    insights['key_findings'].append(f"ê°€ì¥ ë†’ì€ ì´ì§ˆì„±: {most_heterogeneous[0]} (CV={most_heterogeneous[1]['cv']:.3f})")
    print(f"   â€¢ {insights['key_findings'][-1]}")
    
    # 2. ê³„ìˆ˜ íŒ¨í„´ ë¶„ì„
    print(f"\nğŸ“ˆ ê³„ìˆ˜ íŒ¨í„´:")
    
    # ì–‘ìˆ˜/ìŒìˆ˜ ê³„ìˆ˜ ë¶„í¬
    positive_coeffs = [var for var, analysis in coefficient_analysis.items() 
                      if analysis['mean'] > 0]
    negative_coeffs = [var for var, analysis in coefficient_analysis.items() 
                      if analysis['mean'] < 0]
    
    insights['coefficient_patterns'].append(f"ì–‘ìˆ˜ ê³„ìˆ˜: {len(positive_coeffs)}ê°œ ({', '.join(positive_coeffs)})")
    insights['coefficient_patterns'].append(f"ìŒìˆ˜ ê³„ìˆ˜: {len(negative_coeffs)}ê°œ ({', '.join(negative_coeffs)})")
    
    for pattern in insights['coefficient_patterns']:
        print(f"   â€¢ {pattern}")
    
    # 3. SEM í†µí•© íš¨ê³¼
    print(f"\nğŸ”— SEM í†µí•© íš¨ê³¼:")
    
    if sem_effect_analysis:
        # ê°€ì¥ ê°•í•œ SEM íš¨ê³¼
        strongest_sem_effect = max(
            [(dce_var, analysis['total_sem_effect']) for dce_var, analysis in sem_effect_analysis.items()],
            key=lambda x: x[1]
        )
        insights['sem_integration_effects'].append(
            f"ê°€ì¥ ê°•í•œ SEM íš¨ê³¼: {strongest_sem_effect[0]} (ì´ íš¨ê³¼={strongest_sem_effect[1]:.3f})"
        )
        
        # SEM íš¨ê³¼ê°€ ìˆëŠ” ë³€ìˆ˜ë“¤
        sem_affected_vars = list(sem_effect_analysis.keys())
        insights['sem_integration_effects'].append(
            f"SEM ì˜í–¥ ë°›ëŠ” ë³€ìˆ˜: {len(sem_affected_vars)}ê°œ ({', '.join(sem_affected_vars)})"
        )
        
        for effect in insights['sem_integration_effects']:
            print(f"   â€¢ {effect}")
    
    # 4. ì‹¤ìš©ì  ì‹œì‚¬ì 
    print(f"\nğŸ’¡ ì‹¤ìš©ì  ì‹œì‚¬ì :")
    
    # ë§ˆì¼€íŒ… ì „ëµ
    if 'sugar_free' in magnitude_analysis and magnitude_analysis['sugar_free']['absolute_magnitude'] > 0.5:
        insights['practical_implications'].append("ë¬´ì„¤íƒ• ì œí’ˆì´ ê°€ì¥ ê°•í•œ ì„ í˜¸ ìš”ì¸ â†’ ë¬´ì„¤íƒ• ë§ˆì¼€íŒ… ì§‘ì¤‘ í•„ìš”")
    
    if 'sugar_health_interaction' in magnitude_analysis and magnitude_analysis['sugar_health_interaction']['absolute_magnitude'] > 1.0:
        insights['practical_implications'].append("ë¬´ì„¤íƒ•Ã—ê±´ê°•ë¼ë²¨ ì‹œë„ˆì§€ íš¨ê³¼ ë§¤ìš° í¼ â†’ ì¡°í•© ì œí’ˆ ê°œë°œ ìš°ì„ ")
    
    # ê°œì¸í™” ì „ëµ
    high_cv_vars = [var for var, analysis in coefficient_analysis.items() if analysis['cv'] > 0.5]
    if high_cv_vars:
        insights['practical_implications'].append(f"ë†’ì€ ê°œì¸ì°¨ ë³€ìˆ˜({', '.join(high_cv_vars)}) â†’ ê°œì¸í™” ë§ˆì¼€íŒ… í•„ìˆ˜")
    
    for implication in insights['practical_implications']:
        print(f"   â€¢ {implication}")
    
    # 5. ë°©ë²•ë¡ ì  ì¸ì‚¬ì´íŠ¸
    print(f"\nğŸ”¬ ë°©ë²•ë¡ ì  ì¸ì‚¬ì´íŠ¸:")
    
    # ì •ê·œì„± ê²€ì • ê²°ê³¼
    normal_vars = [var for var, analysis in coefficient_analysis.items() if analysis['is_normal']]
    non_normal_vars = [var for var, analysis in coefficient_analysis.items() if not analysis['is_normal']]
    
    insights['methodological_insights'].append(f"ì •ê·œë¶„í¬ ë”°ë¥´ëŠ” ê³„ìˆ˜: {len(normal_vars)}ê°œ")
    insights['methodological_insights'].append(f"ë¹„ì •ê·œë¶„í¬ ê³„ìˆ˜: {len(non_normal_vars)}ê°œ â†’ ëŒ€ì•ˆ ë¶„í¬ ê³ ë ¤ í•„ìš”")
    
    # ì´ìƒì¹˜ ë¬¸ì œ
    high_outlier_vars = [var for var, analysis in stability_analysis.items() 
                        if analysis['outlier_percentage'] > 10]
    if high_outlier_vars:
        insights['methodological_insights'].append(f"ì´ìƒì¹˜ ë§ì€ ë³€ìˆ˜({', '.join(high_outlier_vars)}) â†’ ë¡œë²„ìŠ¤íŠ¸ ì¶”ì • í•„ìš”")
    
    for insight in insights['methodological_insights']:
        print(f"   â€¢ {insight}")
    
    return insights


def save_coefficient_analysis_results(coefficient_analysis, sem_effect_analysis, 
                                    magnitude_analysis, stability_analysis, insights):
    """ê³„ìˆ˜ ë¶„ì„ ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤."""
    
    print(f"\nğŸ’¾ ê³„ìˆ˜ ë¶„ì„ ê²°ê³¼ ì €ì¥:")
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # ì¢…í•© ê²°ê³¼ ì €ì¥
    comprehensive_results = {
        'analysis_info': {
            'analysis_type': 'RPL Coefficient Analysis',
            'analysis_date': datetime.now().isoformat(),
            'description': 'RPL íš¨ìš©í•¨ìˆ˜ ê³„ìˆ˜ ìƒì„¸ ê²€í†  ë° ë¶„ì„'
        },
        'coefficient_analysis': coefficient_analysis,
        'sem_effect_analysis': sem_effect_analysis,
        'magnitude_analysis': magnitude_analysis,
        'stability_analysis': stability_analysis,
        'insights': insights
    }
    
    # JSON íŒŒì¼ ì €ì¥
    json_file = Path(f"rpl_coefficient_analysis_{timestamp}.json")
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(comprehensive_results, f, ensure_ascii=False, indent=2)
    print(f"   âœ“ ì¢…í•© ë¶„ì„: {json_file}")
    
    # ê³„ìˆ˜ ìš”ì•½ CSV ì €ì¥
    summary_data = []
    for var_name, analysis in coefficient_analysis.items():
        summary_data.append({
            'variable': var_name,
            'mean': analysis['mean'],
            'std': analysis['std'],
            'cv': analysis['cv'],
            'min': analysis['min'],
            'max': analysis['max'],
            'stability_score': stability_analysis.get(var_name, {}).get('stability_score', 0),
            'magnitude_rank': magnitude_analysis.get(var_name, {}).get('rank', 0)
        })
    
    summary_df = pd.DataFrame(summary_data)
    summary_file = Path(f"rpl_coefficient_summary_{timestamp}.csv")
    summary_df.to_csv(summary_file, index=False, encoding='utf-8-sig')
    print(f"   âœ“ ê³„ìˆ˜ ìš”ì•½: {summary_file}")
    
    return json_file, summary_file


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # 1. ë°ì´í„° ë¡œë“œ
    heterogeneity_df, individual_coeffs_df, comprehensive_data = load_rpl_results()
    
    # 2. ê³„ìˆ˜ ë¶„í¬ ë¶„ì„
    coefficient_analysis = analyze_coefficient_distributions(heterogeneity_df, individual_coeffs_df)
    
    # 3. SEM íš¨ê³¼ ë¶„ì„
    sem_effect_analysis = analyze_sem_effects(comprehensive_data)
    
    # 4. ê³„ìˆ˜ í¬ê¸° ë¹„êµ
    magnitude_analysis = compare_coefficient_magnitudes(coefficient_analysis)
    
    # 5. ê³„ìˆ˜ ì•ˆì •ì„± ë¶„ì„
    stability_analysis = analyze_coefficient_stability(coefficient_analysis)
    
    # 6. ì¸ì‚¬ì´íŠ¸ ë„ì¶œ
    insights = generate_coefficient_insights(coefficient_analysis, sem_effect_analysis,
                                           magnitude_analysis, stability_analysis)
    
    # 7. ê²°ê³¼ ì €ì¥
    json_file, summary_file = save_coefficient_analysis_results(
        coefficient_analysis, sem_effect_analysis, magnitude_analysis, 
        stability_analysis, insights
    )
    
    # ìµœì¢… ìš”ì•½
    print(f"\n" + "=" * 80)
    print("RPL ê³„ìˆ˜ ë¶„ì„ ì™„ë£Œ")
    print("=" * 80)
    
    print(f"ğŸ“Š ë¶„ì„ ì™„ë£Œ:")
    print(f"   âœ… {len(coefficient_analysis)}ê°œ ë³€ìˆ˜ ê³„ìˆ˜ ë¶„í¬ ë¶„ì„")
    print(f"   âœ… {len(sem_effect_analysis)}ê°œ ë³€ìˆ˜ SEM íš¨ê³¼ ë¶„ì„")
    print(f"   âœ… ê³„ìˆ˜ í¬ê¸° ë° ì•ˆì •ì„± í‰ê°€")
    print(f"   âœ… ì‹¤ìš©ì  ì¸ì‚¬ì´íŠ¸ ë„ì¶œ")
    
    print(f"\nğŸ¯ í•µì‹¬ ê²°ê³¼:")
    for finding in insights['key_findings'][:3]:
        print(f"   â€¢ {finding}")
    
    print(f"\nğŸ“ ìƒì„± íŒŒì¼:")
    print(f"   ğŸ“„ {json_file}")
    print(f"   ğŸ“„ {summary_file}")
    
    return True


if __name__ == "__main__":
    main()
