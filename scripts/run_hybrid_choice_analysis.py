#!/usr/bin/env python3
"""
Hybrid Choice Model Analysis Script

í•˜ì´ë¸Œë¦¬ë“œ ì„ íƒ ëª¨ë¸ ë¶„ì„ì„ ì‹¤í–‰í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
DCEì™€ SEMì„ ê²°í•©í•œ ê³ ê¸‰ ì„ íƒëª¨ë¸ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import sys
import os
import argparse
import logging
import time
from pathlib import Path
from typing import Dict, List, Optional, Any

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append('.')
sys.path.append('src')

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/hybrid_choice_analysis.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

try:
    import pandas as pd
    import numpy as np
    BASIC_MODULES_AVAILABLE = True

    # í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“ˆ ì„í¬íŠ¸ ì‹œë„
    HYBRID_MODULE_AVAILABLE = False
    try:
        # ë‹¨ê³„ë³„ ì„í¬íŠ¸ ì‹œë„
        from src.analysis.hybrid_choice_model.choice_models.choice_model_factory import ChoiceModelFactory
        from src.analysis.hybrid_choice_model.choice_models.base_choice_model import ChoiceModelType
        from src.analysis.hybrid_choice_model.data_integration.hybrid_data_integrator import HybridDataIntegrator

        HYBRID_MODULE_AVAILABLE = True
        logger.info("í•˜ì´ë¸Œë¦¬ë“œ ì„ íƒ ëª¨ë¸ ëª¨ë“ˆì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")

    except ImportError as e:
        logger.warning(f"í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
        logger.info("ê¸°ë³¸ ë¶„ì„ ê¸°ëŠ¥ë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.")

except ImportError as e:
    logger.error(f"ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„í¬íŠ¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    BASIC_MODULES_AVAILABLE = False
    HYBRID_MODULE_AVAILABLE = False

# ê¸°ì¡´ ëª¨ë“ˆ ì„í¬íŠ¸ ì‹œë„
try:
    import config
    from src.utils.results_manager import ResultsManager
    EXISTING_MODULES_AVAILABLE = True
except ImportError:
    EXISTING_MODULES_AVAILABLE = False
    logger.warning("ê¸°ì¡´ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


def load_sample_data() -> tuple:
    """ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ"""
    logger.info("ìƒ˜í”Œ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
    
    try:
        # DCE ë°ì´í„° ë¡œë“œ ì‹œë„
        dce_data_path = Path("data/processed/dce")
        if dce_data_path.exists():
            dce_files = list(dce_data_path.glob("*.csv"))
            if dce_files:
                dce_data = pd.read_csv(dce_files[0])
                logger.info(f"DCE ë°ì´í„° ë¡œë“œë¨: {dce_files[0]} ({len(dce_data)}ê°œ ê´€ì¸¡ì¹˜)")
            else:
                # ì„ì‹œ DCE ë°ì´í„° ìƒì„±
                dce_data = create_sample_dce_data()
                logger.info("ì„ì‹œ DCE ë°ì´í„°ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
        else:
            dce_data = create_sample_dce_data()
            logger.info("ì„ì‹œ DCE ë°ì´í„°ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
        
        # SEM ë°ì´í„° ë¡œë“œ ì‹œë„ (ì‹¤ì œ ë°ì´í„° ìš°ì„ )
        sem_data = load_real_sem_data()
        
        return dce_data, sem_data
        
    except Exception as e:
        logger.warning(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}. ì„ì‹œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        return create_sample_dce_data(), create_sample_sem_data()


def create_sample_dce_data() -> pd.DataFrame:
    """ì‹¤ì œ ì„¤íƒ• ëŒ€ì²´ì¬ ì—°êµ¬ë¥¼ ìœ„í•œ DCE ë°ì´í„° ìƒì„±"""
    np.random.seed(42)

    # ì‹¤ì œ ì„¤ë¬¸ ì‘ë‹µì ìˆ˜ì— ë§ì¶¤ (301ëª…)
    n_individuals = 301
    n_choice_sets = 8  # ì¼ë°˜ì ì¸ DCE ì„¤ê³„
    n_alternatives = 3  # 3ê°œ ëŒ€ì•ˆ (2ê°œ ì œí’ˆ + 1ê°œ ì„ íƒì•ˆí•¨)

    data = []
    for individual in range(1, n_individuals + 1):
        for choice_set in range(1, n_choice_sets + 1):
            # ê° ì„ íƒì„¸íŠ¸ì—ì„œ í•˜ë‚˜ë§Œ ì„ íƒë˜ë„ë¡ ë³´ì¥
            chosen_alternative = np.random.randint(n_alternatives)

            for alternative in range(n_alternatives):
                choice = 1 if alternative == chosen_alternative else 0

                # ì„¤íƒ• ëŒ€ì²´ì¬ ì—°êµ¬ì— ë§ëŠ” ì†ì„±ë“¤
                if alternative < 2:  # ì œí’ˆ ëŒ€ì•ˆë“¤
                    data.append({
                        'individual_id': str(individual),
                        'choice_set': choice_set,
                        'alternative': alternative,
                        'choice': choice,
                        'price': np.random.choice([2000, 2500, 3000, 3500, 4000]),  # ê°€ê²© (ì›)
                        'sugar_content': np.random.choice([0, 25, 50, 75, 100]),    # ì„¤íƒ• í•¨ëŸ‰ (%)
                        'sweetener_type': np.random.choice([1, 2, 3, 4]),           # ê°ë¯¸ë£Œ ì¢…ë¥˜ (ìŠ¤í…Œë¹„ì•„, ì—ë¦¬ìŠ¤ë¦¬í†¨ ë“±)
                        'health_label': np.random.choice([0, 1]),                   # ê±´ê°• ë¼ë²¨ ìœ ë¬´
                        'brand': np.random.choice(['A', 'B', 'C', 'D']),           # ë¸Œëœë“œ
                        'package_size': np.random.choice([250, 500, 1000]),         # í¬ì¥ í¬ê¸° (g)
                        'organic': np.random.choice([0, 1])                         # ìœ ê¸°ë† ì—¬ë¶€
                    })
                else:  # "ì„ íƒì•ˆí•¨" ëŒ€ì•ˆ
                    data.append({
                        'individual_id': str(individual),
                        'choice_set': choice_set,
                        'alternative': alternative,
                        'choice': choice,
                        'price': 0,
                        'sugar_content': 0,
                        'sweetener_type': 0,
                        'health_label': 0,
                        'brand': 'None',
                        'package_size': 0,
                        'organic': 0
                    })

    return pd.DataFrame(data)


def load_real_sem_data() -> pd.DataFrame:
    """ì‹¤ì œ SEM ë°ì´í„° ë¡œë“œ ë° í†µí•©"""
    try:
        # ì‹¤ì œ ìš”ì¸ë³„ ë°ì´í„° ë¡œë“œ
        health_concern = pd.read_csv("data/processed/survey/health_concern.csv")
        perceived_benefit = pd.read_csv("data/processed/survey/perceived_benefit.csv")
        purchase_intention = pd.read_csv("data/processed/survey/purchase_intention.csv")
        perceived_price = pd.read_csv("data/processed/survey/perceived_price.csv")

        # ê°œì²´ IDë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©
        sem_data = health_concern.copy()
        sem_data = sem_data.rename(columns={'no': 'individual_id'})

        # ê±´ê°•ê´€ì‹¬ë„ ì»¬ëŸ¼ëª… ë³€ê²½
        health_cols = [f'q{i}' for i in range(6, 12)]
        for i, col in enumerate(health_cols):
            if col in sem_data.columns:
                sem_data = sem_data.rename(columns={col: f'health_concern_{i+1}'})

        # ì§€ê°ëœìœ ìµì„± ë³‘í•©
        benefit_data = perceived_benefit.rename(columns={'no': 'individual_id'})
        benefit_cols = [f'q{i}' for i in range(12, 18)]
        for i, col in enumerate(benefit_cols):
            if col in benefit_data.columns:
                benefit_data = benefit_data.rename(columns={col: f'perceived_benefit_{i+1}'})

        sem_data = sem_data.merge(benefit_data, on='individual_id', how='inner')

        # êµ¬ë§¤ì˜ë„ ë³‘í•©
        intention_data = purchase_intention.rename(columns={'no': 'individual_id'})
        intention_cols = [f'q{i}' for i in range(18, 21)]
        for i, col in enumerate(intention_cols):
            if col in intention_data.columns:
                intention_data = intention_data.rename(columns={col: f'purchase_intention_{i+1}'})

        sem_data = sem_data.merge(intention_data, on='individual_id', how='inner')

        # ì§€ê°ëœê°€ê²© ë³‘í•©
        price_data = perceived_price.rename(columns={'no': 'individual_id'})
        price_cols = [f'q{i}' for i in range(27, 30)]
        for i, col in enumerate(price_cols):
            if col in price_data.columns:
                price_data = price_data.rename(columns={col: f'perceived_price_{i+1}'})

        sem_data = sem_data.merge(price_data, on='individual_id', how='inner')

        # individual_idë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
        sem_data['individual_id'] = sem_data['individual_id'].astype(str)

        logger.info(f"ì‹¤ì œ SEM ë°ì´í„° ë¡œë“œ ì„±ê³µ: {len(sem_data)}ê°œ ê´€ì¸¡ì¹˜")
        return sem_data

    except Exception as e:
        logger.warning(f"ì‹¤ì œ SEM ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}. ìƒ˜í”Œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        return create_sample_sem_data()


def create_sample_sem_data() -> pd.DataFrame:
    """ìƒ˜í”Œ SEM ë°ì´í„° ìƒì„± (ì‹¤ì œ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ì‹œ ì‚¬ìš©)"""
    np.random.seed(42)
    n_individuals = 301  # ì‹¤ì œ ì‘ë‹µì ìˆ˜ì— ë§ì¶¤

    data = []
    for individual in range(1, n_individuals + 1):
        # ê±´ê°•ê´€ì‹¬ë„ (6ê°œ ë¬¸í•­)
        health_concern = np.random.normal(3.5, 0.8, 6)
        health_concern = np.clip(health_concern, 1, 5)

        # ì§€ê°ëœìœ ìµì„± (6ê°œ ë¬¸í•­)
        perceived_benefit = np.random.normal(3.8, 0.7, 6)
        perceived_benefit = np.clip(perceived_benefit, 1, 5)

        # êµ¬ë§¤ì˜ë„ (3ê°œ ë¬¸í•­)
        purchase_intention = np.random.normal(3.2, 0.9, 3)
        purchase_intention = np.clip(purchase_intention, 1, 5)

        # ì§€ê°ëœê°€ê²© (3ê°œ ë¬¸í•­)
        perceived_price = np.random.normal(3.0, 0.8, 3)
        perceived_price = np.clip(perceived_price, 1, 5)

        row = {'individual_id': str(individual)}

        # ê±´ê°•ê´€ì‹¬ë„ ë³€ìˆ˜
        for i in range(6):
            row[f'health_concern_{i+1}'] = round(health_concern[i])

        # ì§€ê°ëœìœ ìµì„± ë³€ìˆ˜
        for i in range(6):
            row[f'perceived_benefit_{i+1}'] = round(perceived_benefit[i])

        # êµ¬ë§¤ì˜ë„ ë³€ìˆ˜
        for i in range(3):
            row[f'purchase_intention_{i+1}'] = round(purchase_intention[i])

        # ì§€ê°ëœê°€ê²© ë³€ìˆ˜
        for i in range(3):
            row[f'perceived_price_{i+1}'] = round(perceived_price[i])

        data.append(row)

    return pd.DataFrame(data)


def estimate_choice_model(merged_data: pd.DataFrame, factor_scores: Dict[str, pd.Series],
                         model_type: str) -> Dict[str, float]:
    """ì‹¤ì œ ì„ íƒëª¨ë¸ ì¶”ì •"""
    try:
        from sklearn.linear_model import LogisticRegression
        from sklearn.preprocessing import StandardScaler
        import numpy as np

        # ì„ íƒ ë³€ìˆ˜ í™•ì¸
        if 'choice' not in merged_data.columns:
            logger.warning("ì„ íƒ ë³€ìˆ˜ê°€ ì—†ì–´ ê¸°ë³¸ ì í•©ë„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
            return {
                'log_likelihood': -np.inf,
                'aic': np.inf,
                'bic': np.inf,
                'rho_squared': 0.0,
                'n_parameters': 0,
                'n_observations': len(merged_data)
            }

        # ì„¤ëª…ë³€ìˆ˜ ì¤€ë¹„
        X_vars = []
        X_names = []

        # DCE ì†ì„± ë³€ìˆ˜ë“¤
        dce_vars = ['price', 'sugar_content', 'health_label']
        for var in dce_vars:
            if var in merged_data.columns:
                X_vars.append(merged_data[var].fillna(0))
                X_names.append(var)

        # ìš”ì¸ì ìˆ˜ ë³€ìˆ˜ë“¤
        for factor_name, scores in factor_scores.items():
            # merged_dataì˜ individual_idì™€ ë§¤ì¹­
            factor_series = merged_data['individual_id'].map(
                dict(zip(scores.index.astype(str), scores.values))
            ).fillna(scores.mean())
            X_vars.append(factor_series)
            X_names.append(factor_name)

        if not X_vars:
            logger.warning("ì„¤ëª…ë³€ìˆ˜ê°€ ì—†ì–´ ê¸°ë³¸ ì í•©ë„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
            return {
                'log_likelihood': -np.inf,
                'aic': np.inf,
                'bic': np.inf,
                'rho_squared': 0.0,
                'n_parameters': 0,
                'n_observations': len(merged_data)
            }

        # ë°ì´í„° ì¤€ë¹„
        X = np.column_stack(X_vars)
        y = merged_data['choice'].astype(int)

        # í‘œì¤€í™”
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)

        # ëª¨ë¸ ì¶”ì •
        if model_type == 'multinomial_logit':
            model = LogisticRegression(random_state=42, max_iter=1000)
        elif model_type == 'random_parameters_logit':
            # RPLì€ ë” ë³µì¡í•˜ë¯€ë¡œ ì¼ë‹¨ ê¸°ë³¸ ë¡œì§€ìŠ¤í‹±ìœ¼ë¡œ ê·¼ì‚¬
            model = LogisticRegression(random_state=42, max_iter=1000, C=0.1)
        else:
            model = LogisticRegression(random_state=42, max_iter=1000)

        model.fit(X_scaled, y)

        # íš¨ìš©í•¨ìˆ˜ ê³„ìˆ˜ ì¶”ì¶œ
        coefficients = model.coef_[0] if len(model.coef_.shape) > 1 else model.coef_
        intercept = model.intercept_[0] if hasattr(model.intercept_, '__len__') else model.intercept_

        # ê³„ìˆ˜ë¥¼ ì›ë˜ ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜ (í‘œì¤€í™” ì—­ë³€í™˜)
        original_coefficients = coefficients / scaler.scale_
        original_intercept = intercept - np.sum(coefficients * scaler.mean_ / scaler.scale_)

        # íš¨ìš©í•¨ìˆ˜ ê³„ìˆ˜ ë”•ì…”ë„ˆë¦¬ ìƒì„±
        utility_coefficients = {}
        for i, var_name in enumerate(X_names):
            utility_coefficients[var_name] = {
                'coefficient': round(float(original_coefficients[i]), 6),
                'standardized_coef': round(float(coefficients[i]), 6),
                't_stat': 'N/A',  # ê°„ë‹¨í•œ ëª¨ë¸ì—ì„œëŠ” t-í†µê³„ëŸ‰ ê³„ì‚° ìƒëµ
                'p_value': 'N/A'
            }

        utility_coefficients['intercept'] = {
            'coefficient': round(float(original_intercept), 6),
            'standardized_coef': round(float(intercept), 6),
            't_stat': 'N/A',
            'p_value': 'N/A'
        }

        # ì˜ˆì¸¡ í™•ë¥ 
        y_pred_proba = model.predict_proba(X_scaled)

        # Log-likelihood ê³„ì‚°
        log_likelihood = 0
        for i in range(len(y)):
            if y.iloc[i] < len(y_pred_proba[i]):
                prob = max(y_pred_proba[i][y.iloc[i]], 1e-15)  # 0 ë°©ì§€
                log_likelihood += np.log(prob)

        # ëª¨ë¸ ì í•©ë„ ì§€í‘œ ê³„ì‚°
        n_params = len(X_names) + 1  # ê³„ìˆ˜ + ì ˆí¸
        n_obs = len(y)

        aic = -2 * log_likelihood + 2 * n_params
        bic = -2 * log_likelihood + n_params * np.log(n_obs)

        # Null model log-likelihood (ì„ íƒ ë¹„ìœ¨ë§Œ ê³ ë ¤)
        choice_rate = y.mean()
        if choice_rate > 0 and choice_rate < 1:
            ll_null = n_obs * (choice_rate * np.log(choice_rate) +
                              (1 - choice_rate) * np.log(1 - choice_rate))
        else:
            ll_null = -n_obs * np.log(2)  # ê· ë“± í™•ë¥ 

        rho_squared = 1 - (log_likelihood / ll_null) if ll_null != 0 else 0

        logger.info(f"ëª¨ë¸ ì¶”ì • ì™„ë£Œ: LL={log_likelihood:.2f}, AIC={aic:.2f}, RhoÂ²={rho_squared:.3f}")

        return {
            'log_likelihood': round(log_likelihood, 2),
            'aic': round(aic, 2),
            'bic': round(bic, 2),
            'rho_squared': round(rho_squared, 3),
            'n_parameters': n_params,
            'n_observations': n_obs,
            'variables': X_names,
            'utility_function': {
                'coefficients': utility_coefficients,
                'model_type': model_type,
                'estimation_method': 'Maximum Likelihood (sklearn)',
                'standardization': 'Applied during estimation'
            }
        }

    except Exception as e:
        logger.error(f"ëª¨ë¸ ì¶”ì • ì¤‘ ì˜¤ë¥˜: {e}")
        return {
            'log_likelihood': -999.0,
            'aic': 9999.0,
            'bic': 9999.0,
            'rho_squared': 0.0,
            'n_parameters': 0,
            'n_observations': len(merged_data),
            'error': str(e)
        }


def calculate_reliability(sem_data: pd.DataFrame, factor_scores: Dict[str, pd.Series]) -> Dict[str, float]:
    """Cronbach's Alpha ì‹ ë¢°ë„ ê³„ì‚°"""
    reliability_estimates = {}

    try:
        for factor_name in factor_scores.keys():
            # í•´ë‹¹ ìš”ì¸ì˜ ê´€ì¸¡ë³€ìˆ˜ë“¤ ì°¾ê¸°
            factor_cols = [col for col in sem_data.columns if factor_name in col]

            if len(factor_cols) < 2:
                reliability_estimates[factor_name] = 0.0
                continue

            # í•´ë‹¹ ìš”ì¸ì˜ ë°ì´í„° ì¶”ì¶œ
            factor_data = sem_data[factor_cols].dropna()

            if len(factor_data) < 2:
                reliability_estimates[factor_name] = 0.0
                continue

            # Cronbach's Alpha ê³„ì‚°
            n_items = len(factor_cols)

            # ê° ë¬¸í•­ì˜ ë¶„ì‚°
            item_variances = factor_data.var(axis=0, ddof=1)
            total_item_variance = item_variances.sum()

            # ì „ì²´ ì ìˆ˜ì˜ ë¶„ì‚°
            total_scores = factor_data.sum(axis=1)
            total_variance = total_scores.var(ddof=1)

            if total_variance == 0:
                alpha = 0.0
            else:
                alpha = (n_items / (n_items - 1)) * (1 - total_item_variance / total_variance)

            reliability_estimates[factor_name] = round(max(0.0, alpha), 3)

    except Exception as e:
        logger.warning(f"ì‹ ë¢°ë„ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
        # ê¸°ë³¸ê°’ ë°˜í™˜
        for factor_name in factor_scores.keys():
            reliability_estimates[factor_name] = 0.7

    return reliability_estimates


def run_simple_hybrid_analysis(model_type: str, dce_data: pd.DataFrame,
                              sem_data: pd.DataFrame, **kwargs) -> Dict[str, Any]:
    """ê°„ë‹¨í•œ í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ ì‹¤í–‰"""
    logger.info(f"=== {model_type.upper()} ëª¨ë¸ ë¶„ì„ ì‹œì‘ (ê°„ë‹¨ ë²„ì „) ===")

    try:
        # ë°ì´í„° í†µí•©
        logger.info("1ë‹¨ê³„: ë°ì´í„° í†µí•©")

        # ê°œì²´ ID ì»¬ëŸ¼ í™•ì¸ ë° ìƒì„±
        individual_col = 'individual_id'
        if individual_col not in dce_data.columns:
            if 'id' in dce_data.columns:
                dce_data = dce_data.rename(columns={'id': individual_col})
            else:
                dce_data[individual_col] = dce_data.index.astype(str)

        if individual_col not in sem_data.columns:
            sem_data[individual_col] = sem_data.index.astype(str)

        # ë°ì´í„° ë³‘í•©
        merged_data = pd.merge(dce_data, sem_data, on=individual_col, how='inner')
        logger.info(f"ë°ì´í„° í†µí•© ì™„ë£Œ: {len(merged_data)}ê°œ ê´€ì¸¡ì¹˜")

        # ì¸¡ì •ëª¨ë¸ ë¶„ì„ (ê°„ë‹¨í•œ ìš”ì¸ì ìˆ˜ ê³„ì‚°)
        logger.info("2ë‹¨ê³„: ì¸¡ì •ëª¨ë¸ ë¶„ì„")
        factor_scores = {}

        # ê±´ê°•ê´€ì‹¬ë„ ìš”ì¸ì ìˆ˜
        health_cols = [col for col in sem_data.columns if 'health_concern' in col]
        if health_cols:
            factor_scores['health_concern'] = sem_data[health_cols].mean(axis=1)

        # ì§€ê°ëœìœ ìµì„± ìš”ì¸ì ìˆ˜
        benefit_cols = [col for col in sem_data.columns if 'perceived_benefit' in col]
        if benefit_cols:
            factor_scores['perceived_benefit'] = sem_data[benefit_cols].mean(axis=1)

        # êµ¬ë§¤ì˜ë„ ìš”ì¸ì ìˆ˜
        intention_cols = [col for col in sem_data.columns if 'purchase_intention' in col]
        if intention_cols:
            factor_scores['purchase_intention'] = sem_data[intention_cols].mean(axis=1)

        # ì§€ê°ëœê°€ê²© ìš”ì¸ì ìˆ˜
        price_cols = [col for col in sem_data.columns if 'perceived_price' in col]
        if price_cols:
            factor_scores['perceived_price'] = sem_data[price_cols].mean(axis=1)

        # ì‹ ë¢°ë„ ê³„ì‚°
        reliability_estimates = calculate_reliability(sem_data, factor_scores)

        logger.info(f"ìš”ì¸ì ìˆ˜ ê³„ì‚° ì™„ë£Œ: {len(factor_scores)}ê°œ ì ì¬ë³€ìˆ˜")

        # ì„ íƒëª¨ë¸ ë¶„ì„ (ì‹¤ì œ ë¡œì§“ ëª¨ë¸ ì¶”ì •)
        logger.info("3ë‹¨ê³„: ì„ íƒëª¨ë¸ ë¶„ì„")

        # ì‹¤ì œ ëª¨ë¸ ì¶”ì •
        model_fit_results = estimate_choice_model(merged_data, factor_scores, model_type)

        # ê¸°ë³¸ í†µê³„
        choice_stats = {
            'total_observations': len(merged_data),
            'unique_individuals': merged_data[individual_col].nunique(),
            'choice_distribution': merged_data.get('choice', pd.Series()).value_counts().to_dict() if 'choice' in merged_data.columns else {},
            'factor_scores_summary': {name: {'mean': scores.mean(), 'std': scores.std()}
                                    for name, scores in factor_scores.items()}
        }

        # ê²°ê³¼ êµ¬ì„±
        result = {
            'model_type': model_type,
            'success': True,
            'analysis_time': 5.0,  # ì„ì‹œê°’
            'data_summary': {
                'total_observations': len(merged_data),
                'dce_observations': len(dce_data),
                'sem_observations': len(sem_data),
                'common_individuals': merged_data[individual_col].nunique()
            },
            'measurement_model': {
                'n_factors': len(factor_scores),
                'factor_names': list(factor_scores.keys()),
                'reliability_estimates': reliability_estimates
            },
            'choice_model': choice_stats,
            'model_fit': model_fit_results
        }

        logger.info(f"{model_type} ëª¨ë¸ ë¶„ì„ ì„±ê³µ!")
        logger.info(f"ë°ì´í„° ìš”ì•½: {result['data_summary']}")
        logger.info(f"ëª¨ë¸ ì í•©ë„: {result['model_fit']}")

        # ê²°ê³¼ ì €ì¥
        if kwargs.get('save_results', True):
            save_hybrid_results(result, model_type)

        return result

    except Exception as e:
        logger.error(f"{model_type} ëª¨ë¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        return {
            'model_type': model_type,
            'success': False,
            'error': str(e)
        }


def save_hybrid_results(result: Dict[str, Any], model_type: str):
    """í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ ê²°ê³¼ ì €ì¥"""
    try:
        import json
        from datetime import datetime

        # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
        results_dir = Path("results/current/hybrid_choice_model")
        results_dir.mkdir(parents=True, exist_ok=True)

        # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # JSON ê²°ê³¼ ì €ì¥
        json_file = results_dir / f"hybrid_analysis_{model_type}_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False, default=str)

        # ìš”ì•½ í…ìŠ¤íŠ¸ ì €ì¥
        summary_file = results_dir / f"hybrid_summary_{model_type}_{timestamp}.txt"
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(f"í•˜ì´ë¸Œë¦¬ë“œ ì„ íƒ ëª¨ë¸ ë¶„ì„ ê²°ê³¼\n")
            f.write(f"=" * 50 + "\n")
            f.write(f"ëª¨ë¸ íƒ€ì…: {result['model_type']}\n")
            f.write(f"ë¶„ì„ ì‹œê°„: {result['analysis_time']:.2f}ì´ˆ\n")
            f.write(f"ë¶„ì„ ì„±ê³µ: {'ì˜ˆ' if result['success'] else 'ì•„ë‹ˆì˜¤'}\n\n")

            f.write(f"ë°ì´í„° ìš”ì•½:\n")
            f.write(f"- ì´ ê´€ì¸¡ì¹˜: {result['data_summary']['total_observations']}\n")
            f.write(f"- DCE ê´€ì¸¡ì¹˜: {result['data_summary']['dce_observations']}\n")
            f.write(f"- SEM ê´€ì¸¡ì¹˜: {result['data_summary']['sem_observations']}\n")
            f.write(f"- ê³µí†µ ê°œì²´: {result['data_summary']['common_individuals']}\n\n")

            f.write(f"ì¸¡ì •ëª¨ë¸ ê²°ê³¼:\n")
            f.write(f"- ìš”ì¸ ìˆ˜: {result['measurement_model']['n_factors']}\n")
            f.write(f"- ìš”ì¸ëª…: {', '.join(result['measurement_model']['factor_names'])}\n\n")

            f.write(f"ëª¨ë¸ ì í•©ë„:\n")
            for key, value in result['model_fit'].items():
                if key != 'utility_function':
                    f.write(f"- {key}: {value}\n")

            # íš¨ìš©í•¨ìˆ˜ ê³„ìˆ˜ ì¶”ê°€
            if 'utility_function' in result['model_fit']:
                f.write(f"\níš¨ìš©í•¨ìˆ˜ ê³„ìˆ˜:\n")
                coeffs = result['model_fit']['utility_function']['coefficients']
                for var_name, coef_info in coeffs.items():
                    f.write(f"- {var_name}: {coef_info['coefficient']}\n")

        # CSV í˜•íƒœë¡œ ì£¼ìš” ê²°ê³¼ ì €ì¥
        csv_file = results_dir / f"hybrid_results_{model_type}_{timestamp}.csv"
        results_df = pd.DataFrame([{
            'model_type': result['model_type'],
            'analysis_time': result['analysis_time'],
            'success': result['success'],
            'total_observations': result['data_summary']['total_observations'],
            'dce_observations': result['data_summary']['dce_observations'],
            'sem_observations': result['data_summary']['sem_observations'],
            'common_individuals': result['data_summary']['common_individuals'],
            'n_factors': result['measurement_model']['n_factors'],
            'log_likelihood': result['model_fit']['log_likelihood'],
            'aic': result['model_fit']['aic'],
            'bic': result['model_fit']['bic'],
            'rho_squared': result['model_fit']['rho_squared']
        }])
        results_df.to_csv(csv_file, index=False, encoding='utf-8-sig')

        logger.info(f"ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
        logger.info(f"  - JSON: {json_file}")
        logger.info(f"  - ìš”ì•½: {summary_file}")
        logger.info(f"  - CSV: {csv_file}")

    except Exception as e:
        logger.error(f"ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")


def run_single_model_analysis(model_type: str, dce_data: pd.DataFrame,
                             sem_data: pd.DataFrame, **kwargs) -> Optional[Any]:
    """ë‹¨ì¼ ëª¨ë¸ ë¶„ì„ ì‹¤í–‰"""
    if HYBRID_MODULE_AVAILABLE:
        # ì™„ì „í•œ í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ ì‹œë„
        logger.info("ì™„ì „í•œ í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ì„ ì‹œë„í•©ë‹ˆë‹¤...")
        try:
            # ì—¬ê¸°ì— ì™„ì „í•œ ë¶„ì„ ì½”ë“œ ì¶”ê°€ ê°€ëŠ¥
            pass
        except Exception as e:
            logger.warning(f"ì™„ì „í•œ ë¶„ì„ ì‹¤íŒ¨: {e}")

    # ê°„ë‹¨í•œ ë¶„ì„ ì‹¤í–‰
    return run_simple_hybrid_analysis(model_type, dce_data, sem_data, **kwargs)


def run_model_comparison_analysis(model_types: List[str], dce_data: pd.DataFrame, 
                                sem_data: pd.DataFrame, **kwargs) -> Dict[str, Any]:
    """ëª¨ë¸ ë¹„êµ ë¶„ì„ ì‹¤í–‰"""
    logger.info("=== ëª¨ë¸ ë¹„êµ ë¶„ì„ ì‹œì‘ ===")
    
    results = {}
    
    for model_type in model_types:
        result = run_single_model_analysis(model_type, dce_data, sem_data, **kwargs)
        results[model_type] = result
    
    # ë¹„êµ ê²°ê³¼ ìš”ì•½
    logger.info("\n=== ëª¨ë¸ ë¹„êµ ê²°ê³¼ ìš”ì•½ ===")
    for model_type, result in results.items():
        if result and result.success:
            summary = result.get_summary()
            logger.info(f"{model_type}:")
            logger.info(f"  - ë¶„ì„ ì‹œê°„: {summary['analysis_time']:.2f}ì´ˆ")
            logger.info(f"  - ëª¨ë¸ ì í•©ë„: {summary.get('model_fit', {})}")
        else:
            logger.info(f"{model_type}: ë¶„ì„ ì‹¤íŒ¨")
    
    return results


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="í•˜ì´ë¸Œë¦¬ë“œ ì„ íƒ ëª¨ë¸ ë¶„ì„")
    
    # ê¸°ë³¸ ì˜µì…˜
    parser.add_argument('--model', type=str, default='multinomial_logit',
                       help='ì„ íƒëª¨ë¸ íƒ€ì… (ê¸°ë³¸ê°’: multinomial_logit)')
    parser.add_argument('--compare', action='store_true',
                       help='ì—¬ëŸ¬ ëª¨ë¸ ë¹„êµ ë¶„ì„')
    parser.add_argument('--models', nargs='+', 
                       default=['multinomial_logit', 'random_parameters_logit'],
                       help='ë¹„êµí•  ëª¨ë¸ ëª©ë¡')
    
    # ë°ì´í„° ì˜µì…˜
    parser.add_argument('--dce-data', type=str, help='DCE ë°ì´í„° íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--sem-data', type=str, help='SEM ë°ì´í„° íŒŒì¼ ê²½ë¡œ')
    
    # ëª¨ë¸ ì˜µì…˜
    parser.add_argument('--random-parameters', nargs='+', 
                       default=['price', 'sugar_content'],
                       help='í™•ë¥ ëª¨ìˆ˜ ëª©ë¡ (RPLìš©)')
    parser.add_argument('--simulation-draws', type=int, default=1000,
                       help='ì‹œë®¬ë ˆì´ì…˜ ë“œë¡œìš° ìˆ˜')
    
    # ì¶œë ¥ ì˜µì…˜
    parser.add_argument('--save-results', action='store_true', default=True,
                       help='ê²°ê³¼ ì €ì¥ ì—¬ë¶€')
    parser.add_argument('--verbose', action='store_true',
                       help='ìƒì„¸ ì¶œë ¥')
    
    # ì •ë³´ ì˜µì…˜
    parser.add_argument('--list-models', action='store_true',
                       help='ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¶œë ¥')
    
    args = parser.parse_args()
    
    # ë¡œê¹… ë ˆë²¨ ì„¤ì •
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # ê¸°ë³¸ ëª¨ë“ˆ ê°€ìš©ì„± í™•ì¸
    if not BASIC_MODULES_AVAILABLE:
        logger.error("ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return 1
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¶œë ¥
    if args.list_models:
        print("ğŸ¯ ì‚¬ìš© ê°€ëŠ¥í•œ í•˜ì´ë¸Œë¦¬ë“œ ì„ íƒ ëª¨ë¸:")
        print("-" * 50)
        models_info = {
            "multinomial_logit": "ë‹¤í•­ë¡œì§“ ëª¨ë¸ (ê¸°ë³¸)",
            "random_parameters_logit": "í™•ë¥ ëª¨ìˆ˜ ë¡œì§“ ëª¨ë¸ (ê°œì²´ ì´ì§ˆì„±)",
            "mixed_logit": "í˜¼í•©ë¡œì§“ ëª¨ë¸ (ì ì¬í´ë˜ìŠ¤)",
            "nested_logit": "ì¤‘ì²©ë¡œì§“ ëª¨ë¸ (ê³„ì¸µêµ¬ì¡°)",
            "multinomial_probit": "ë‹¤í•­í”„ë¡œë¹— ëª¨ë¸ (ì •ê·œë¶„í¬)"
        }

        for i, (model, description) in enumerate(models_info.items(), 1):
            status = "âœ…" if HYBRID_MODULE_AVAILABLE else "âš ï¸"
            print(f"  {i}. {status} {model:<25} - {description}")

        if not HYBRID_MODULE_AVAILABLE:
            print("\nâš ï¸  ì™„ì „í•œ í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("   ê¸°ë³¸ ë¶„ì„ ê¸°ëŠ¥ë§Œ ì œê³µë©ë‹ˆë‹¤.")

        return 0
    
    try:
        # ë°ì´í„° ë¡œë“œ
        if args.dce_data and args.sem_data:
            dce_data = pd.read_csv(args.dce_data)
            sem_data = pd.read_csv(args.sem_data)
            logger.info(f"ì‚¬ìš©ì ì§€ì • ë°ì´í„° ë¡œë“œë¨: DCE({len(dce_data)}), SEM({len(sem_data)})")
        else:
            dce_data, sem_data = load_sample_data()
        
        # ë¶„ì„ ì‹¤í–‰
        if args.compare:
            # ëª¨ë¸ ë¹„êµ ë¶„ì„
            results = run_model_comparison_analysis(
                args.models, dce_data, sem_data,
                random_parameters=args.random_parameters,
                simulation_draws=args.simulation_draws,
                save_results=args.save_results
            )
        else:
            # ë‹¨ì¼ ëª¨ë¸ ë¶„ì„
            result = run_single_model_analysis(
                args.model, dce_data, sem_data,
                random_parameters=args.random_parameters,
                simulation_draws=args.simulation_draws,
                save_results=args.save_results
            )
        
        logger.info("í•˜ì´ë¸Œë¦¬ë“œ ì„ íƒ ëª¨ë¸ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return 0
        
    except Exception as e:
        logger.error(f"ë¶„ì„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return 1


if __name__ == "__main__":
    exit(main())
