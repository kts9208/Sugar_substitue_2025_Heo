"""
GPU ê°€ì† ë‹¤ì¤‘ ì ì¬ë³€ìˆ˜ ICLV ëª¨ë¸ í…ŒìŠ¤íŠ¸

CuPyë¥¼ ì‚¬ìš©í•˜ì—¬ GPUì—ì„œ ëª¨ë¸ì„ ì¶”ì •í•©ë‹ˆë‹¤.
"""

import sys
from pathlib import Path
import pandas as pd
import multiprocessing

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.analysis.hybrid_choice_model.iclv_models.multi_latent_config import create_default_multi_lv_config
from src.analysis.hybrid_choice_model.iclv_models.gpu_multi_latent_estimator import GPUMultiLatentSimultaneousEstimator
from src.analysis.hybrid_choice_model.iclv_models.gpu_measurement_equations import GPU_AVAILABLE


def main():
    print("=" * 70)
    print("ğŸš€ GPU ê°€ì† ë‹¤ì¤‘ ì ì¬ë³€ìˆ˜ ICLV ë™ì‹œì¶”ì • (5ê°œ ì ì¬ë³€ìˆ˜)")
    print("=" * 70)
    
    # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    if GPU_AVAILABLE:
        print("\nâœ… GPU ì‚¬ìš© ê°€ëŠ¥!")
        try:
            import cupy as cp
            print(f"   CuPy ë²„ì „: {cp.__version__}")
            print(f"   CUDA ì‚¬ìš© ê°€ëŠ¥: {cp.cuda.is_available()}")
            if cp.cuda.is_available():
                print(f"   GPU ê°œìˆ˜: {cp.cuda.runtime.getDeviceCount()}")
                cp.cuda.Device(0).use()
                props = cp.cuda.runtime.getDeviceProperties(0)
                print(f"   GPU ì´ë¦„: {props['name'].decode()}")
                print(f"   GPU ë©”ëª¨ë¦¬: {props['totalGlobalMem'] / 1024**3:.2f} GB")
        except Exception as e:
            print(f"   âš ï¸ GPU ì •ë³´ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
    else:
        print("\nâš ï¸ CuPy ë¯¸ì„¤ì¹˜ - CPU ëª¨ë“œë¡œ ì‘ë™")
    
    # 1. ë°ì´í„° ë¡œë“œ
    print("\n" + "=" * 70)
    print("1. ë°ì´í„° ë¡œë“œ")
    print("=" * 70)
    
    data_path = project_root / 'data' / 'processed' / 'iclv' / 'integrated_data.csv'
    print(f"ë°ì´í„° ê²½ë¡œ: {data_path}")
    
    data = pd.read_csv(data_path)
    n_individuals = data['respondent_id'].nunique()
    
    print(f"âœ“ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
    print(f"  - ê°œì¸ ìˆ˜: {n_individuals:,}")
    print(f"  - ê´€ì¸¡ì¹˜ ìˆ˜: {len(data):,}")
    print(f"  - ê°œì¸ë‹¹ ì„ íƒ ìƒí™©: {len(data) / n_individuals:.1f}")
    
    # 2. ì„¤ì •
    print("\n" + "=" * 70)
    print("2. ëª¨ë¸ ì„¤ì •")
    print("=" * 70)
    
    n_cpus = multiprocessing.cpu_count()
    use_parallel = True
    n_cores = max(1, n_cpus - 1)
    
    print(f"CPU ì½”ì–´: {n_cpus}ê°œ")
    print(f"ë³‘ë ¬ì²˜ë¦¬: {use_parallel}")
    print(f"ì‚¬ìš© ì½”ì–´: {n_cores}ê°œ")
    
    # GPU ì‚¬ìš© ì—¬ë¶€ ì„ íƒ
    use_gpu = GPU_AVAILABLE
    if use_gpu:
        print(f"GPU ê°€ì†: âœ… í™œì„±í™”")
    else:
        print(f"GPU ê°€ì†: âŒ ë¹„í™œì„±í™” (CuPy ë¯¸ì„¤ì¹˜)")
    
    config = create_default_multi_lv_config(
        n_draws=100,
        max_iterations=1000,
        use_parallel=use_parallel,
        n_cores=n_cores
    )
    
    print(f"âœ“ ì„¤ì • ì™„ë£Œ")
    print(f"  - Halton draws: {config.estimation.n_draws}")
    print(f"  - ìµœëŒ€ ë°˜ë³µ: {config.estimation.max_iterations}")
    print(f"  - ìµœì í™” ë°©ë²•: {config.estimation.optimizer}")
    
    # 3. ëª¨ë¸ ìƒì„±
    print("\n" + "=" * 70)
    print("3. GPU ëª¨ë¸ ìƒì„±")
    print("=" * 70)
    
    estimator = GPUMultiLatentSimultaneousEstimator(config, data, use_gpu=use_gpu)
    
    print(f"âœ“ ëª¨ë¸ ìƒì„± ì™„ë£Œ")
    
    # 4. ì¶”ì •
    print("\n" + "=" * 70)
    print("4. ëª¨ë¸ ì¶”ì • ì‹œì‘")
    print("=" * 70)
    
    results = estimator.estimate()
    
    # 5. ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 70)
    print("5. ì¶”ì • ê²°ê³¼")
    print("=" * 70)
    
    print(f"\nìµœì¢… ë¡œê·¸ìš°ë„: {results['log_likelihood']:.4f}")
    print(f"ìˆ˜ë ´ ì—¬ë¶€: {results['success']}")
    print(f"ë°˜ë³µ íšŸìˆ˜: {results['n_iterations']}")
    print(f"ì†Œìš” ì‹œê°„: {results['time_elapsed']/60:.1f}ë¶„")
    
    # íŒŒë¼ë¯¸í„° ìš”ì•½
    print("\n" + "-" * 70)
    print("íŒŒë¼ë¯¸í„° ìš”ì•½")
    print("-" * 70)
    
    params = results['params']
    
    # êµ¬ì¡°ëª¨ë¸ íŒŒë¼ë¯¸í„°
    print("\n[êµ¬ì¡°ëª¨ë¸]")
    print(f"  gamma_lv (ì ì¬ë³€ìˆ˜ ê³„ìˆ˜): {params['structural']['gamma_lv']}")
    print(f"  gamma_x (ê³µë³€ëŸ‰ ê³„ìˆ˜): {params['structural']['gamma_x']}")
    
    # ì„ íƒëª¨ë¸ íŒŒë¼ë¯¸í„°
    print("\n[ì„ íƒëª¨ë¸]")
    print(f"  intercept: {params['choice']['intercept']:.4f}")
    print(f"  beta (ì†ì„± ê³„ìˆ˜): {params['choice']['beta']}")
    print(f"  lambda (ì ì¬ë³€ìˆ˜ ê³„ìˆ˜): {params['choice']['lambda']:.4f}")
    
    # ì¸¡ì •ëª¨ë¸ íŒŒë¼ë¯¸í„° (ìš”ì•½)
    print("\n[ì¸¡ì •ëª¨ë¸]")
    for lv_name, lv_params in params['measurement'].items():
        print(f"  {lv_name}:")
        print(f"    zeta (ìš”ì¸ì ì¬ëŸ‰): {lv_params['zeta'][:3]}... (ì²˜ìŒ 3ê°œ)")
        print(f"    tau (ì„ê³„ê°’): {lv_params['tau'].shape}")
    
    print("\n" + "=" * 70)
    print("âœ… ì¶”ì • ì™„ë£Œ!")
    print("=" * 70)
    
    # GPU vs CPU ì„±ëŠ¥ ë¹„êµ ì •ë³´
    if use_gpu:
        print("\nğŸ’¡ GPU ê°€ì†ì´ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("   ì¸¡ì •ëª¨ë¸ì˜ ì •ê·œë¶„í¬ CDF ê³„ì‚°ì´ GPUì—ì„œ ìˆ˜í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("\nğŸ’¡ CPU ëª¨ë“œë¡œ ì‹¤í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("   GPU ê°€ì†ì„ ì‚¬ìš©í•˜ë ¤ë©´ CuPyë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”:")
        print("   pip install cupy-cuda12x")


if __name__ == '__main__':
    main()

