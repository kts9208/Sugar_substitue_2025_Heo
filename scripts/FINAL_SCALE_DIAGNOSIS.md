# 🎯 잠재변수 비유의성 원인: 스케일 불일치 확인

## ✅ 핵심 발견: 스케일 문제가 맞습니다!

---

## 📊 문제 확인

### 1. 가격 변수의 스케일이 비정상적으로 작음

**원본 DCE 데이터:**
- 가격: 2000원, 2500원, 3000원
- 표준편차: ~408원

**integrated_data_cleaned.csv:**
- 가격: 2, 2.5, 3 (1000으로 나눔)
- 표준편차: **0.408**

**문제:**
- 가격을 1000으로 나눠서 스케일이 매우 작아짐
- 다른 변수들과 스케일 불균형 발생

---

## 📈 스케일 비교

| 변수 | 표준편차 | 범위 | 스케일 타입 |
|------|---------|------|------------|
| **price** | **0.408** | [2, 3] | ⚠️ 매우 작음 |
| health_label | 0.500 | [0, 1] | 이진형 |
| **purchase_intention** | **0.822** | [-2.27, 1.43] | 연속형 |
| perceived_price | 0.608 | [-1.97, 1.26] | 연속형 |
| nutrition_knowledge | 0.969 | [-3.01, 2.24] | 연속형 |

**스케일 비율:**
- purchase_intention / price = **2.0배**
- nutrition_knowledge / price = **2.4배**

---

## 🔍 효과 크기 분석

### 추정된 계수

| 파라미터 | 계수 | 표준오차 | 스케일 | 효과 크기 (β×σ) |
|---------|------|---------|--------|----------------|
| β_price | -1.616 | 0.111 | 0.408 | **-0.660** |
| β_health_label | 1.273 | 0.095 | 0.500 | **+0.636** |
| λ_purchase_intention | -0.018 | 0.317 | 0.822 | **-0.015** |

### 계수 크기 비율

```
|β_price| / |λ_main| = 87.8배
```

**해석:**
- 가격의 스케일이 작아서 → 계수가 크게 추정됨 (-1.616)
- 잠재점수의 스케일이 커서 → 계수가 작게 추정됨 (-0.018)
- **스케일 차이를 계수가 보정하려고 하지만 불완전함**

---

## ⚠️ 문제의 영향

### 1. 추정 불안정성

스케일 차이가 크면:
- 최적화 알고리즘이 불안정해짐
- 작은 계수의 표준오차가 커짐
- 수치적 정밀도 문제 발생

### 2. 잠재변수 계수의 큰 표준오차

| 파라미터 | 계수 | 표준오차 | SE/|계수| 비율 |
|---------|------|---------|--------------|
| β_price | -1.616 | 0.111 | **0.07** (7%) |
| λ_main | -0.018 | 0.317 | **17.2** (1720%!) |

**문제:**
- 잠재변수 계수의 표준오차가 계수 자체보다 17배 큼
- 이는 추정이 매우 불안정함을 의미
- 스케일 불일치로 인한 수치적 문제

---

## 💡 해결 방안

### 방안 1: 가격 변수를 원래 스케일로 복원 ⭐ (권장)

**현재:**
```python
price = [2, 2.5, 3]  # 표준편차 0.408
```

**수정:**
```python
price = [2000, 2500, 3000]  # 표준편차 408
```

**효과:**
- 가격 표준편차: 0.408 → 408 (1000배)
- 가격 계수: -1.616 → -0.00162 (1/1000배)
- 효과 크기는 동일: -1.616 × 0.408 = -0.00162 × 408 = -0.660

**장점:**
- 모든 변수가 비슷한 스케일 (수백~수천)
- 수치적 안정성 향상
- 계수 해석이 직관적 (1원 증가의 효과)

---

### 방안 2: 모든 변수 표준화 (Z-score)

**방법:**
```python
X_standardized = (X - mean(X)) / std(X)
```

**효과:**
- 모든 변수가 평균 0, 분산 1
- 계수 해석: 1 표준편차 변화의 효과

**장점:**
- 완벽한 스케일 균형
- 계수 크기로 상대적 중요도 비교 가능

**단점:**
- 계수 해석이 덜 직관적
- 원래 단위로 역변환 필요

---

### 방안 3: 가격을 100원 단위로 변환

**방법:**
```python
price_scaled = price_original / 100  # [20, 25, 30]
```

**효과:**
- 가격 표준편차: 0.408 → 4.08
- 잠재점수와 비슷한 스케일

**장점:**
- 해석이 비교적 직관적 (100원 단위)
- 스케일 균형 개선

---

## 🔧 즉시 실행 권장사항

### 1단계: 가격 스케일 복원

```python
# integrated_data_cleaned.csv 수정
df['price'] = df['price'] * 1000  # 2, 2.5, 3 → 2000, 2500, 3000
```

### 2단계: 모델 재추정

- 순차추정 다시 실행
- 부트스트랩 다시 실행 (1000회)

### 3단계: 결과 비교

**예상 결과:**
- 잠재변수 계수의 표준오차 감소
- 추정 안정성 향상
- 유의성 개선 가능성

---

## 📊 예상 효과

### 현재 (price 스케일 = 0.408)

```
λ_main = -0.018 ± 0.317  (SE/|β| = 17.2)
→ 비유의 (p = 0.970)
```

### 수정 후 (price 스케일 = 408)

```
예상: λ_main = -0.018 ± 0.05~0.10  (SE/|β| = 3~5)
→ 유의 가능성 증가
```

**이유:**
- 스케일 균형으로 수치적 안정성 향상
- 최적화 알고리즘이 더 정확하게 수렴
- 표준오차 감소

---

## 🎯 결론

### ✅ 스케일 문제가 주요 원인입니다!

1. **가격 변수의 스케일이 비정상적으로 작음** (0.408)
2. **잠재점수의 스케일이 상대적으로 큼** (0.6~1.0)
3. **스케일 차이가 2~2.4배**
4. **이로 인해 잠재변수 계수의 표준오차가 매우 큼**

### 🔧 해결책

**즉시 실행:**
- 가격을 원래 스케일(원 단위)로 복원
- 모델 재추정

**기대 효과:**
- 추정 안정성 향상
- 표준오차 감소
- 잠재변수 유의성 개선 가능

---

## 📝 다음 단계

1. ✅ 데이터 수정: price × 1000
2. ✅ 순차추정 재실행
3. ✅ 부트스트랩 재실행 (1000회)
4. ✅ 결과 비교 및 보고

